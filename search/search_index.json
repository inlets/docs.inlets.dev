{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Inlets","text":"<p>Inlets is a cloud-native tunnel built to run over restrictive networks, NAT and firewalls. Use it to connect bare-metal, VMs, containers and Kubernetes to the cloud.</p> <p></p> <p>You can visit the inlets homepage at https://inlets.dev/</p> <p>With inlets you are in control of your data, unlike with a SaaS tunnel where shared servers mean your data may be at risk. You can use inlets for local development and in your production environment. It works just as well on bare-metal as in VMs, containers and Kubernetes clusters.</p> <p>inlets is not just compatible with tricky networks and Cloud Native architecture, it was purpose-built for them.</p> <p>Common use-cases include:</p> <ul> <li>Exposing local HTTPS, TCP, or websocket endpoints on the Internet</li> <li>Replacing SaaS tunnels that are too restrictive</li> <li>Self-hosting from a homelab or on-premises datacenter</li> <li>Deploying and monitoring apps across multiple locations</li> <li>Receiving webhooks and testing OAuth integrations</li> <li>Remote customer support</li> </ul> <p>Do you want to scale to dozens, hundreds or thousands of tunnels? You may be looking for inlets uplink</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Inlets tunnels connect to each other over a secure websocket with TLS encryption. Over that private connection, you can then tunnel HTTPS or TCP traffic to computers in another network or to the Internet.</p> <p>One of the most common use-cases is to expose a local HTTP endpoint on the Internet via a HTTPS tunnel. You may be working with webhooks, integrating with OAuth, sharing a draft of a blog post or integrating with a partner's API.</p> <p></p> <p>After deploying an inlets HTTPS server on a public cloud VM, you can then connect the client and access the local service from the Internet.</p> <p>There is more that inlets can do for you than exposing local endpoints. inlets also supports local forwarding and can be used to replace legacy solutions like SSH and VPNs.</p> <p>Learn how inlets compares to VPNs and other solutions in the inlets FAQ.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>These guides walk you through a specific use-case with inlets. If you have questions or cannot find what you need, there are options for connecting with the community at the end of this page.</p> <p>Inlets can tunnel either HTTP or TCP traffic:</p> <ul> <li>HTTP (L7) tunnels can be used to connect one or more HTTP endpoints from one network to another. A single tunnel can expose multiple websites or hosts, including LoadBalancing and multiple clients to one server.</li> <li>TCP (L4) tunnels can be used to connect TCP services such as a database, a reverse proxy, RDP, Kubernetes or SSH to the Internet. A single tunnel can expose multiple ports on an exit-server and load balance between clients</li> </ul> <p>You'll find tutorials in the navigation for HTTP and TCP tunnels, along with a dedicated section for integrating with Kubernetes.</p>"},{"location":"#downloading-inlets","title":"Downloading inlets","text":"<p>inlets is available for Windows, MacOS (Apple Silicon) and Linux:</p> <ul> <li>Download a release</li> </ul> <p>You can also use the container image from ghcr.io: <code>ghcr.io/inlets/inlets-pro:latest</code></p>"},{"location":"#becoming-a-tunnel-provider-or-operating-a-hosting-service","title":"Becoming a tunnel provider or operating a hosting service","text":"<p>Inlets Uplink is a complete solution for Kubernetes that makes it quick and easy to onboard hundreds or thousands of tenants. It can also be used to host tunnel servers on Kubernetes, for smaller amounts of tunnels.</p> <p>Learn more: Inlets Uplink</p>"},{"location":"#reference-documentation","title":"Reference documentation","text":""},{"location":"#inletsctl","title":"inletsctl","text":"<p>Learn how to use inletsctl to provision tunnel servers on various public clouds.</p> <ul> <li>inletsctl reference</li> </ul>"},{"location":"#inlets-operator","title":"inlets-operator","text":"<p>Learn how to set up the inlets-operator for Kubernetes, which provisions public cloud VMs and gives IP addresses to your public LoadBalancers.</p> <ul> <li>inlets-operator reference</li> </ul>"},{"location":"#other-resources","title":"Other resources","text":"<p>For news, use-cases and guides check out the blog:</p> <ul> <li>Official Inlets blog</li> </ul> <p>Watch a video, or read a blog post from the community:</p> <ul> <li>Community tutorials</li> </ul> <p>Open Source tools for managing inlets tunnels:</p> <ul> <li>Inlets Operator for Kubernetes LoadBalancers</li> <li>inletsctl to provision tunnel servers</li> <li>inlets helm charts for clients and servers</li> </ul>"},{"location":"#connecting-with-the-inlets-community","title":"Connecting with the inlets community","text":"<p>Who built inlets? Inlets \u00ae is a commercial solution developed and supported by OpenFaaS Ltd.</p> <p>You can also contact the team via the contact page.</p> <p>The code for this website is open source and available on GitHub</p> <p>inlets is proud to be featured on the Cloud Native Landscape in the Service Proxy category.</p> <p></p>"},{"location":"cloud/","title":"Inlets Cloud","text":"<p>Inlets Cloud is the quickest and easiest way to expose your local services to the Internet. </p> <p>A tunnel server is set up for you in your preferred region, and then you can generate a CLI command or Kubernetes manifest to connect the tunnel to it.</p> <p>Blog posts/tutorials:</p> <ul> <li>Managed HTTPS tunnels in one-click with inlets cloud</li> <li>SSH Into Any Private Host With Inlets Cloud</li> </ul>"},{"location":"cloud/#supported-regions","title":"Supported regions","text":"<p>Each Inlets Cloud region is an independent installation of inlets-uplink which is managed centrally from the Inlets Cloud dashboard.</p> <p>The following regions are available:</p> <ul> <li>cambs1 - London, United Kingdom</li> <li>us-east-1 - Virginia, USA</li> <li>ap-southeast-1 - Singapore</li> </ul> <p>If you'd like to request an additional region, send us a message on the Inlets Discord server.</p>"},{"location":"cloud/#types-of-tunnel","title":"Types of tunnel","text":"<ul> <li> <p>HTTPS terminated tunnels (tryinlets.dev)</p> <p>Try out a HTTPS tunnel with a random name generated for you under the <code>tryinlets.dev</code> domain.</p> <p>Copy the CLI command or Kubernetes YAML and connect your tunnel.</p> <p>You can then access your local service i.e. mkdocs via the generated URL i.e. <code>https://heavy-snow.cambs1.tryinlets.dev</code>.</p> </li> <li> <p>HTTPS terminated tunnels (Bring Your Own Domain)</p> <p>First off, verify one of your domains (i.e. <code>example.com</code> by creating a TXT record as directed in the dashboard.</p> <p>Next, deploy a HTTPS tunnel and specify your custom domain. Inlets Cloud will create a tunnel server and a TLS certificate for you. Then connect the client to the tunnel server using the connection string provided in the dashboard.</p> <p>This is perfect for dashboards, web applications, OpenFaaS, blogs, and any other HTTP service you want to expose to the Internet.</p> </li> <li> <p>Ingress tunnels</p> <p>Use an Ingress tunnel expose ports 80 and 443 with TCP pass-through to your local reverse proxy or Kubernetes Ingress Controller/Istio.</p> </li> <li> <p>SSH tunnels</p> <p>You can expose one or more SSH services to the Internet using the <code>inlets-pro snimux</code> command and an Ingress Tunnel. This is useful for SSH access to your home network or a remote server.</p> <p>Unlike traditional SSH tunnels, you can also implement an IP allow list in the config file for the <code>inlets-pro snimux server</code>.</p> </li> </ul>"},{"location":"cloud/#proxy-protocol","title":"Proxy Protocol","text":"<p>When using the Ingress tunnel type, you can enable the Proxy Protocol. This is useful for services that need to know the original client IP address.</p> <p>Whenever you enable the Proxy Protocol for a tunnel, you'll need to configure your tunneled service to accept the Proxy Protocol header.</p> <p>Both v1 and v2 are supported.</p>"},{"location":"cloud/#get-started-with-inlets-cloud","title":"Get started with Inlets Cloud","text":"<p>An inlets subscription is a pre-requisite. If you don't have one yet, you can sign-up here on a monthly basis.</p> <p>During beta, there is no additional charge or fee for using Inlets Cloud. You can use it to test the service and provide feedback.</p> <p>Just register with the same email you use for your inlets subscription and we will send you an invite.</p> <p>You can log into the inlets cloud dashboard using a magic link sent to your email address.</p>"},{"location":"cloud/#comments-questions-and-suggestions","title":"Comments, questions and suggestions","text":"<p>For any kind of feedback, please use the Inlets Discord server. You'll find a link in your Inlets Cloud dashboard.</p>"},{"location":"cloud/security-groups/","title":"Security Groups for Inlets Cloud","text":"<p>If you want to restrict access to your exposed services, you can set up a Security Group in the Inlets Cloud dashboard.</p> <p>You can add IP addresses or CIDR ranges into a Security Group which will be used to filter incoming traffic to your tunnel server.</p>"},{"location":"cloud/security-groups/#security-groups-for-dynamic-ips","title":"Security Groups for dynamic IPs","text":"<p>If you want to put a dynamic IP address into the Security Group, then you can use the following script to update an existing group by API.</p> <ul> <li>Find the <code>PROJECT_ID</code> through the dashboard and update it in the script</li> <li>Then take the <code>GROUP_NAME</code> from the address bar when viewing your given Security Group</li> <li>Create a file called <code>token.txt</code> and put your API token in it. You can request the API token from the Inlets Discord server.</li> </ul> <pre><code>#!/bin/bash\n\n# File to store the last known IP\nIP_FILE=\".last_known_ip\"\n\n# Get current IP\nCURRENT_IP=$(curl -s -f -L -S https://checkip.amazonaws.com)\n\n# Exit if we couldn't get the IP\nif [ -z \"$CURRENT_IP\" ]; then\n    echo \"Failed to fetch current IP address\"\n    exit 1\nfi\n\n# Check if we need to update\nif [ -f \"$IP_FILE\" ] &amp;&amp; [ \"$(cat $IP_FILE)\" == \"$CURRENT_IP\" ]; then\n    echo \"IP hasn't changed. No update needed.\"\n    exit 0\nfi\n\nPROJECT_ID=1\nGROUP_NAME=\"dynamic-ip\"\nTOKEN=$(cat ./token.txt)\n\n# Update the security group\nif curl -i -f -s -S -L https://cloud.inlets.dev/api/security-groups/$GROUP_NAME?project=$PROJECT_ID \\\n    -X PATCH  \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    --data-binary '{\"allow\": [\"'\"$CURRENT_IP\"'\"]}'; then\n\n    # Only store the IP if the update was successful\n    echo \"$CURRENT_IP\" &gt; \"$IP_FILE\"\n    echo \"Successfully updated IP to: $CURRENT_IP\"\nelse\n    echo \"Failed to update security group\"\n    exit 1\nfi\n</code></pre> <p>You can create a cron expression on a Linux server using <code>crontab -e</code> to run this script every 5 minutes:</p> <pre><code>*/5 * * * * /home/alex/update-security-group.sh\n</code></pre> <p>This will check your current IP address and update the Security Group if it has changed.</p> <p>Important note on rate limiting</p> <p>The script will check your current IP address and update the Security Group if it has changed before making an API call.</p>"},{"location":"reference/","title":"Reference documentation","text":""},{"location":"reference/#inletsctl","title":"inletsctl","text":"<p>Learn how to use inletsctl to provision tunnel servers on various public clouds.</p> <ul> <li>inletsctl reference</li> </ul>"},{"location":"reference/#inlets-operator","title":"inlets-operator","text":"<p>Learn how to set up the inlets-operator for Kubernetes, which provisions public cloud VMs and gives IP addresses to your public LoadBalancers.</p> <ul> <li>inlets-operator reference</li> </ul>"},{"location":"reference/#github-repositories","title":"GitHub repositories","text":"<ul> <li>inlets-pro</li> <li>inlets-operator</li> <li>inletsctl</li> <li>inlets helm charts</li> </ul>"},{"location":"reference/faq/","title":"Inlets FAQ","text":"<p>Inlets concepts and Frequently Asked Questions (FAQ)</p>"},{"location":"reference/faq/#why-did-we-build-inlets","title":"Why did we build inlets?","text":"<p>We built inlets to make it easy to expose a local service on the Internet and to overcome limitations with SaaS tunnels and VPNs. </p> <ul> <li>It was built to overcome limitations in SaaS tunnels - such as lack of privacy, control and rate-limits</li> <li>It doesn't just integrate with containers and Kubernetes, it was purpose-built to run in them</li> <li>It's easy to run on Windows, Linux and MacOS with a self-contained binary</li> <li>It doesn't need to run as root, doesn't depend on iptables, doesn't need a tun device or NET_ADMIN capability</li> </ul> <p>There are many different networking tools available such as VPNs and SaaS tunnels - each with its own set of pros and cons, and use-cases. It's very likely that you will use several tools together to get the best out of each of them.</p>"},{"location":"reference/faq/#how-does-inlets-compare-to-other-tools-and-solutions","title":"How does inlets compare to other tools and solutions?","text":"<p>Are you curious about the advantages of using inlets vs. alternatives? We must first ask, advantages vs. what other tool or service.</p> <p>SaaS tunnels provide a convenient way to expose services for the purposes of development, however they are often:</p> <ul> <li>subject to overreaching terms of service, including licensing your content, censoring, and monitoring your traffic for violations</li> <li>blocked by corporate IT</li> <li>running on shared infrastructure (servers) with other customers</li> <li>subject to stringent rate-limits that affect productivity</li> <li>priced per subdomain</li> <li>unable to offer standard TCP ports like 22, 80, 443, etc</li> </ul> <p>You run inlets on your own servers, so you do not run into those restrictions. Your data remains your own and is kept private.</p> <p>When compared to VPNs such as Wireguard, Tailscale and OpenVPN, we have to ask what the use-case is.</p> <p>A traditional VPN is built to connect hosts and entire IP ranges together. This can potentially expose a large number of machines and users to each other and requires complex Access Control Lists or authorization rules. If this is your use-case, a traditional VPN is probably the right tool for the job.</p> <p>Inlets is designed to connect or expose services between networks - either HTTP or TCP.</p> <p>For example:</p> <ul> <li>Receiving webhooks to a local application</li> <li>Sharing a blog post draft with a colleague or client</li> <li>Providing remote access to your homelab when away from home</li> <li>Self-hosting websites or services on Kubernetes clusters</li> <li>Getting working LoadBalancers with public IPs for local Kubernetes clusters</li> </ul> <p>You can also use inlets to replace Direct Connect or a VPN when you just need to connect a number of services privately and not an entire network range.</p> <p>Many of the inlets community use a VPN alongside inlets, because they are different tools for different use-cases.</p> <p>We often write about use-cases for public and private inlets tunnels on the blog.</p>"},{"location":"reference/faq/#whats-the-difference-between-inlets-inletsctl-and-inlets-operator","title":"What's the difference between inlets, inletsctl and inlets-operator?","text":"<p>inlets-pro aka \"inlets\" is the command-line tool that contains both the client and server required to set up HTTP and TCP tunnels.</p> <p>The inlets-pro server is usually set up on a computer with a public IP address, then the inlets-pro client is run on your own machine, or a separate computer that can reach the service or server you want to expose.</p> <p>You can download inlets-pro and inletsctl with the \"curl | sh\" commands provided at the start of each tutorial, this works best on a Linux host, or with Git Bash if using Windows.</p> <p>Did you know? You can also download binaries for inlets-pro and inletsctl on GitHub, for Windows users you'll want \"inlets-pro.exe\" and for MacOS, you'll want \"inlets-pro-darwin\".</p> <p>For instance, on Windows machines you'll need \"inlets-pro.exe\"</p> <p>See also: inlets-pro releases</p> <p>inletsctl is a tool that can set up a tunnel server for you on around a dozen popular clouds. It exists to make it quicker and more convenience to set up a HTTPS or TCP tunnel to expose a local service.</p> <p>It has three jobs:</p> <ol> <li>Create the VM for you</li> <li>Install the inlets-pro server in TCP or HTTPS mode (as specified) with systemd</li> <li>Inform you of the token and connection string</li> </ol> <p>You can download the inletsctl tool with \"curl | sh\" or from the inletsctl releases page.</p> <p>Find out more: inletsctl reference page</p> <p>inlets-operator is a Kubernetes Operator that will create tunnel servers for you, on your chosen cloud for any LoadBalancers that you expose within a private cluster.</p> <p>Find out more: inlets-operator reference page</p>"},{"location":"reference/faq/#what-is-the-networking-model-for-inlets","title":"What is the networking model for inlets?","text":"<p>Whilst some networking tools such as Bittorrent use a peer-to-peer network, inlets uses a more traditional client/server model.</p> <p>One or more client tunnels connect to a tunnel server and advertise which services they are able to provide. Then, whenever the server receives traffic for one of those advertised services, it will forward it through the tunnel to the client. The tunnel client will then forward that on to the service it advertised.</p> <p>The tunnel server may also be referred to as an \"exit\" server because it is the connection point for the client to another network or the Internet.</p> <p>If you install and run the inlets server on a computer, it can be referred to as a tunnel server or exit server. These servers can also be automated through cloud-init, terraform, or tools maintained by the inlets community such as inletsctl.</p> <p></p> <p>Pictured: the website <code>http://127.0.0.1:3000</code> is exposed through an encrypted tunnel to users at: <code>https://example.com</code></p> <p>For remote forwarding, the client tends to be run within a private network, with an <code>--upstream</code> flag used to specify where incoming traffic needs to be routed. The tunnel server can then be run on an Internet-facing network, or any other network reachable by the client.</p>"},{"location":"reference/faq/#what-kind-of-layers-and-protocols-are-supported","title":"What kind of layers and protocols are supported?","text":"<p>Inlets works at a higher level than traditional VPNs because it is designed to connect services together, rather than hosts directly.</p> <ul> <li>HTTP - Layer 7 of the OSI model, used for web traffic such as websites and RESTful APIs</li> <li>TCP - Layer 4 of the OSI model, used for TCP traffic like SSH, TLS, databases, RDP, etc</li> </ul> <p>Because VPNs are designed to connect hosts together over a shared IP space, they also involve tedious IP address management and allocation.</p> <p>Inlets connects services, so for TCP traffic, you need only think about TCP ports.</p> <p>For HTTP traffic, you need only to think about domain names.</p>"},{"location":"reference/faq/#do-i-want-a-tcp-or-https-tunnel","title":"Do I want a TCP or HTTPS tunnel?","text":"<p>If you're exposing websites, blogs, docs, APIs and webhooks, you should use a HTTPS tunnel.</p> <p>For HTTP tunnels, Rate Error and Duration (RED) metrics are collected for any service you expose, even if it doesn't have its own instrumentation support.</p> <p>For anything that doesn't fit into that model, a TCP tunnel may be a better option.</p> <p>Common examples are: TLS, websockets, RDP, VNC, SSH, database protocols, NATS, or legacy medical protocols such as DiCom.</p> <p>TCP tunnels can also be used to forward traffic to a reverse proxy like Nginx, Caddy, Traefik or Istio, sitting behind a firewall or NAT by forwarding port 80 and 443.</p> <p>TCP traffic is forwarded directly between the two hosts without any decryption of bytes. The active connection count and frequency can be monitored along with the amount of throughput.</p>"},{"location":"reference/faq/#how-much-memory-or-ram-does-a-tunnel-require","title":"How much memory or RAM does a tunnel require?","text":"<p>Each tunnel client and tunnel server run on different machines in their own process. Typically we have observed single-digit RAM consumption for tunnels.</p> <p>This means that you can scale to hundreds or thousands of tunnels with modest hardware.</p>"},{"location":"reference/faq/#how-tunnels-can-i-run","title":"How tunnels can I run?","text":"<p>The best way to scale to huge numbers of tunnels is with Inlets Uplink.</p> <p>Providing you have the license entitlement to run them, you can run as many as you require.</p> <p>If you are hosting tunnel servers within Kubernetes, you can typically run 5000 tunnels per Kubernetes namespace, with inlets uplink you could create as many namespaces as you need.</p> <p>The typical amount of Pods that can run on a Kubernetes node is 100, and since the system requirements are so low, you could use cheaper, lower specification nodes whilst scaling out.</p> <p>If your tunnel clients are distributed into different countries or regions, you could deploy inlets uplink into multiple Kubernetes clusters.</p>"},{"location":"reference/faq/#does-inlets-use-tcp-or-udp","title":"Does inlets use TCP or UDP?","text":"<p>Inlets uses a websocket over TCP, so that it can penetrate HTTP proxies, captive portals, firewalls, and other kinds of NAT. As long as the client can make an outbound connection, a tunnel can be established. The use of HTTPS means that inlets will have similar latency and throughput to a HTTPS server or SSH tunnel.</p> <p>Once you have an inlets tunnel established, you can use it to tunnel traffic to TCP and HTTPS sockets within the private network of the client.</p> <p>Most VPNs tend to use UDP for communication due to its low overhead which results in lower latency and higher throughput. Certain tools and products such as OpenVPN, SSH and Tailscale can be configured to emulate a TCP stack over a TCP connection, this can lead to unexpected issues.</p> <p>Inlets connections send data, rather than emulating a TCP over TCP stack, so doesn't suffer from this problem.</p>"},{"location":"reference/faq/#are-both-remote-and-local-forwarding-supported","title":"Are both remote and local forwarding supported?","text":"<p>Remote forwarding is where a local service is forwarded from the client's network to the inlets tunnel server.</p> <p></p> <p>Remote forwarding pushes a local endpoint to a remote host for access on another network</p> <p>This is the most common use-case and would be used to expose a local HTTP server to the public Internet via a tunnel server.</p> <p>Local forwarding is used to forward a service on the tunnel server or tunnel server's network back to the client, so that it can be accessed using a port on localhost.</p> <p></p> <p>Local forwarding brings a remote service back to localhost for accessing</p> <p>An example would be that you have a webserver and MySQL database. The HTTP server is public and can access the database via its own loopback adapter, but the Internet cannot. So how do you access that MySQL database from CI, or from your local machine? Connect a client with local forwarding, and bring the MySQL port back to your local machine or CI runner, and then use the MySQL CLI to access it.</p> <p>A developer at the UK Government uses inlets to forward a NATS message queue from a staging environment to his local machine for testing. Learn more</p>"},{"location":"reference/faq/#whats-the-difference-between-the-data-plane-and-control-plane","title":"What's the difference between the data plane and control plane?","text":"<p>The data plane is any service or port that carries traffic from the tunnel server to the tunnel client, and your private TCP or HTTP services. It can be exposed on all interfaces, or only bound to loopback for private access, in a similar way to a VPN.</p> <p>If you were exposing SSH on an internal machine from port <code>2222</code>, your data-plane may be exposed on port <code>2222</code></p> <p>The control-plane is a TLS-encrypted, authenticated websocket that is used to connect clients to servers. All traffic ultimately passes over the control-plane's link, so remains encrypted and private.</p> <p>Your control-plane's port is usually <code>8123</code> when used directly, or <code>443</code> when used behind a reverse proxy or Kubernetes Ingress Controller.</p> <p>An example from the article: The Simple Way To Connect Existing Apps to Public Cloud</p> <p>A legacy MSSQL server runs on Windows Server behind the firewall in a private datacenter. Your organisation cannot risk migrating it to an AWS EC2 instance at this time, but can move the microservice that needs to access it.</p> <p>The inlets tunnel allows for the MSSQL service to be tunneled privately to the EC2 instance's local network for accessing, but is not exposed on the Internet. All traffic is encrypted over the wire due to the TLS connection of inlets.</p> <p></p> <p>Hybrid Cloud in action using an inlets tunnel to access the on-premises database</p> <p>This concept is referred to as a a \"split plane\" because the control plane is available to public clients on all adapters, and the data plane is only available on local or private adapters on the server.</p>"},{"location":"reference/faq/#is-there-a-reference-guide-to-the-cli","title":"Is there a reference guide to the CLI?","text":"<p>The inlets-pro binary has built-in help commands and examples, just run <code>inlets-pro tcp/http client/server --help</code>.</p> <p>A separate CLI reference guide is also available here: inlets-pro CLI reference</p>"},{"location":"reference/faq/#is-inlets-secure","title":"Is inlets secure?","text":"<p>All traffic sent over an inlets tunnel is encapsulated in a TLS-encrypted websocket, which prevents eavesdropping. This is technically similar to HTTPS, but you'll see a URL of <code>wss://</code> instead.</p> <p>The tunnel client is authenticated using an API token which is generated by the tunnel administrator, or by automated tooling.</p> <p>Additional authentication mechanisms can be set up using a reverse proxy such as Nginx.</p>"},{"location":"reference/faq/#do-i-have-to-expose-services-on-the-internet-to-use-inlets","title":"Do I have to expose services on the Internet to use inlets?","text":"<p>No, inlets can be used to tunnel one or more services to another network without exposing them on the Internet.</p> <p>The <code>--data-addr 127.0.0.1:</code> flag for inlets servers binds the data plane to the server's loopback address, meaning that only other processing running on it can access the tunneled services. You could also use a private network adapter or VPC IP address in the <code>--data-addr</code> flag.</p>"},{"location":"reference/faq/#how-do-i-monitor-inlets","title":"How do I monitor inlets?","text":"<p>See the following blog post for details on the <code>inlets status</code> command and the various Prometheus metrics that are made available.</p> <p>Measure and monitor your inlets tunnels</p>"},{"location":"reference/faq/#how-do-you-scale-inlets","title":"How do you scale inlets?","text":"<p>Inlets HTTP servers can support a high number of clients, either for load-balancing the same internal service to a number of clients, or for a number of distinct endpoints.</p> <p>Tunnel servers are easy to scale through the use of containers, and can benefit from the resilience that a Kubernetes cluster can bring:</p> <p>See also: How we scaled inlets to thousands of tunnels with Kubernetes</p>"},{"location":"reference/faq/#does-inlets-support-high-availability-ha","title":"Does inlets support High Availability (HA)?","text":"<p>For the inlets client, it is possible to connect multiple inlets tunnel clients for the same service, such as a company blog. Traffic will be distributed across the clients and if one of those clients goes down or crashes, the other will continue to serve requests.</p> <p>For the inlets tunnel server, the easiest option is to run the server in a supervisor that can restart the tunnel service quickly or allow it to run more than one replica. Systemd can be used to restart tunnel servers should they run into issues, likewise you can run the server in a container, or as a Kubernetes Pod.</p> <p></p> <p>HA example with an AWS ELB</p> <p>For example, you may place a cloud load-balancer in front of the data-plane port of two inlets server processes. Requests to the stable load-balancer IP address will be distributed between the two virtual machines and their respective inlets server tunnel processes.  </p>"},{"location":"reference/faq/#is-ipv6-supported","title":"Is IPv6 supported?","text":"<p>Yes, see also: How to serve traffic to IPv6 users with inlets</p>"},{"location":"reference/faq/#what-if-the-websocket-disconnects","title":"What if the websocket disconnects?","text":"<p>The client will reconnect automatically and can be configured with systemd or a Windows service to stay running in the background. See also <code>inlets pro tcp/http server/client --generate=systemd</code> for generating systemd unit files.</p> <p>When used in combination with a Kubernetes ingress controller or reverse proxy of your own, then the websocket may timeout. These timeout settings can usually be configured to remove any potential issue.</p> <p>Monitoring in inlets allows for you to monitor the reliability of your clients and servers, which are often running in distinct networks.</p>"},{"location":"reference/faq/#how-much-does-inlets-cost","title":"How much does inlets cost?","text":"<p>Monthly and annual subscriptions are available via Gumroad.</p> <p>You can also purchase a static license for offline or air-gapped environments.</p> <p>For more, see the Pricing page</p>"},{"location":"reference/faq/#what-happens-when-the-license-expires","title":"What happens when the license expires?","text":"<p>If you're using a Gumroad license, and keep your billing relationship active, then the software will work for as long as you keep paying. The Gumroad license server needs to be reachable by the inlets client.</p> <p>If you're using a static license, then the software will continue to run, even after your license has expired, unless you restart the software. You can either rotate the token on your inlets clients in an automated or manual fashion, or purchase a token for a longer period of time up front.</p>"},{"location":"reference/faq/#can-i-get-professional-help","title":"Can I get professional help?","text":"<p>Inlets is designed to be self-service and is well documented, but perhaps you could use some direction?</p> <p>Business licenses come with support via email, however you are welcome to contact OpenFaaS Ltd to ask about a consulting project.</p>"},{"location":"reference/inlets-operator/","title":"inlets-operator reference documentation","text":"<p>The inlets/inlets-operator brings LoadBalancers with public IP addresses to your local Kubernetes clusters.</p> <p>It works by creating VMs and running an inlets Pro tunnel server for you, the VM's public IP is then attached to the cluster and an inlets client Pod runs for you.</p> <p>For each provider, the minimum requirements tend to be:</p> <ul> <li>An access token - for the operator to create VMs for inlets Pro servers</li> <li>A region - where to create the VMs</li> </ul> <p>Helm or Arkade?</p> <p>You can install the inlets-operator's Helm chart using a single command with arkade. arkade is an open-source Kubernetes marketplace and easy to use. Helm involves more commands, and is preferred by power users.</p> <p>Get your inlets subscription here</p>"},{"location":"reference/inlets-operator/#tunnel-custom-resource-definition-crd-and-lifecycle","title":"Tunnel Custom Resource Definition (CRD) and lifecycle","text":"<p>The inlets-operator uses a custom resource definition (CRD) to create tunnels. The CRD is called <code>Tunnel</code> and its full name is <code>tunnels.operator.inlets.dev</code></p> <pre><code>$ kubectl get tunnels -n default\nNAMESPACE   NAME             SERVICE   HOSTSTATUS   HOSTIP        CREATED\ndefault     nginx-1-tunnel   nginx-1   active       46.101.1.67   2m45s\n</code></pre> <p>The CRD can be used to view and monitor tunnels. The <code>HOSTSTATUS</code> field shows the status of the tunnel, and the <code>HOSTIP</code> field shows the public IP address of the tunnel.</p> <p>The tunnel's IP address will also be written directly to any <code>Service</code> with a type of <code>LoadBalancer</code>.</p> <pre><code>$ kubectl get svc -n default\nNAME         TYPE           CLUSTER-IP    EXTERNAL-IP               PORT(S)        AGE\nkubernetes   ClusterIP      10.96.0.1     &lt;none&gt;                    443/TCP        6m26s\nnginx-1      LoadBalancer   10.96.94.18   46.101.1.67,46.101.1.67   80:31194/TCP   4m21s\n</code></pre> <p>The lifecycle of a tunnel is tied to the Service in Kubernetes.</p> <p>To delete a tunnel permanently, you can delete the Service:</p> <pre><code>kubectl delete svc nginx-1\n</code></pre> <p>To have the tunnel server re-created, you can delete the tunnel CustomResource, this causes the operator to re-create the tunnel:</p> <pre><code>kubectl delete tunnel nginx-1-tunnel\n</code></pre> <p>Bear in mind that if you delete your cluster before you delete the LoadBalancer service, then the inlets-operator will have no way to remove the tunnel servers that have been created for you. Therefore, you should always delete the LoadBalancer service before deleting the cluster. If you should forget, and delete your K3s or KinD cluster, then you can go into your cloud account and delete the VMs manually.</p> <p>As a rule, the name of the VM will match the name of the service in Kubernetes.</p>"},{"location":"reference/inlets-operator/#working-with-another-loadbalancer","title":"Working with another LoadBalancer","text":"<p>If you're running metal-lb or kube-vip to provide local IP addresses for LoadBalancer services, then you can annotate the services you wish to expose to the Internet with <code>operator.inlets.dev/manage=1</code>, then set <code>annotatedOnly: true</code> in the inlets-operator Helm chart.</p>"},{"location":"reference/inlets-operator/#install-inlets-operator-using-arkade","title":"Install inlets-operator using arkade","text":"<pre><code>export REGION=lon1\nexport PROVIDER=digitalocean\n\narkade install inlets-operator \\\n --provider $PROVIDER \\ # Name of the cloud provider to provision the exit-node on.\n --region $REGION \\ # Used with cloud providers that require a region.\n --token-file $HOME/Downloads/do-access-token.txt # Token file/Service Account Key file with the access to the cloud provider.\n</code></pre>"},{"location":"reference/inlets-operator/#install-inlets-operator-using-helm","title":"Install inlets-operator using helm","text":"<p>The following instructions are a generic example, you should refer to each specific heading to understand how to create the required API keys for a given cloud provider.</p> <ul> <li>Some providers require an access key, others also need a secret key.</li> <li>Some providers only use a region, others use a zone and projectID too.</li> <li>There are additional flags you can set via values.yaml or the <code>--set</code> flag.</li> </ul> <p>You can view the inlets-operator chart on GitHub to learn more.</p> <pre><code># Create a namespace for inlets-operator\nkubectl create namespace inlets\n\n# Create a secret to store the service account key file\nkubectl create secret generic inlets-access-key \\\n  --namespace inlets \\\n  --from-file inlets-access-key=$HOME/Downloads/do-access-token.txt\n\n# Create a secret to store the inlets-pro license\nkubectl create secret generic \\\n  --namespace inlets \\\n  inlets-license --from-file license=$HOME/.inlets/LICENSE\n\n# Add and update the inlets-operator helm repo\n# You only need to do this once.\nhelm repo add inlets https://inlets.github.io/inlets-operator/\n\nexport REGION=lon1\nexport PROVIDER=digitalocean\n\n# Update the Helm repository and perform an installation\nhelm repo update &amp;&amp; \\\n  helm upgrade inlets-operator --install inlets/inlets-operator \\\n  --namespace inlets \\\n  --set provider=$PROVIDER \\\n  --set region=$REGION\n</code></pre>"},{"location":"reference/inlets-operator/#instructions-per-cloud","title":"Instructions per cloud","text":""},{"location":"reference/inlets-operator/#create-tunnel-servers-on-digitalocean","title":"Create tunnel servers on DigitalOcean","text":"<p>The DigitalOcean provider is fast, cost effective and easy to set it. It's recommended for most users.</p> <p>Create an API access token with full read/write permissions and save it to: <code>$HOME/Downloads/do-access-token.txt</code>.</p> <p>Now, install the chart with arkade using the above options:</p> <pre><code>arkade install inlets-operator \\\n --provider digitalocean \\\n --region lon1 \\\n --token-file $HOME/Downloads/do-access-token.txt\n</code></pre> <p>If you have the DigitalOcean CLI (<code>doctl</code>) installed, then you can use it to list available regions and their codes to input into the above command. Bear in mind that some regions are showing no availability for starting new VMs.</p> <pre><code>doctl compute region ls\nSlug    Name               Available\nnyc1    New York 1         true\nsfo1    San Francisco 1    false\nnyc2    New York 2         false\nams2    Amsterdam 2        false\nsgp1    Singapore 1        true\nlon1    London 1           true\nnyc3    New York 3         true\nams3    Amsterdam 3        true\nfra1    Frankfurt 1        true\ntor1    Toronto 1          true\nsfo2    San Francisco 2    true\nblr1    Bangalore 1        true\nsfo3    San Francisco 3    true\nsyd1    Sydney 1           true\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-aws-ec2","title":"Create tunnel servers on AWS EC2","text":"<p>Instructions for AWS EC2</p> <p>To use the instructions below you must have the AWS CLI configured with sufficient permissions to create users and roles.</p> <ul> <li>Create a AWS IAM Policy with the following:</li> </ul> <p>Create a file named <code>policy.json</code> with the following content</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeImages\",\n        \"ec2:TerminateInstances\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:CreateTags\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:RunInstances\",\n        \"ec2:DescribeInstanceStatus\"\n      ],\n      \"Resource\": [\"*\"]\n    }\n  ]\n}\n</code></pre> <p>Create the policy in AWS</p> <pre><code>aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json\n</code></pre> <ul> <li>Create an IAM user</li> </ul> <pre><code>aws iam create-user --user-name inlets-automation\n</code></pre> <ul> <li>Add the Policy to the IAM user</li> </ul> <p>We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below.</p> <pre><code>export AWS_ACCOUNT_NUMBER=\"Your AWS Account Number\"\naws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam::${AWS_ACCOUNT_NUMBER}:policy/inlets-automation\n</code></pre> <ul> <li>Generate an access key for your IAM User</li> </ul> <p>The below commands will create a set of credentials and save them into files for use later on.</p> <p>we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually.</p> <pre><code>ACCESS_KEY_JSON=$(aws iam create-access-key --user-name inlets-automation)\necho $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId &gt; ~/Downloads/aws-access-key\necho $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey &gt; ~/Downloads/aws-secret-access-key\n</code></pre> <p>Install the chart with arkade using the above options:</p> <pre><code>arkade install inlets-operator \\\n --provider ec2 \\\n --region eu-west-1 \\\n --token-file $HOME/Downloads/aws-access-key \\\n --secret-key-file $HOME/Downloads/aws-secret-access-key\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-google-compute-engine-gce","title":"Create tunnel servers on Google Compute Engine (GCE)","text":"<p>Instructions for Google Cloud</p> <p>It is assumed that you have gcloud installed and configured on your machine. If not, then follow the instructions here</p> <p>To get your service account key file, follow the steps below:</p> <pre><code># Get current projectID\nexport PROJECTID=$(gcloud config get-value core/project 2&gt;/dev/null)\n\n# Create a service account\ngcloud iam service-accounts create inlets \\\n  --description \"inlets-operator service account\" \\\n  --display-name \"inlets\"\n\n# Get service account email\nexport SERVICEACCOUNT=$(gcloud iam service-accounts list | grep inlets | awk '{print $2}')\n\n# Assign appropriate roles to inlets service account\ngcloud projects add-iam-policy-binding $PROJECTID \\\n  --member serviceAccount:$SERVICEACCOUNT \\\n  --role roles/compute.admin\n\ngcloud projects add-iam-policy-binding $PROJECTID \\\n  --member serviceAccount:$SERVICEACCOUNT \\\n  --role roles/iam.serviceAccountUser\n\n# Create inlets service account key file\ngcloud iam service-accounts keys create key.json \\\n  --iam-account $SERVICEACCOUNT\n</code></pre> <p>Install the chart with arkade using the above options:</p> <pre><code>arkade install inlets-operator \\\n    --provider gce \\\n    --project-id $PROJECTID \\\n    --zone us-central1-a \\\n    --token-file key.json\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-hetzner","title":"Create tunnel servers on Hetzner","text":"<p>Create an API key with read/write access, save it to ~/hetzner.txt.</p> <pre><code>arkade install inlets-operator \\\n    --provider hetzner \\\n    --region eu-central \\\n    --token-file ~/hetzner.txt\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-azure","title":"Create tunnel servers on Azure","text":"<p>Instructions for Azure</p> <p>Prerequisites:</p> <ul> <li>You will need <code>az</code>. See Install the Azure CLI</li> <li>You'll need to have run <code>az login</code> also</li> </ul> <p>Generate Azure authentication file:</p> <pre><code>SUBSCRIPTION_ID=\"YOUR_SUBSCRIPTION_ID\"\naz ad sp create-for-rbac --role Contributor --scopes \"/subscriptions/$SUBSCRIPTION_ID\" --sdk-auth \\\n  &gt; $HOME/Downloads/client_credentials.json\n</code></pre> <p>Find your region code with:</p> <pre><code>az account list-locations -o table\n\nDisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nUnited Kingdom            ukwest               United Kingdom\n</code></pre> <p>Install using helm:</p> <pre><code>export SUBSCRIPTION_ID=\"YOUR_SUBSCRIPTION_ID\"\nexport AZURE_REGION=\"ukwest\"\nexport INLETS_LICENSE=\"$(cat ~/.inlets/LICENSE)\"\nexport ACCESS_KEY=\"$HOME/Downloads/client_credentials.json\"\n\nkubectl create secret generic inlets-access-key \\\n  --from-file=inlets-access-key=$ACCESS_KEY\n\nhelm repo add inlets https://inlets.github.io/inlets-operator/\nhelm repo update\n\nhelm upgrade inlets-operator --install inlets/inlets-operator \\\n  --set provider=azure,region=$AZURE_REGION \\\n  --set subscriptionID=$SUBSCRIPTION_ID\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-linode","title":"Create tunnel servers on Linode","text":"<p>Instructions for Linode</p> <p>Install using helm:</p> <pre><code># Create a secret to store the service account key file\nkubectl create secret generic inlets-access-key --from-literal inlets-access-key=&lt;Linode API Access Key&gt;\n\n# Add and update the inlets-operator helm repo\nhelm repo add inlets https://inlets.github.io/inlets-operator/\n\nhelm repo update\n\n# Install inlets-operator with the required fields\nhelm upgrade inlets-operator --install inlets/inlets-operator \\\n  --set provider=linode \\\n  --set region=us-east\n</code></pre> <p>You can also install the inlets-operator using a single command using arkade, arkade runs against any Kubernetes cluster.</p> <p>Install the chart with arkade using the above options:</p> <p><code>bash arkade install inlets-operator \\  --provider linode \\  --region us-east \\  --access-key $LINODE_ACCESS_KEY</code></p>"},{"location":"reference/inletsctl/","title":"inletsctl reference documentation","text":"<p>inletsctl is an automation tool for inlets/-pro.</p> <p>Features:</p> <ul> <li><code>create</code> / <code>delete</code> cloud VMs with inlets/-pro pre-installed via systemd</li> <li><code>download [--pro]</code> - download the inlets/-pro binaries to your local computer</li> <li><code>kfwd</code> - forward services from a Kubernetes cluster to your local machine using inlets/-pro</li> </ul> <p>View the code on GitHub: inlets/inletsctl</p>"},{"location":"reference/inletsctl/#install-inletsctl","title":"Install <code>inletsctl</code>","text":"<p>You can install inletsctl using its installer, or from the GitHub releases page</p> <pre><code># Install to local directory (and for Windows users)\ncurl -sLSf https://inletsctl.inlets.dev | sh\n\n# Install directly to /usr/local/bin/\ncurl -sLSf https://inletsctl.inlets.dev | sudo sh\n</code></pre> <p>Windows users are encouraged to use git bash to install the inletsctl binary.</p>"},{"location":"reference/inletsctl/#downloading-inlets-pro","title":"Downloading inlets-pro","text":"<p>The <code>inletsctl download</code> command can be used to download the inlets/-pro binaries.</p> <p>Example usage:</p> <pre><code># Download the latest inlets-pro binary\ninletsctl download\n\n# Download a specific version of inlets-pro\ninletsctl download --version 0.8.5\n</code></pre>"},{"location":"reference/inletsctl/#the-create-command","title":"The <code>create</code> command","text":""},{"location":"reference/inletsctl/#create-a-https-tunnel-with-a-custom-domain","title":"Create a HTTPS tunnel with a custom domain","text":"<p>This example uses DigitalOcean to create a cloud VM and then exposes a local service via the newly created exit-server.</p> <p>Let's say we want to expose a Grafana server on our internal network to the Internet via Let's Encrypt and HTTPS?</p> <pre><code>export DOMAIN=\"grafana.example.com\"\n\ninletsctl create \\\n  --provider digitalocean \\\n  --region=\"lon1\" \\\n  --access-token-file $HOME/do-access-token \\\n  --letsencrypt-domain $DOMAIN \\\n  --letsencrypt-issuer prod\n</code></pre> <p>You can also use <code>--letsencrypt-issuer</code> with the <code>staging</code> value whilst testing since Let's Encrypt rate-limits how many certificates you can obtain within a week.</p> <p>Create a DNS A record for the IP address so that <code>grafana.example.com</code> for instance resolves to that IP. For instance you could run:</p> <pre><code>doctl compute domain create \\\n  --ip-address 46.101.60.161 grafana.example.com\n</code></pre> <p>Now run the command that you were given, and if you wish, change the upstream to point to the domain explicitly:</p> <pre><code># Obtain a license at https://inlets.dev\n# Store it at $HOME/.inlets/LICENSE or use --help for more options\n\n# Where to route traffic from the inlets server\nexport UPSTREAM=\"grafana.example.com=http://192.168.0.100:3000\"\n\ninlets-pro http client --url \"wss://46.101.60.161:8123\" \\\n--token \"lRdRELPrkhA0kxwY0eWoaviWvOoYG0tj212d7Ff0zEVgpnAfh5WjygUVVcZ8xJRJ\" \\\n--upstream $UPSTREAM\n\nTo delete:\n  inletsctl delete --provider digitalocean --id \"248562460\"\n</code></pre> <p>You can also specify more than one domain and upstream for the same tunnel, so you could expose OpenFaaS and Grafana separately for instance.</p> <p>Update the <code>inletsctl create</code> command with multiple domains such as: <code>--letsencrypt-domain openfaas.example.com --letsencrypt-domain grafana.example.com</code></p> <p>Then for the <code>inlets-pro client</code> command, update the upstream in the same way by repeating the flag once per upstream mapping: <code>--upstream openfaas.example.com=http://127.0.0.1:8080</code> <code>--upstream grafana.example.com=http://192.168.0.100:3000</code>.</p> <p>Note that in previous inlets versions, multiple upstream values were given in a single flag, separated by commas, this has now been deprecated for the above syntax.</p>"},{"location":"reference/inletsctl/#create-a-http-tunnel","title":"Create a HTTP tunnel","text":"<p>This example uses Linode to create a cloud VM and then exposes a local service via the newly created exit-server.</p> <pre><code>export REGION=\"eu-west\"\n\ninletsctl create \\\n  --provider linode \\\n  --region=\"$REGION\" \\\n  --access-token-file $HOME/do-access-token\n</code></pre> <p>You'll see the host being provisioned, it usually takes just a few seconds:</p> <pre><code>Using provider: linode\nRequesting host: peaceful-lewin8 in eu-west, from linode\n2021/06/01 15:56:03 Provisioning host with Linode\nHost: 248561704, status: \n[1/500] Host: 248561704, status: new\n...\n[11/500] Host: 248561704, status: active\n\ninlets Pro (0.7.0) exit-server summary:\n  IP: 188.166.168.90\n  Auth-token: dZTkeCNYgrTPvFGLifyVYW6mlP78ny3jhyKM1apDL5XjmHMLYY6MsX8S2aUoj8uI\n</code></pre> <p>Now run the command given to you, changing the <code>--upstream</code> URL to match a local URL such as <code>http://localhost:3000</code></p> <pre><code># Obtain a license at https://inlets.dev\nexport LICENSE=\"$HOME/.inlets/license\"\n\n# Give a single value or comma-separated\nexport PORTS=\"3000\"\n\n# Where to route traffic from the inlets server\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://188.166.168.90:8123/connect\" \\\n  --token \"dZTkeCNYgrTPvFGLifyVYW6mlP78ny3jhyKM1apDL5XjmHMLYY6MsX8S2aUoj8uI\" \\\n  --upstream $UPSTREAM \\\n  --ports $PORTS\n</code></pre> <p>The client will look for your license in <code>$HOME/.inlets/LICENSE</code>, but you can also use the <code>--license/--license-file</code> flag if you wish.</p> <p>You can then access your local website via the Internet and the exit-server's IP at:</p> <p>http://188.166.168.90</p> <p>When you're done, you can delete the host using its ID or IP address:</p> <pre><code>  inletsctl delete --provider linode --id \"248561704\"\n  inletsctl delete --provider linode --ip \"188.166.168.90\"\n</code></pre>"},{"location":"reference/inletsctl/#create-a-tunnel-for-a-tcp-service","title":"Create a tunnel for a TCP service","text":"<p>This example is similar to the previous one, but also adds link-level encryption between your local service and the exit-server.</p> <p>In addition, you can also expose pure TCP traffic such as SSH or Postgresql.</p> <pre><code>inletsctl create \\\n  --provider digitalocean \\\n  --access-token-file $HOME/do-access-token \\\n  --pro\n</code></pre> <p>Note the output:</p> <pre><code>inlets Pro (0.7.0) exit-server summary:\n  IP: 142.93.34.79\n  Auth-token: TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\n\nCommand:\n  export LICENSE=\"\"\n  export PORTS=\"8000\"\n  export UPSTREAM=\"localhost\"\n\n  inlets-pro tcp client --url \"wss://142.93.34.79:8123/connect\" \\\n        --token \"TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\" \\\n        --license \"$LICENSE\" \\\n        --upstream $UPSTREAM \\\n        --ports $PORTS\n\nTo Delete:\n          inletsctl delete --provider digitalocean --id \"205463570\"\n</code></pre> <p>Run a local service that uses TCP such as MariaDB:</p> <pre><code>head -c 16 /dev/urandom |shasum \n8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\n\nexport PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\n\ndocker run --name mariadb \\\n-p 3306:3306 \\\n-e MYSQL_ROOT_PASSWORD=8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 \\\n-ti mariadb:latest\n</code></pre> <p>Connect to the tunnel updating the ports to <code>3306</code></p> <pre><code>export LICENSE=\"$(cat ~/LICENSE)\"\nexport PORTS=\"3306\"\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://142.93.34.79:8123/connect\" \\\n      --token \"TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\" \\\n      --license \"$LICENSE\" \\\n      --upstream $UPSTREAM \\\n      --ports $PORTS\n</code></pre> <p>Now connect to your MariaDB instance from its public IP address:</p> <pre><code>export PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\nexport EXIT_IP=\"142.93.34.79\"\n\ndocker run -it --rm mariadb:latest mysql -h $EXIT_IP -P 3306 -uroot -p$PASSWORD\n\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 3\nServer version: 10.5.5-MariaDB-1:10.5.5+maria~focal mariadb.org binary distribution\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [(none)]&gt; create database test; \nQuery OK, 1 row affected (0.039 sec)\n</code></pre>"},{"location":"reference/inletsctl/#examples-for-specific-cloud-providers","title":"Examples for specific cloud providers","text":""},{"location":"reference/inletsctl/#example-usage-with-aws-ec2","title":"Example usage with AWS EC2","text":"<p>To use the instructions below you must have the AWS CLI configured with sufficient permissions to  create users and roles. </p> <ul> <li>Create a AWS IAM Policy with the following:</li> </ul> <p>Create a file named <code>policy.json</code> with the following content</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [  \n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeImages\",\n                \"ec2:TerminateInstances\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateTags\",\n                \"ec2:DeleteSecurityGroup\",\n                \"ec2:RunInstances\",\n                \"ec2:DescribeInstanceStatus\"\n            ],\n            \"Resource\": [\"*\"]\n        }\n    ]\n}\n</code></pre> <p>Create the policy in AWS </p> <pre><code>aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json\n</code></pre> <ul> <li>Create an IAM user</li> </ul> <pre><code>aws iam create-user --user-name inlets-automation\n</code></pre> <ul> <li>Add the Policy to the IAM user</li> </ul> <p>We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below.</p> <pre><code>export AWS_ACCOUNT_NUMBER=\"Your AWS Account Number\"\naws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam::${AWS_ACCOUNT_NUMBER}:policy/inlets-automation\n</code></pre> <ul> <li>Generate an access key for your IAM User </li> </ul> <p>The below commands will create a set of credentials and save them into files for use later on.</p> <p>we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually.</p> <pre><code>ACCESS_KEY_JSON=$(aws iam create-access-key --user-name inlets-automation)\necho $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId &gt; access-key.txt\necho $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey &gt; secret-key.txt\n</code></pre> <ul> <li>Create an exit-server:</li> </ul> <pre><code>inletsctl create \\\n  --provider ec2 \\\n  --region eu-west-1 \\\n  --access-token-file ./access-key.txt \\\n  --secret-key-file ./secret-key.txt\n</code></pre> <ul> <li>Delete an exit-server:</li> </ul> <pre><code>export IP=\"\"\n\ninletsctl create \\\n  --provider ec2 \\\n  --region eu-west-1 \\\n  --access-token-file ./access-key.txt \\\n  --secret-key-file ./secret-key.txt \\\n  --ip $IP\n</code></pre>"},{"location":"reference/inletsctl/#example-usage-with-aws-ec2-temporary-credentials","title":"Example usage with AWS EC2 Temporary Credentials","text":"<p>To use the instructions below you must have the AWS CLI configured with sufficient permissions to  create users and roles. </p> <p>The following instructions use <code>get-session-token</code> to illustrate the concept.  However, it is expected that real world usage would more likely make use of <code>assume-role</code> to obtain temporary credentials.</p> <ul> <li>Create a AWS IAM Policy with the following:</li> </ul> <p>Create a file named <code>policy.json</code> with the following content</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [  \n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeImages\",\n                \"ec2:TerminateInstances\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateTags\",\n                \"ec2:DeleteSecurityGroup\",\n                \"ec2:RunInstances\",\n                \"ec2:DescribeInstanceStatus\"\n            ],\n            \"Resource\": [\"*\"]\n        }\n    ]\n}\n</code></pre> <ul> <li>Create the policy in AWS </li> </ul> <pre><code>aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json\n</code></pre> <ul> <li>Create an IAM user</li> </ul> <pre><code>aws iam create-user --user-name inlets-automation\n</code></pre> <ul> <li>Add the Policy to the IAM user</li> </ul> <p>We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below.</p> <pre><code>export AWS_ACCOUNT_NUMBER=\"Your AWS Account Number\"\naws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam::${AWS_ACCOUNT_NUMBER}:policy/inlets-automation\n</code></pre> <ul> <li>Generate an access key for your IAM User </li> </ul> <p>The below commands will create a set of credentials and save them into files for use later on.</p> <p>we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually.</p> <pre><code>ACCESS_KEY_JSON=$(aws iam create-access-key --user-name inlets-automation)\nexport AWS_ACCESS_KEY_ID=$(echo $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId)\nexport AWS_SECRET_ACCESS_KEY=$(echo $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey)\n</code></pre> <ul> <li>Check that calls are now being executed by the <code>inlets-automation</code> IAM User.</li> </ul> <pre><code>aws sts get-caller-identity\n</code></pre> <ul> <li>Ask STS for some temporary credentials</li> </ul> <pre><code>TEMP_CREDS=$(aws sts get-session-token)\n</code></pre> <ul> <li>Break out the required elements</li> </ul> <pre><code>echo $TEMP_CREDS | jq -r .Credentials.AccessKeyId &gt; access-key.txt    \necho $TEMP_CREDS | jq -r .Credentials.SecretAccessKey &gt; secret-key.txt\necho $TEMP_CREDS | jq -r .Credentials.SessionToken &gt; session-token.txt\n</code></pre> <ul> <li>Create an exit-server using temporary credentials:</li> </ul> <pre><code>inletsctl create \\\n  --provider ec2 \\\n  --region eu-west-1 \\\n  --access-token-file ./access-key.txt \\\n  --secret-key-file ./secret-key.txt \\\n  --session-token-file ./session-token.txt\n</code></pre> <ul> <li>Delete an exit-server using temporary credentials:</li> </ul> <pre><code>export INSTANCEID=\"\"\n\ninletsctl delete \\\n  --provider ec2 \\\n  --id $INSTANCEID\n  --access-token-file ./access-key.txt \\\n  --secret-key-file ./secret-key.txt \\\n  --session-token-file ./session-token.txt\n</code></pre>"},{"location":"reference/inletsctl/#example-usage-with-google-compute-engine","title":"Example usage with Google Compute Engine","text":"<p>Bear in mind that standard GCE VMs are created with an ephemeral IP address, which is subject to change. In order to make your tunnel's address stable, you should Reserve a static IP address and assign it to your VM. A static IP costs around 2.88 USD / mo.</p> <ul> <li>One time setup required for a service account key</li> </ul> <p>It is assumed that you have gcloud installed and configured on your machine. If not, then follow the instructions here</p> <pre><code># Get current projectID\nexport PROJECTID=$(gcloud config get-value core/project 2&gt;/dev/null)\n\n# Create a service account\ngcloud iam service-accounts create inlets \\\n--description \"inlets-operator service account\" \\\n--display-name \"inlets\"\n\n# Get service account email\nexport SERVICEACCOUNT=$(gcloud iam service-accounts list | grep inlets | awk '{print $2}')\n\n# Assign appropriate roles to inlets service account\ngcloud projects add-iam-policy-binding $PROJECTID \\\n--member serviceAccount:$SERVICEACCOUNT \\\n--role roles/compute.admin\n\ngcloud projects add-iam-policy-binding $PROJECTID \\\n--member serviceAccount:$SERVICEACCOUNT \\\n--role roles/iam.serviceAccountUser\n\n# Create inlets service account key file\ngcloud iam service-accounts keys create key.json \\\n--iam-account $SERVICEACCOUNT\n</code></pre> <ul> <li>Create a tunnel using the service account and project ID</li> </ul> <pre><code># Create a TCP tunnel server\ninletsctl create \\\n  --provider gce \\\n  --project-id=$PROJECTID \\\n  --access-token-file=key.json \\\n  --tcp\n\n# Create a HTTP / HTTPS tunnel server\ninletsctl create \\\n  -p gce \\\n  --project-id=$PROJECTID \\\n  -f=key.json\n\n# Or specify any valid Google Cloud Zone optional zone, by default it get provisioned in us-central1-a\ninletsctl create -p gce \\\n  --project-id=$PROJECTID \\\n  -f key.json \\\n  --zone=us-central1-a\n</code></pre> <p>If you need the tunnel server for any period of time, remember to Reserve a static IP address and assign it to your VM.</p> <p>Then SSH into the host and make sure you update inlets to make use of it:</p> <p>Edit <code>IP=</code> in <code>/etc/default/inlets-pro</code> then run <code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart inlets-pro</code></p> <p>The <code>inlets-pro http/tcp --url wss://...</code> flag should also be updated with the static IP.</p> <p>In a future version of inletsctl, we may automate the above.</p>"},{"location":"reference/inletsctl/#example-usage-with-azure","title":"Example usage with Azure","text":"<p>Prerequisites:</p> <ul> <li>You will need <code>az</code>. See Install the Azure CLI</li> </ul> <p>Generate Azure auth file  <pre><code>SUBSCRIPTION_ID=\"YOUR_SUBSCRIPTION_ID\"\naz ad sp create-for-rbac --role Contributor --scopes \"/subscriptions/$SUBSCRIPTION_ID\" --sdk-auth \\\n  &gt; $HOME/Downloads/client_credentials.json\n</code></pre></p> <p>List Azure available regions <pre><code>az account list-locations -o table\n</code></pre></p> <p>Create <pre><code>inletsctl create --provider=azure --subscription-id=4d68ee0c-7079-48d2-b15c-f294f9b11a9e \\\n  --region=eastus --access-token-file=~/Downloads/client_credentials.json \n</code></pre></p> <p>Delete <pre><code>inletsctl delete --provider=azure --id inlets-clever-volhard8 \\\n  --subscription-id=4d68ee0c-7079-48d2-b15c-f294f9b11a9e \\\n  --region=eastus --access-token-file=~/Downloads/client_credentials.json\n</code></pre></p>"},{"location":"reference/inletsctl/#example-usage-with-hetzner","title":"Example usage with Hetzner","text":"<pre><code># Obtain the API token from Hetzner Cloud Console.\nexport TOKEN=\"\"\n\ninletsctl create --provider hetzner \\\n  --access-token $TOKEN \\\n  --region hel1\n</code></pre> <p>Available regions are <code>hel1</code> (Helsinki), <code>nur1</code> (Nuremberg), <code>fsn1</code> (Falkenstein).</p>"},{"location":"reference/inletsctl/#example-usage-with-linode","title":"Example usage with Linode","text":"<p>Prerequisites:</p> <ul> <li>Prepare a Linode API Access Token. See Create Linode API Access token </li> </ul> <p>Create <pre><code>inletsctl create --provider=linode --access-token=&lt;API Access Token&gt; --region=us-east\n</code></pre></p> <p>Delete <pre><code>inletsctl delete --provider=linode --access-token=&lt;API Access Token&gt; --id &lt;instance id&gt;\n</code></pre></p>"},{"location":"reference/inletsctl/#example-usage-with-scaleway","title":"Example usage with Scaleway","text":"<pre><code># Obtain from your Scaleway dashboard:\nexport TOKEN=\"\"\nexport SECRET_KEY=\"\"\nexport ORG_ID=\"\"\n\ninletsctl create --provider scaleway \\\n  --access-token $TOKEN\n  --secret-key $SECRET_KEY --organisation-id $ORG_ID\n</code></pre> <p>The region is hard-coded to France / Paris 1.</p>"},{"location":"reference/inletsctl/#example-usage-with-ovhcloud","title":"Example usage with OVHcloud","text":"<p>You need to create API keys for the ovhCloud country/continent you're going to deploy with inletsctl.  For an overview of available endpoint check supported-apis documentation</p> <p>For, example, Europe visit https://eu.api.ovh.com/createToken to create your API keys.</p> <p>However, the specific value for the endpoint flag are following:</p> <ul> <li><code>ovh-eu</code> for OVH Europe API</li> <li><code>ovh-us</code> for OVH US API</li> <li><code>ovh-ca</code> for OVH Canada API</li> <li><code>soyoustart-eu</code> for So you Start Europe API</li> <li><code>soyoustart-ca</code> for So you Start Canada API</li> <li><code>kimsufi-eu</code> for Kimsufi Europe API</li> <li><code>kimsufi-ca</code> for Kimsufi Canada API</li> </ul> <p><code>ovh-eu</code> is the default endpoint and <code>DE1</code> the default region. </p> <p>For the proper <code>rights</code> choose all HTTP Verbs (GET,PUT,DELETE, POST), and we need only the <code>/cloud/</code> API.</p> <pre><code>export APPLICATION_KEY=\"\"\nexport APPLICATION_SECRET=\"\"\nexport CONSUMER_KEY=\"\"\nexport ENDPOINT=\"\"\nexport PROJECT_ID=\"\"\n\ninletsctl create --provider ovh \\\n  --access-token $APPLICATION_KEY \\\n  --secret-key $APPLICATION_SECRET \n  --consumer-key $CONSUMER_KEY \\ \n  --project-id $SERVICENAME \\\n  --endpoint $ENDPOINT\n</code></pre>"},{"location":"reference/inletsctl/#the-delete-command","title":"The <code>delete</code> command","text":"<p>The delete command takes an id or IP address which are given to you at the end of the <code>inletsctl create</code> command. You'll also need to specify your cloud access token.</p> <pre><code>inletsctl delete \\\n  --provider digitalocean \\\n  --access-token-file ~/Downloads/do-access-token \\\n  --id 164857028 \\\n</code></pre> <p>Or delete via IP:</p> <pre><code>inletsctl delete \\\n  --provider digitalocean \\\n  --access-token-file ~/Downloads/do-access-token \\\n  --ip 209.97.131.180 \\\n</code></pre>"},{"location":"reference/inletsctl/#kfwd-kubernetes-service-forwarding","title":"kfwd - Kubernetes service forwarding","text":"<p>kfwd runs an inlets-pro server on your local computer, then deploys an inlets client in your Kubernetes cluster using a Pod. This enables your local computer to access services from within the cluster as if they were running on your laptop.</p> <p>inlets Pro allows you to access any TCP service within the cluster, using an encrypted link:</p> <p>Forward the <code>figlet</code> pod from <code>openfaas-fn</code> on port <code>8080</code>:</p> <pre><code>inletsctl kfwd \\\n  --pro \\\n  --license $(cat ~/LICENSE)\n  --from figlet:8080 \\\n  --namespace openfaas-fn \\\n  --if 192.168.0.14\n</code></pre> <p>Note the <code>if</code> parameter is the IP address of your local computer, this must be reachable from the Kubernetes cluster.</p> <p>Then access the service via <code>http://127.0.0.1:8080</code>.</p>"},{"location":"reference/inletsctl/#troubleshooting","title":"Troubleshooting","text":"<p>inletsctl provisions a host called an exit node or exit server using public cloud APIs. It then  prints out a connection string.</p> <p>Are you unable to connect your client to the exit server?</p>"},{"location":"reference/inletsctl/#troubleshooting-inlets-pro","title":"Troubleshooting inlets Pro","text":"<p>If using auto-tls (the default), check that port 8123 is accessible. It should be serving a file with a self-signed certificate, run the following:</p> <pre><code>export IP=192.168.0.1\ncurl -k https://$IP:8123/.well-known/ca.crt\n</code></pre> <p>If you see connection refused, log in to the host over SSH and check the service via systemctl:</p> <pre><code>sudo systemctl status inlets-pro\n\n# Check its logs\nsudo journalctl -u inlets-pro\n</code></pre> <p>You can also check the configuration in <code>/etc/default/inlets-pro</code>, to make sure that an IP address and token are configured.</p>"},{"location":"reference/inletsctl/#configuration-using-environment-variables","title":"Configuration using environment variables","text":"<p>You may want to set an environment variable that points at your <code>access-token-file</code> or <code>secret-key-file</code></p> <p>Inlets will look for the following:</p> <pre><code># For providers that use --access-token-file\nINLETS_ACCESS_TOKEN\n\n# For providers that use --secret-key-file\nINLETS_SECRET_KEY\n</code></pre> <p>With the correct one of these set you wont need to add the flag on every command execution. </p> <p>You can set the following syntax in your <code>bashrc</code> (or equivalent for your shell)</p> <pre><code>export INLETS_ACCESS_TOKEN=$(cat my-token.txt)\n\n# or set the INLETS_SECRET_KEY for those providors that use this\nexport INLETS_SECRET_KEY=$(cat my-token.txt)\n</code></pre>"},{"location":"tutorial/automated-http-server/","title":"Create a tunnel server with inletsctl","text":""},{"location":"tutorial/automated-http-server/#automate-a-http-tunnel-server","title":"Automate a HTTP tunnel server","text":"<p>Learn how to serve traffic from your private network over a private tunnel server.</p> <p>At the end of this tutorial, you'll have a a secure TLS public endpoint using your own DNS and domain, which you can use to access your internal services or webpages.</p> <p>I'll show you how to:</p> <ul> <li>automate a tunnel server on a public cloud provider with inlets pre-loaded onto it, </li> <li>how to connect a client from your home or private network</li> <li>how to tunnel one or more services</li> <li>and what else you can do </li> </ul> <p>In a previous article, I explained some of the differences between SaaS and private tunnel servers.</p>"},{"location":"tutorial/automated-http-server/#create-your-tunnel-server","title":"Create your tunnel server","text":"<p>With SaaS tunnels, your tunnels server processes run on shared servers with other users. With a private tunnel server like inlets, you need to create a server somewhere on the Internet to run the tunnel. It should be created with a public IP address that you can use to accept traffic and proxy it into your private network.</p> <p></p> <p>Pictured: Inlets Conceptual architecture</p> <p>The simplest way to do this is to use the inletsctl tool, which supports around a dozen clouds. The alternative is to set up a VPS or install inlets-pro onto a server you already have set up, and then add a systemd unit file so that it restarts if the tunnel or server should crash for any reason.</p> <p>To see a list of supported clouds run:</p> <pre><code>inletsctl create --help\n</code></pre> <p>For instructions on how to create an API key or service account for each, feel free to browse the docs.</p> <pre><code>inletsctl create \\\n --region lon1 \\\n --provider digitalocean \\\n --access-token-file ~/digital-ocean-api-key.txt \\\n --letsencrypt-domain blog.example.com\n</code></pre> <p>A VM will be created in your account using the cheapest plan available, for DigitalOcean this costs 5 USD / mo at time of writing.</p> <p>You can also run your tunnel server in the free tier of GCP, Oracle Cloud or on Fly.io at no additional cost.</p> <p>Once the tunnel server has been created, you will receive:</p> <ul> <li>The IP address</li> <li>An endpoint for the inlets client to connect to</li> <li>A token for the inlets client to use when connecting</li> </ul> <p>Take a note of these.</p> <p>Now create a DNS \"A\" record for the IP address of the tunnel server on your domain control panel.</p> <p>Personally, I'm a fan of Google Domains and the .dev domains, but DigitalOcean can also manage domains through their CLI:</p> <pre><code>export IP=\"\"\nexport SUBDOMAIN=\"blog.example.com\"\n\ndoctl compute domain create $SUBDOMAIN \\\n  --ip-address $IP\n</code></pre> <p>How does the TLS encryption work?</p> <p>The inlets server process will attempt to get a TLS certificate from Let's Encrypt using a HTTP01 Acme challenge.</p> <p>What if I have multiple sites?</p> <p>You can pass a number of sub-domains, for instance:</p> <pre><code> --letsencrypt-domain blog.example.com,grafana.example.com\n</code></pre>"},{"location":"tutorial/automated-http-server/#connect-your-tunnel-client","title":"Connect your tunnel client","text":"<p>The tunnel client can be run as and when required, or you can generate a systemd unit file so that you can have it running in the background. You can run the tunnel on the same machine as the service that you're proxying, or you can run it on another computer. It's entirely up to you.</p> <p>So you could have a Raspberry Pi which just runs Raspberry Pi OS Lite and an inlets client, and nothing else. In this way you're creating a kind of router appliance.</p> <p>Let's imagine you've run a Node.js express service on your computer:</p> <pre><code>$ git clone https://github.com/alexellis/alexellis.io \\\n  --depth=1\n$ cd alexellis.io/\n$ npm install\n$ npm start\n\nalexellis.io started on port: http://0.0.0.0:3000\n</code></pre> <p>inlets also has its own built-in file-server with password protection and the ability to disable browsing for sharing private links. You can expose the built-in file-server when you want to share files directly, without having to upload them first: The simple way to share files directly from your computer</p> <p>You can download the inlets client using the inletsctl tool:</p> <pre><code>$ sudo inletsctl download\n</code></pre> <p>Now you can start the tunnel client and start serving a test version of my personal homepage <code>alexellis.io</code>:</p> <pre><code>$ export URL=\"\"\n$ export TOKEN=\"\"\n\n$ inlets-pro http client \\\n  --url $URL \\\n  --token $TOKEN \\\n  --upstream blog.example.com=http://127.0.0.1:3000\n</code></pre> <p>What if my services are running on different computers?</p> <p>If they are all within the same network, then you can run the client in one place and have it point at the various internal IP addresses.</p> <pre><code>$ inlets-pro http client \\\n  --url $URL \\\n  --token $TOKEN \\\n  --upstream blog.example.com=http://127.0.0.1:3000 \\\n  --upstream grafana.example.com=http://192.168.0.100:3000\n</code></pre> <p>If they are on different networks, you can simply run multiple clients, just change the <code>--upstream</code> flag on each client.</p> <p>How can I run the client in the background?</p> <p>For Linux hosts, you can generate a systemd unit file for inlets by using the <code>--generate systemd</code> flag to the client or server command.</p> <p>Then simply copy the resulting file to the correct location on your system and install it:</p> <pre><code>$ export URL=\"\"\n$ export TOKEN=\"\"\n\n$ inlets-pro http client \\\n  --url $URL \\\n  --token $TOKEN \\\n  --upstream blog.example.com=http://127.0.0.1:3000 \\\n  --generate=systemd &gt; inlets.service\n\n$ sudo cp inlets.service /etc/systemd/system/\n$ sudo systemctl enable inlets\n</code></pre> <p>You can then check the logs or service status:</p> <pre><code>$ sudo journalctl -u inlets\n$ sudo systemctl status inlets\n</code></pre>"},{"location":"tutorial/automated-http-server/#access-your-website-over-the-tunnel","title":"Access your website over the tunnel","text":"<p>You can now access your local website being served at http://127.0.0.1:3000 over the tunnel by visiting the domain you created:</p> <p>https://blog.example.com/</p>"},{"location":"tutorial/automated-http-server/#your-ip-goes-where-you-go","title":"Your IP goes where you go","text":"<p>You can close the lid on your laptop, and open it again in Starbucks or your favourite local independent coffee shop. As soon as you reconnect the client, your local server will be available over the tunnel at the same IP address and domain: https://blog.example.com/</p> <p>I used this technique to test a live demo for the KubeCon conference. I then took a flight from London to San Diego and was able to receive traffic to my Raspberry Pi whilst tethering on a local SIM card.</p> <p></p> <p>Tethering my Raspberry Pi with K3s in San Diego</p>"},{"location":"tutorial/automated-http-server/#wrapping-up","title":"Wrapping up","text":"<p>In a very short period of time we created a private tunnel server on a public cloud of our choice, then we created a DNS record for it, and connected a client and accessed our local website.</p> <p>You can get started with inlets through a monthly subscription.</p> <p>When would you need this?</p> <ul> <li>If you're self-hosting websites, you already have some equipment at home, so it can work out cheaper.</li> <li>If you're running a Kubernetes cluster or K3s on a Raspberry Pi, it can be much cheaper over the course of a year.</li> <li>But it's also incredibly convenient for sharing files and for testing APIs or OAuth flows during development.</li> </ul> <p>Ben Potter at Coder is writing up a tutorial on how to access a private VSCode server from anywhere using a private tunnel. If you would like to learn more, follow @inletsdev for when it gets published.</p> <p></p> <p>Andrew Meier put it this way:</p> <p>\"I prefer to play around with different projects without having to worry about my costs skyrocketing. I had a few Raspberry Pis and wondered if I could use them as a cluster. After a bit of searching #k3s and  inlets gave me my answer\"</p> <p></p> <p>Andrew's K3s cluster, with inlets</p> <p>Read his blog post: Personal Infrastructure with Inlets, k3s, and Pulumi</p>"},{"location":"tutorial/automated-http-server/#you-may-also-like","title":"You may also like","text":"<ul> <li>Tunnel a service or ingress from Kubernetes</li> <li>Share a file without uploading it through inlets tunnels</li> <li>Connecting my boat to the Internet with inlets</li> </ul>"},{"location":"tutorial/automated-tcp-server/","title":"Create a tunnel server with inletsctl","text":"<p>When should you create a TCP tunnel server?</p> <p>It is advisable to only tunnel TCP services which include their own encryption such as TLS, SSH, RDP, or a Reverse Proxy. To expose a plaintext HTTP endpoint such as <code>http://localhost:3000</code>, use an HTTPS tunnel instead which will provide TLS to your end-users.</p> <p>TCP tunnel servers can be set up manually or automatically with inletsctl.</p> <p>This page shows the steps needed to create a tunnel server via inletsctl.</p> <p>For a step-by-step guide on exposing a TCP service such as SSH, see Expose SSH over a TCP tunnel.</p>"},{"location":"tutorial/automated-tcp-server/#obtain-a-cloud-api-token","title":"Obtain a cloud API token","text":"<p>inletsctl provisions a new cloud VM with inlets preinstalled using cloud-init.</p> <p>You'll need to obtain an API token from your provider of choice using the inletsctl reference documentation.</p>"},{"location":"tutorial/automated-tcp-server/#create-a-tunnel-server-via-inletsctl","title":"Create a tunnel server via inletsctl","text":"<p>Once you've obtained an API token, you can create a tunnel server with the following command:</p> <pre><code>export PROVIDER=\"\"\nexport REGION=\"\"\nexport ACCESS_TOKEN_FILE_PATH=\"\"\n\ninletsctl create \\\n    --provider $PROVIDER \\\n    --access-token-file $ACCESS_TOKEN_FILE_PATH \\\n    --region $REGION \\\n    --tcp\n</code></pre> <p>This will create a new VM in the London region and install inlets-pro on it.</p> <p>The command will output a sample command for the <code>inlets-pro client</code> command:</p>"},{"location":"tutorial/automated-tcp-server/#run-the-tunnel-client","title":"Run the tunnel client","text":"<p>The <code>inletsctl create</code> command will output a sample command for the <code>inlets-pro client</code> command.</p> <p>To expose SSH from localhost, add:</p> <pre><code> --upstream 127.0.0.1 \\\n --port 2222\n</code></pre> <p>To expose SSH from a remote machine on your local network i.e. <code>192.168.1.20</code>, add:</p> <pre><code> --upstream 192.168.1.20 \\\n --port 2222\n</code></pre> <p>To expose ports 80 and 443 from a machine where you have a reverse proxy running such as Caddy, add:</p> <pre><code> --upstream 192.168.1.20 \\\n --port 80 \\\n --port 443\n</code></pre>"},{"location":"tutorial/caddy-http-tunnel/","title":"Expose Caddy from your private network","text":""},{"location":"tutorial/caddy-http-tunnel/#custom-reverse-proxy-with-caddy","title":"Custom reverse proxy with Caddy","text":"<p>In this tutorial we'll set up an inlets TCP tunnel server to forward ports 80 and 443 to a reverse proxy server running on our local machine. Caddy will receive a TCP stream from the public tunnel server for ports 80 and 443. It can terminate TLS and also allow you to host multiple sites with ease.</p> <p>Caddy is a free and open-source reverse proxy. It's often used on web-servers to add TLS to one or more virtual websites.</p>"},{"location":"tutorial/caddy-http-tunnel/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A Linux server, Windows and MacOS are also supported</li> <li>The inlets-pro binary at /usr/local/bin/</li> <li>Access to a DNS control plane for a domain you control</li> </ul> <p>You can run through the same instructions with other reverse proxies such as Nginx, or Traefik.</p> <p>Scenario: * You want to share a file such as a VM image or a ISO over the Internet, with HTTPS, directly from your laptop. * You have one or more websites or APIs running on-premises or within your home-lab and want to expose them on the Internet.</p> <p>Get your inlets subscription here</p>"},{"location":"tutorial/caddy-http-tunnel/#setup-your-exit-node","title":"Setup your exit node","text":"<p>Provision a cloud VM on DigitalOcean or another IaaS provider using inletsctl:</p> <pre><code>inletsctl create \\\n --provider digitalocean \\\n --region lon1 \\\n --pro\n</code></pre> <p>Note the <code>--url</code> and <code>TOKEN</code> given to you in this step.</p>"},{"location":"tutorial/caddy-http-tunnel/#setup-your-dns-a-record","title":"Setup your DNS A record","text":"<p>Setup a DNS A record for the site you want to expose using the public IP of the cloud VM</p> <ul> <li><code>178.128.40.109</code> = <code>service.example.com</code></li> </ul>"},{"location":"tutorial/caddy-http-tunnel/#run-a-local-server-to-share-files","title":"Run a local server to share files","text":"<p>Do not run this command in your home folder, as it will expose your entire home directory.</p> <p>Instead, create a temporary directory and serve that instead:</p> <pre><code>mkdir -p /tmp/shared/\ncd /tmp/shared/\n\necho \"Hello world\" &gt; WELCOME.txt\n\ninlets-pro fileserver --webroot ./ \\\n  --allow-browsing\n</code></pre> <p>The command listens on port <code>8080</code> by default, but you can change is as desired with <code>--port</code></p> <p>The <code>--allow-browsing</code> flag allows directory listing and traversal through the browser.</p> <p>If you're sharing files with a colleague or friend, you can add <code>--allow-browsing=false</code> and share the exact URL with them instead.</p>"},{"location":"tutorial/caddy-http-tunnel/#start-the-inlets-pro-client-on-your-local-side","title":"Start the inlets-pro client on your local side","text":"<p>Downloads the inlets Pro client:</p> <pre><code>sudo inletsctl download\n</code></pre> <p>Run the inlets-pro client, using the TOKEN and IP given to you from the previous step.</p> <p>The client will look for your license in <code>$HOME/.inlets/LICENSE</code>, but you can also use the <code>--license/--license-file</code> flag if you wish.</p> <pre><code>export IP=\"\"        # take this from the exit-server\nexport TOKEN=\"\"     # take this from the exit-server\n\ninlets-pro tcp client \\\n  --url wss://$IP:8123/connect \\\n  --ports 80,443 \\\n  --token $TOKEN \\\n  --upstream localhost\n</code></pre> <p>Note that <code>--upstream localhost</code> will connect to Caddy running on your computer, if you are running Caddy on another machine, use its IP address here.</p>"},{"location":"tutorial/caddy-http-tunnel/#setup-caddy-2x","title":"Setup Caddy 2.x","text":"<p>Here's an example Caddyfile that will reverse-proxy to the local file-server using the domain name <code>service.example.com</code>:</p> <pre><code>{\n  acme_ca https://acme-staging-v02.api.letsencrypt.org/directory\n}\n\nservice.example.com\n\nreverse_proxy 127.0.0.1:8080 {\n}\n</code></pre> <p>Note the <code>acme_ca</code> being used will receive a staging certificate, remove it to obtain a production TLS certificate.</p> <p>Now download Caddy 2.x for your operating system. You can get it from the downloads page, or if you're a Linux user on an amd64 or arm64 machine, you can use arkade to do everything required via <code>arkade system install caddy</code>. See <code>arkade system install --help</code> for more options.</p> <p>Once you have the binary, you can run it with the following command:</p> <pre><code>sudo ./caddy run \\\n  -config ./Caddyfile\n</code></pre> <p><code>sudo</code> - is required to bind to port 80 and 443, although you can potentially update your OS to allow binding to low ports without root access. See this StackOverflow question for more.</p> <p>You should now be able to access the fileserver via the <code>https://service.example.com</code> URL.</p> <p>If you wanted to expose something else like Grafana, you could simply edit your Caddyfile's <code>reverse_proxy</code> line, then restart Caddy.</p> <p>Caddy also supports multiple domains within the same file, so that you can expose multiple internal or private websites through the same tunnel.</p> <pre><code>{\n  email \"webmaster@example.com\"\n}\n\nblog.example.com {\n  reverse_proxy 127.0.0.1:4000\n}\n\nopenfaas.example.com {\n      reverse_proxy 127.0.0.1:8080\n}\n</code></pre> <p>If you have services running on other machines you can change <code>127.0.0.1:8080</code> to a different IP address such as that of your Raspberry Pi if you had something like OpenFaaS CE or faasd CE running there.</p>"},{"location":"tutorial/caddy-http-tunnel/#check-it-all-worked","title":"Check it all worked","text":"<p>You'll see that Caddy can now obtain a TLS certificate.</p> <p>Go ahead and visit: <code>https://service.example.com</code></p> <p>Congratulations, you've now served a TLS certificate directly from your laptop. You can close caddy and open it again at a later date. Caddy will re-use the certificate it already obtained and it will be valid for 3 months. To renew, just keep Caddy running or open it again whenever you need it.</p>"},{"location":"tutorial/community/","title":"Community tutorials and guides","text":"<p>Note: Any material not hosted on <code>inlets.dev</code> may be written by a third-party.</p> <p>If you have a tutorial or video to submit, feel free to send a Pull Request</p>"},{"location":"tutorial/community/#case-studies","title":"Case studies","text":"<p>You can read testimonials on the main homepage</p> <ul> <li>Connecting my boat to the Internet with inlets by Mark Sharpley</li> <li>How Riskfuel is using Inlets to build machine learning models at scale by Addison van den Hoeven</li> <li>Ingress to ECS Anywhere, from anywhere, using Inlets by Nathan Peck</li> <li>Reliable local port-forwarding from Kubernetes for a Developer at UK Gov</li> </ul>"},{"location":"tutorial/community/#videos","title":"Videos","text":"<p>Webinars:</p> <ul> <li>A tale of two networks - demos and use-cases for inlets tunnels (Mar 2021) by Alex Ellis and Johan Siebens</li> <li>Crossing network boundaries with Kubernetes and inlets (Mar 2021) by Alex Ellis and Johan Siebens</li> </ul> <p>Walk-through videos:</p> <ul> <li>inlets-operator - Get Ingress and Public IPs for private Kubernetes (Mar 2020) by Alex Ellis</li> <li>Inlets Operator - get a LoadBalancer from any Kubernetes cluster (Oct 2019) by Alex Ellis</li> <li>Hacking on the Inlets Operator for Equinix Metal (Jul 2021) by Alex Ellis and David McKay</li> </ul>"},{"location":"tutorial/community/#tutorials","title":"Tutorials","text":"<ul> <li>A Tour of Inlets - A Tunnel Built for the Cloud (Aug 2021) by Zespre Schmidt</li> <li>Control Access to your on-prem services with Cloud IAP and inlets Pro (Dec 2020) by Johan Siebens</li> <li>Secure access using HashiCorp Boundary &amp; inlets Pro Better Together (Oct 2020) by Johan Siebens</li> <li>Quake III Arena, k3s and a Raspberry Pi (Nov 2020) by Johan Siebens</li> <li>Argo CD for your private Raspberry Pi k3s cluster (Aug 2020) by Johan Siebens</li> <li>Get a TLS-enabled Docker registry in 5 minutes (Feb 2020) by Alex Ellis</li> <li>A bit of Istio before tea-time (May 2021) by Alex Ellis</li> <li>Access your local cluster like a managed Kubernetes engine by Alex Ellis</li> <li>Exploring Kubernetes Operator Pattern (Jan 2021) by Ivan Velichko</li> </ul>"},{"location":"tutorial/community/#official-blog-posts","title":"Official blog posts","text":"<p>See inlets.dev/blog</p>"},{"location":"tutorial/dual-tunnels/","title":"Enable dual TCP and HTTPs tunnels","text":""},{"location":"tutorial/dual-tunnels/#setting-up-dual-tcp-and-https-tunnels","title":"Setting up dual TCP and HTTPS tunnels","text":"<p>In this tutorial we will set both a dual tunnel for exposing HTTP and TCP services from the same server. </p> <p>Whilst it's easier to automate two separate servers or cloud instances for your tunnels, you may want to reduce your costs.</p> <p>The use-case may be that you have a number of OpenFaaS functions running on your Raspberry Pi which serve traffic to users, but you also want to connect via SSH and VNC.</p>"},{"location":"tutorial/dual-tunnels/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A Linux server, Windows and MacOS are also supported</li> <li>The inlets-pro binary at /usr/local/bin/</li> <li>Access to a DNS control plane for a domain you control</li> </ul>"},{"location":"tutorial/dual-tunnels/#create-the-https-tunnel-server-first","title":"Create the HTTPS tunnel server first","text":"<p>Create a HTTPS tunnel server using the manual tutorial or automated tutorial.</p> <p>Once it's running, check you can connect to it, and then log in with SSH.</p> <p>You'll find a systemd service named <code>inlets-pro</code> running the HTTPS tunnel with a specific authentication token and set of parameters.</p> <p>Now, generate a new systemd unit file for the TCP tunnel.</p> <p>I would suggest generating a new token for this tunnel.</p> <pre><code>TOKEN=\"$(head -c 32 /dev/urandom | base64 | cut -d \"-\" -f1)\"\n\n# Find the instance's public IPv4 address:\nPUBLIC_IP=\"$(curl -s https://checkip.amazonaws.com)\"\n</code></pre> <p>Let's imagine the public IP resolved to <code>46.101.128.5</code> which is part of the DigitalOcean range.</p> <pre><code>inlets-pro tcp server \\\n --token \"$TOKEN\" \\\n --auto-tls-san $PUBLIC_IP \\\n --generate=systemd &gt; inlets-pro-tcp.service\n</code></pre> <p>Example:</p> <pre><code>[Unit]\nDescription=inlets Pro TCP Server\nAfter=network.target\n\n[Service]\nType=simple\nRestart=always\nRestartSec=5\nStartLimitInterval=0\nExecStart=/usr/local/bin/inlets-pro tcp server --auto-tls --auto-tls-san=46.101.128.5 --control-addr=0.0.0.0 --token=\"k1wCR+2j41TXqqq/UTLJzcuzhmSJbU5NY32VqnNOnog=\" --control-port=8124 --auto-tls-path=/tmp/inlets-pro-tcp\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>We need to update the control-port for this inlets tunnel server via the <code>--control-port</code> flag. Use port 8124 since 8123 is already in use by the HTTP tunnel. Add <code>--control-port 8124</code> to the <code>ExecStart</code> line.</p> <p>We need to add a new flag so that generated TLS certificates are placed in a unique directory, and don't clash. Add <code>--auto-tls-path /tmp/inlets-pro-tcp/</code> to the same line.</p> <p>Next install the unit file with:</p> <pre><code>sudo cp inlets-pro-tcp.service /etc/systemd/system/\nsudo systemctl daemon-reload\nsudo systemctl enable inlets-pro-tcp.service\n\nsudo systemctl restart inlets-pro-tcp.service\n</code></pre> <p>You'll now be able to check the logs for the server:</p> <pre><code>sudo journalctl -u inlets-pro-tcp\n</code></pre> <p>Finally you can connect your TCP client:</p> <pre><code>inlets-pro tcp client \\\n  --token \"k1wCR+2j41TXqqq/UTLJzcuzhmSJbU5NY32VqnNOnog=\" \\\n  --upstream 192.168.0.15 \\\n  --ports 2222,5900 \\\n  --url wss://46.101.128.5:8124\n</code></pre> <p>Note that <code>5900</code> is the default port for VNC. Port <code>2222</code> was used for SSH as not to conflict with the version running on the tunnel server.</p> <p>You can now connect to the public IP of your server via SSH and VNC:</p> <p>For example:</p> <pre><code>ssh -p 2222 pi@46.101.128.5\n</code></pre>"},{"location":"tutorial/dual-tunnels/#wrapping-up","title":"Wrapping up","text":"<p>You now have a TCP and HTTPS tunnel server running on the same host. This was made possibly by changing the control-plane port and auto-TLS path for the second server, and having it start automatically through a separate systemd service.</p> <p>This technique may save you a few dollars per month, but it may not be worth your time compared to how quick and easy it is to create two separate servers with <code>inletsctl create</code>.</p>"},{"location":"tutorial/http-authentication/","title":"Authenticate HTTPs tunnels","text":""},{"location":"tutorial/http-authentication/#built-in-http-authentication","title":"Built-in HTTP Authentication","text":"<p>This page applies to inlets-pro 0.10.0 and onwards.</p> <p>Services exposed over HTTP tunnels can have additional authentication added to them using a mechanism built-into inlets.</p> <p>The <code>inlets-pro http</code> command provides three options:</p> <ol> <li>Basic Authentication</li> <li>Bearer Token Authentication</li> <li>OAuth</li> </ol> <p>You may also be interested in IP filtering, which is configured on the server. This restricts access to a provided set of source IPs or CIDRs. </p>"},{"location":"tutorial/http-authentication/#basic-authentication","title":"Basic Authentication","text":"<p>Basic authentication is a simple way to restrict access to your service by requiring a username and password.</p> <p>When a user visits the URL of the service in a web-browser, they will be prompted to enter a username and password.</p> <p>If they're using curl, then they can pass the username and password using the <code>--user</code> flag.</p> <pre><code>curl --user username:password https://example.com\n</code></pre> <p>Or simply pass the username and password as part of the URL:</p> <pre><code>curl https://username:password@example.com\n</code></pre> <p>The username has a default of <code>admin</code> for brevity, but can be overridden if you like:</p> <pre><code>inlets-pro http client \\\n    --basic-auth-username admin \\\n    --basic-auth-password password \\\n</code></pre> <p>You can generate a string password using the <code>openssl</code> command:</p> <pre><code>openssl rand -base64 32\n</code></pre>"},{"location":"tutorial/http-authentication/#bearer-token-authentication","title":"Bearer Token Authentication","text":"<p>If you're exposing an endpoint that does not need to be accessed via a web-browser, then you can use Bearer Token Authentication.</p> <p>This is useful for exposing endpoints that are used by mobile apps or other non-web clients.</p> <p>To enable Bearer Token Authentication, you can use the <code>--bearer-token</code> flag when starting the <code>inlets-pro http</code> command.</p> <pre><code>inlets-pro http client \\\n    --bearer-token token \\\n</code></pre> <p>Both Bearer Token and Basic Authentication can be used together by supplying both flags.</p> <pre><code>inlets-pro http client \\\n    --bearer-token token \\\n    --basic-auth-username admin \\\n    --basic-auth-password password \\\n</code></pre>"},{"location":"tutorial/http-authentication/#oauth","title":"OAuth","text":"<p>With OAuth:</p> <ul> <li>You can define access for multiple users using usernames or email addresses</li> <li>Avoid managing credentials in your application</li> <li>Use an existing well-known provider for authentication such as GitHub</li> </ul> <p>The OAuth 2.0 flow requires a web-browser, so if you anticipate mixed use, then you can combine it with Bearer Token Authentication, for headless clients.</p> <p>The tunnel client currently has three reserved paths for OAuth: - <code>/_/oauth/login</code> hosts the login page. - <code>/_/oauth/logout</code> can be used to log out. - <code>/_/oauth/callback</code> is used for the OAuth 2.0 callbacks.</p> <p>Paths prefixed with <code>/_/oauth</code> can not be used by the tunneled service.</p>"},{"location":"tutorial/http-authentication/#example-with-githubcom","title":"Example with GitHub.com","text":"<p>The example below will expose: <code>http://127.0.0.1:3000</code> using the domain name <code>tunnel.example.com</code>.</p> <p>In order to use GitHub as the OAuth provider, you need to create a new OAuth application.</p> <ol> <li>Go to GitHub Developer Settings</li> <li>Click on \"New OAuth App\"</li> <li>Enter a name for the application, for example <code>inlets-tunnel</code></li> <li>Enter the callback URL, for example <code>http://tunnel.example.com/_/oauth/callback</code></li> <li>Click on \"Register application\"</li> <li>Click \"Generate a new client secret\"</li> </ol> <p>You will be given a client ID and client secret, which you can use to authenticate with GitHub.</p> <p>We suggest saving this in a convenient location, for example: <code>~/.inlets/oauth-client-id</code> and <code>~/.inlets/oauth-client-secret</code>.</p> <pre><code>inlets-pro http client \\\n    --oauth-client-id $(cat ~/.inlets/oauth-client-id) \\\n    --oauth-client-secret $(cat ~/.inlets/oauth-client-secret) \\\n    --upstream tunnel.example.com=http://127.0.0.1:3000 \\\n    --oauth-provider github \\\n    --oauth-acl alexellis \\\n    --oauth-acl name@example.com\n</code></pre> <p>Access to the tunnel can be controlled using the <code>--oauth-acl</code> flag. Users can be filtered by username and email.</p> <p>Tunnels using a commercial inlets license can also control access based on organisation membership. Providing the flag <code>--oauth-acl=org:inlets</code> would allow all users that are a member of the inlets GitHub organisation to access the tunnel.</p> <p>Once authenticated, a cookie will be set on the domain i.e. <code>tunnel.example.com</code> and the user will be redirected back to the root URL of the service <code>/</code>.</p> <p>The duration of the cookie defaults to 1 hour, but can be extended through the <code>--oauth-cookie-ttl</code> flag i.e.</p> <pre><code>inlets-pro http client \\\n+  --oauth-cookie-ttl 24h \\\n</code></pre>"},{"location":"tutorial/http-authentication/#example-with-google","title":"Example with Google","text":"<p>To use the Google provider you need a commercial Inlets license.</p> <ol> <li>Setup a new project in the Google API console</li> <li> <p>Configure the project OAuth consent screen.</p> <p>Follow the steps to configure the OAuth consent screen.</p> <p>If you are a Google Workspace user you can make your app available to any user within your organization by registering it as an internal app.</p> </li> <li> <p>Create a new OAuth client</p> <p>Create a new OAuth client with the application type <code>Web Application</code>. Fill out the name and add the callback URL for your tunnel to the list of valid redirect URIs.</p> <p>Example of a redirect uri: <code>http://tunnel.example.com/_/oauth/callback</code>. The callback for a tunnel is always available at <code>/_/oauth/callback</code>.</p> <p></p> </li> <li> <p>Save the Client ID and Client secret in a convenient place so they can be used when connecting the tunnel.</p> </li> </ol> <p>Connect the client:</p> <pre><code>inlets-pro http client \\\n    --upstream tunnel.example.com=http://127.0.0.1:3000 \\\n    --oauth-client-id $(cat ~/.inlets/oauth-client-id) \\\n    --oauth-client-secret $(cat ~/.inlets/oauth-client-secret) \\\n    --oauth-provider google \\\n    --oauth-acl example@gmail.com\n</code></pre> <p>You can control which users are allowed to access the tunnel by providing an email address using the <code>--oauth-acl</code> flag. </p> <p>More providers will be added over time, based upon requests from users, so if you want to use Facebook, GitLab, etc, send us an email to help with prioritisation.</p>"},{"location":"tutorial/http-header-modification/","title":"HTTP Header Modification","text":"<p>This page applies to inlets-pro 0.10.0 and onwards.</p> <p>When you expose a local service over a HTTP tunnel, it may need the headers of the request or those of the response to be modified.</p> <p>The <code>inlets-pro http</code> command provides a mechanism to modify the headers of the request and response, without having to write an additional proxy to do so.</p> <ol> <li>Add a header to the request</li> <li>Add a header to the response</li> <li>Remove a header from the request</li> <li>Remove a header from the response</li> </ol> <p>Each flag is provided in the format of either: <code>Header</code> or <code>Header: Value</code>, and can be provided multiple times to work on multiple headers.</p>"},{"location":"tutorial/http-header-modification/#add-a-header-to-the-request","title":"Add a header to the request","text":"<p>To add a header to the request, you can use the <code>--request-header-add</code> flag.</p> <p>For instance, if you have a client that calls the exposed service, but needs to add a special header like <code>X-Special-Header</code> to the request, you can do the following:</p> <pre><code>inlets-pro http client \\\n    --request-header-add \"X-Special-Header: 1234567890\"\n</code></pre> <p>Multiple headers can be added at once:</p> <pre><code>inlets-pro http client \\\n    --request-header-add \"X-Special-Header: 1234567890\" \\\n    --request-header-add \"X-Another-Header: 0987654321\"\n</code></pre>"},{"location":"tutorial/http-header-modification/#add-a-header-to-the-response","title":"Add a header to the response","text":"<p>To add a header to the HTTP response from the exposed service, you can use the <code>--response-header-add</code> flag.</p> <p>For example, to add a header to the response from the server to enable CORS:</p> <pre><code>inlets-pro http client \\\n    --response-header-add \"Access-Control-Allow-Origin: *\"\n</code></pre>"},{"location":"tutorial/http-header-modification/#remove-a-header-from-the-request","title":"Remove a header from the request","text":"<p>If you do not have control over the request headers because they are being sent by a third-party service or application, you can remove the ones you do not want to send.</p> <p>Here is how you could remove the User-Agent header for privacy or security reasons:</p> <pre><code>inlets-pro http client \\\n    --request-header-remove User-Agent\n</code></pre>"},{"location":"tutorial/http-header-modification/#remove-a-header-from-the-response","title":"Remove a header from the response","text":"<p>Let's say that you're using Caddy, and want to remove the <code>X-Served-By</code> header from the request for security or privacy reasons.</p> <p>To remove a header from the request, you can use the <code>--request-header-remove</code> flag.</p> <pre><code>inlets-pro http client \\\n  --request-header-remove X-Served-By\n</code></pre>"},{"location":"tutorial/ip-filtering/","title":"IP filtering","text":"<p>HTTP and TCP tunnels support filtering of incoming requests by IP address through an allow list.</p> <p>When enabled, only requests from the specified IP addresses or CIDR ranges will be allowed to connect.</p> <p>You can read more about this feature in the announcement</p>"},{"location":"tutorial/ip-filtering/#how-to-enable-ip-filtering","title":"How to enable IP filtering","text":"<p>To enable IP filtering, you can use the <code>--allow-cidr</code> flag to the inlets-pro http or tcp server commands.</p> <p>You can log into a host created by inletsctl, or you can create a host manually.</p> <p>The below example allows requests from the IP address <code>192.168.1.1</code> and the CIDR range <code>192.168.2.0/24</code>.</p> <p>For HTTP tunnels:</p> <pre><code>inlets-pro http server \\\n    --allow-ips 192.168.1.1 \\\n    --allow-ips 192.168.1.0/24\n</code></pre> <p>For TCP tunnels:</p> <pre><code>inlets-pro tcp server \\\n    --allow-ips 192.168.1.1 \\\n    --allow-ips 192.168.1.0/24\n</code></pre>"},{"location":"tutorial/istio-gateway/","title":"Tutorial: Expose an Istio gateway with the inlets-operator","text":"<p>In this tutorial we will configure the inlets-operator to get a public IP for the Istio Ingress Gateway. This will allow you to receive HTTPS certificates via LetsEncrypt and cert-manager and access services running in your cluster on their own public domain.</p>"},{"location":"tutorial/istio-gateway/#install-arkade","title":"Install arkade","text":"<p>Arkade is a simple CLI tool that provides a quick way to install various apps and download common binaries much quicker.</p> <p>To install arkade run:</p> <pre><code>curl -sSLf https://get.arkade.dev/ | sudo sh\n</code></pre>"},{"location":"tutorial/istio-gateway/#create-a-kubernetes-cluster-with-kind","title":"Create a kubernetes cluster with kinD","text":"<p>We're going to use KinD, which runs inside a container with Docker for Mac or the Docker daemon. MacOS cannot actually run containers or Kubernetes itself, so projects like Docker for Mac create a small Linux VM and hide it away.</p> <p>Download the kind and kubectl binaries if you don't have them already:</p> <pre><code>arkade get kind\narkade get kubectl\n</code></pre> <p>Now create a cluster:</p> <pre><code>$ kind create cluster\n</code></pre> <p>The initial creation could take a few minutes, but subsequent clusters creations are much faster.</p> <pre><code>Creating cluster \"kind\" ...\n \u2713 Ensuring node image (kindest/node:v1.19.0) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6  \n \u2713 Writing configuration \ud83d\udcdc \n \u2713 Starting control-plane \ud83d\udd79\ufe0f \n \u2713 Installing CNI \ud83d\udd0c \n \u2713 Installing StorageClass \ud83d\udcbe \nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nHave a nice day! \ud83d\udc4b\n</code></pre> <pre><code>kubectl get node -o wide\n\nNAME                 STATUS     ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION     CONTAINER-RUNTIME\nkind-control-plane      Ready   master   35s   v1.18.0   172.17.0.2    &lt;none&gt;        Ubuntu 19.10   5.3.0-26-generic   containerd://1.3.2\n</code></pre> <p>The above shows one node is Ready, so we can move on and install Istio.</p>"},{"location":"tutorial/istio-gateway/#install-istio","title":"Install Istio","text":"<p>You can install Istio using the documentation site at Istio.io, but we're going to use arkade instead since it gives us a one-line install and also bundles a version of Istio configuration for constrained development environments like a KinD cluster.</p> <p>It is always possible to use the <code>--set</code> flag to override or pass in additional values for the Istio chart.</p> <pre><code>arkade install istio --help\n\nInstall istio\n\nUsage:\n  arkade install istio [flags]\n\nExamples:\n  arkade install istio --loadbalancer\n\nFlags:\n      --cpu string               Allocate CPU resource (default \"100m\")\n  -h, --help                     help for istio\n      --istio-namespace string   Namespace for the app (default \"istio-system\")\n      --memory string            Allocate Memory resource (default \"100Mi\")\n      --namespace string         Namespace for the app (default \"default\")\n      --profile string           Set istio profile (default \"default\")\n      --set stringArray          Use custom flags or override existing flags \n                                 (example --set prometheus.enabled=false)\n  -v, --version string           Specify a version of Istio (default \"1.11.4\")\n\nGlobal Flags:\n      --kubeconfig string   Local path for your kubeconfig file\n      --wait                If we should wait for the resource to be ready before returning (helm3 only, default false)\n</code></pre> <p>Install Istio:</p> <pre><code>arkade install istio\n</code></pre> <p>At the moment we don't have a public IP for the Istio gateway. The next step is te install the inlets operator so we can get one.</p> <pre><code>kubectl get -n istio-system \\\n  svc/istio-ingressgateway\n\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                      AGE\nistio-ingressgateway   LoadBalancer   10.43.92.145   &lt;pending&gt;     15021:32382/TCP,80:31487/TCP,443:31692/TCP   3m28s\n</code></pre>"},{"location":"tutorial/istio-gateway/#install-the-inlets-operator","title":"Install the inlets-operator","text":"<p>The inlets-operator lets you get public LoadBalancers on your local Kubernetes cluster. It does this by creating a VM to run an inlets tunnel server in the cloud of your choice for each LoadBalancer. It then plumbs in an inlets client to connect to it using a deployment.</p> <p>The inlets-operator can also be installed with arkade.</p> <p>Save an access token for your cloud provider as <code>$HOME/access-token</code>, in this example we're using DigitalOcean. Other providers may also need a secret token in addition to the API key.</p> <p>Your inlets license should be already saved at: <code>$HOME/.inlets/LICENSE</code>, if it's not, you can move it there or use the <code>--license-file</code> flag.</p> <pre><code>export ACCESS_TOKEN=$HOME/access-token\n\narkade install inlets-operator \\\n --provider digitalocean \\\n --region lon1 \\\n --token-file $ACCESS_TOKEN \\\n --license-file \"$HOME/.inlets/LICENSE\"\n</code></pre> <p>You can run <code>arkade install inlets-operator --help</code> to see a list of other cloud providers or take a look at the inlets-operator reference documentation.</p> <ul> <li>Set the <code>--region</code> flag as required, it's best to have low latency between your current location and where the exit-servers will be provisioned.</li> </ul> <p>Once the inlets-operator is installed we can start watching for the public IP to appear.</p> <pre><code>kubectl get -n istio-system \\\n  svc/istio-ingressgateway -w\n\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP\nistio-ingressgateway   LoadBalancer   10.106.220.170   &lt;pending&gt;\nistio-ingressgateway   LoadBalancer   10.106.220.170   165.227.237.77\n</code></pre>"},{"location":"tutorial/istio-gateway/#install-cert-manager","title":"Install cert-manager","text":"<p>Install cert-manager, which can be integrated with Istio gateways to manage TLS certificates.</p> <pre><code>arkade install cert-manager\n</code></pre>"},{"location":"tutorial/istio-gateway/#a-quick-recap","title":"A quick recap","text":"<p>This is what we have so far:</p> <ul> <li> <p>Istio</p> <p>The istio service mesh. Among other things, it comes with the istio Ingress Gateway that will get a public address via an inlets tunnel.</p> </li> <li> <p>inlets-operator</p> <p>The inlets operator provides us with a public VirtualIP for the istio Ingress Gateway</p> </li> <li> <p>cert-manager</p> <p>Integrates with Istio gateways to provide TLS certificates through the HTTP01 or DNS01 challenges from LetsEncrypt.</p> </li> </ul>"},{"location":"tutorial/istio-gateway/#deploy-an-application-and-get-a-tls-certificate","title":"Deploy an application and get a TLS certificate","text":"<p>Istio uses the Bookinfo Application as an example in their documentation. We will also use this example.</p> <p>Enable side-car injection and then deploy the BookInfo manifests:</p> <pre><code>kubectl label namespace default istio-injection=enabled\n\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/platform/kube/bookinfo.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/networking/bookinfo-gateway.yaml\n</code></pre> <p>We can verify that the book application is up and running and accessible form our local computer on local host by running: <pre><code>kubectl port-forward -n istio-system \\\n  svc/istio-ingressgateway 31380:80\n</code></pre></p> <p>Then send a request to it with <code>curl</code>:</p> <pre><code>curl -sS http://127.0.0.1:31380/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n&lt;title&gt;Simple Bookstore App&lt;/title&gt;\n</code></pre> <p>Since we set up the inlets operator in the previous step to get an external IP for the Istio ingress gateway we should now also be able to access the app using that public IP.</p> <p>Open a browser and navigate to the /productpage URL using the EXTERNAL-IP:</p> <pre><code>http://165.227.237.77/productpage\n</code></pre> <p></p> <p>TLS certificates require a domain name and DNS A or CNAME entry. You can create those in the admin panel of your provider. They should point to the external IP of the Istio Ingress gateway. We will use the <code>bookinfo.example.com</code> domain as an example.</p> <pre><code>export EMAIL=\"you@example.com\"\n\ncat &gt; issuer-prod.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: $EMAIL\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - selector: {}\n      http01:\n        ingress:\n          class: istio\nEOF\n</code></pre> <p>Note that ingress class is set to <code>class: istio</code>.</p> <p>We are using the Let's Encrypt production server which has strict limits on the API. A staging server is also available at <code>https://acme-staging-v02.api.letsencrypt.org/directory</code>. If you are creating a lot of certificates while testing a deployment it would be better to use the staging server.</p> <p>Edit <code>email</code>, then run: <code>kubectl apply -f issuer-prod.yaml</code>.</p> <p>Create a new certificate resource <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: ingress-cert\n  namespace: istio-system\nspec:\n  secretName: ingress-cert\n  commonName: bookinfo.example.com\n  dnsNames:\n  - bookinfo.example.com\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n</code></pre></p> <p>Edit the bookinfo gateway, <code>kubectl edit gateway/bookinfo-gateway</code> and reference the certificate secret in the TLS configuration under <code>credentialName</code>.</p> <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: ingress-cert # This should match the Certificate secretName\n    hosts:\n    - bookinfo.example.com\n</code></pre> <p>You can always checkout the Istio documentation for more information on how to integrate cert-manager.</p> <p>We can use curl again to access the bookinfo application this time with our custom domain and over a secure connection. Alternatively you can open the URL in your browser.</p> <pre><code>curl -sS https://bookinfo.example.com/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n&lt;title&gt;Simple Bookstore App&lt;/title&gt;\n</code></pre>"},{"location":"tutorial/istio-gateway/#wrapping-up","title":"Wrapping up","text":"<p>Through the use of the inlets-operator we were able to get a public IP for the Istio Ingress gateway. This allows you to access services on your cluster whether you are running it in an on-premises datacenter, within a VM or on your local laptop.</p> <p>There is no need to open a firewall port, set-up port-forwarding rules, configure dynamic DNS or any of the usual hacks. You will get a public IP and it will \"just work\" for any TCP traffic you may have. </p>"},{"location":"tutorial/kubernetes-api-server/","title":"Tutorial: Expose a local Kubernetes API Server","text":"<p>In this tutorial, we'll show you how to expose a local Kubernetes API Server on the Internet, so that you can access it from anywhere, just like with a managed cloud provider.</p>"},{"location":"tutorial/kubernetes-api-server/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A computer or laptop running MacOS or Linux, or Git Bash or WSL on Windows</li> <li>Docker for Mac / Docker Daemon - installed in the normal way, you probably have this already</li> <li>Kubernetes running locally with kubeadm, K3s, K3d, Minikube, KinD, Docker Desktop, etc</li> </ul>"},{"location":"tutorial/kubernetes-api-server/#the-kubernetes-cluster","title":"The Kubernetes cluster","text":"<p>By default every Kubernetes cluster has TLS enabled to encrypt any HTTP REST messages that go over its control-plane. The TLS certificate has to be bound to a certain name, sometimes called a TLS SAN.</p> <p>The certificate is usually only valid for \"kubernetes.default.svc\", and can only be accessed from within the cluster.</p> <p></p> <p>Kubernetes on tour - get access to your cluster from anywhere, without having to resort to complex tooling like VPNs.</p> <p>When a managed cloud provider provisions you a cluster, they'll add additional names into the certificate like \"customer1.lke.eu.linode.com\" which is then added to your generated kubeconfig file that you download in the dashboard.</p> <p>We have five steps run through to expose the API server:</p> <ol> <li>Create a Kubernetes cluster</li> <li>Create a VM on the public cloud with an inlets TCP server running onit</li> <li>Create a DNS entry for the public VM's IP address</li> <li>Configure a TLS SAN, if possible with a new domain name</li> <li>Set up an inlets client as a Pod to forward traffic to the Kubernetes API Server</li> </ol> <p>Once we have all this in place, we can take our existing kubeconfig file and edit the URL, so that instead of pointing at our LAN IP or localhost, it points to the domain mapped to the public VM.</p>"},{"location":"tutorial/kubernetes-api-server/#create-a-cluster","title":"Create a cluster","text":"<p>You can create a cluster on any machine by using KinD:</p> <pre><code>arkade get kind\nkind create cluster\n</code></pre> <p>If you have a Raspberry Pi or a Linux Server, you can install K3s using <code>k3sup</code>:</p> <pre><code>arkade get k3sup\n\nk3sup install --ip 192.168.1.101 --user pi\n</code></pre> <p>In either case, you'll get back a kubeconfig file.</p> <p>Here's a snippet of what I got back from running <code>k3sup install</code>:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: https://192.168.1.101:6443\n</code></pre> <p>The server field will need to be changed to the new public address later on.</p>"},{"location":"tutorial/kubernetes-api-server/#create-a-vm-on-the-public-cloud-with-an-inlets-tcp-server-running-on-it","title":"Create a VM on the public cloud with an inlets TCP server running on it","text":"<p>Just like when Linode Kubernetes Engine provisions us a domain like <code>\"customer1.lke.eu.linode.com\"</code>, we'll need our own subdomain too, so that the certificate can be issued for it.</p> <p>In order to create the DNS record, we a public IP which we will get by creating a tunnel server on our preferred cloud and in a region that's close to us.</p> <pre><code>arkade get inletsctl\n\nexport ACCESS_TOKEN=\"\" # Retreive this from your cloud dashboard\n\ninletsctl create \\\n  --provider linode \\\n  --tcp \\\n  --access-token $ACCESS_TOKEN \\\n  --region eu-west\n</code></pre> <p>Save the connection info from inletsctl into a text file for later.</p> <pre><code># Give a single value or comma-separated\nexport PORTS=\"8000\"\n\n# Where to route traffic from the inlets server\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://139.160.201.143:8123\" \\\n  --token \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" \\\n  --upstream $UPSTREAM \\\n  --ports $PORTS\n</code></pre> <p>Create a DNS subdomain for the IP address you were given:</p> <ul> <li><code>k3s.example.com</code> =&gt; <code>139.160.201.143</code></li> </ul> <p>Check that you can resolve the IP with a ping <code>ping -c 1 k3s.example.com</code></p> <p>Now check the status of the inlets server:</p> <pre><code>export TOKEN=\"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\"\n\ninlets-pro status --url \"wss://139.160.201.143:8123\" \\\n  --token \"$TOKEN\"\n</code></pre> <p>Output:</p> <pre><code>inlets server status. Version: 0.9.3 - 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514\n\nServer info:\nHostname:       localhost\nProcess uptime: 5 seconds ago\nMode:           tcp\nVersion:        0.9.3 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514\n\nNo clients connected.\n</code></pre> <p>We can now move onto the next step.</p>"},{"location":"tutorial/kubernetes-api-server/#configure-a-tls-san-if-possible-with-a-new-domain-name","title":"Configure a TLS SAN, if possible with a new domain name","text":"<p>With k3s, it's trivial to add additional TLS SAN names for the Kubernetes API Server.</p> <p>If you run the <code>k3sup install</code> command again, it'll update your configuration:</p> <pre><code>k3sup install \\\n  --ip 192.168.1.101 \\\n  --user pi \\\n  --tls-san k3s.example.com\n</code></pre> <p>You'll now have the custom domain along with the default <code>kubernetes.default.svc</code> as valid names in the generated certificate.</p> <p>If you're not running on k3s, or use a service where you cannot change the TLS SAN, then we'll show you what to do in the next step.</p>"},{"location":"tutorial/kubernetes-api-server/#update-your-kubeconfig-file-with-the-new-endpoint","title":"Update your kubeconfig file with the new endpoint","text":"<p>We need to update our kubeconfig file to point at the custom domain instead of at whatever loopback or LAN address it currently does.</p> <p>For K3s users, change the server URL:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: https://192.168.1.101:6443\n</code></pre> <p>To:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: https://k3s.example.com:443\n</code></pre> <p>For any user where you cannot regenerate the TLS certificate for the API Server, you can specify the server name in the config file:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: https://k3s.example.com:443\n    tls-server-name: kubernetes\n</code></pre> <p>For more details see: Support TLS Server Name overrides in kubeconfig file #88769</p> <p>Save the changes to your kubeconfig file.</p>"},{"location":"tutorial/kubernetes-api-server/#connect-the-tunnel","title":"Connect the tunnel","text":"<p>The tunnel acts like a router, it takes any TCP packets sent to port 6443 (k3s) or 443 (Kubernetes) and forwards them down the tunnel to the inlets client. The inlets client then looks at its own \"--upstream\" value to decide where to finally send the data.</p> <p>Save <code>inlets-k8s-api.yaml</code>:</p> <pre><code>export LICENSE=\"$(cat $HOME/.inlets/LICENSE)\"\nexport TOKEN=\"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" # populate with the token from inletsctl\nexport SERVER_IP=\"139.160.201.143\" # populate with the server IP, not the domain\n\ncat &gt; inlets-k8s-api.yaml &lt;&lt;EOF\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inlets-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inlets-client\n  template:\n    metadata:\n      labels:\n        app: inlets-client\n    spec:\n      containers:\n      - name: inlets-client\n        image: ghcr.io/inlets/inlets-pro:0.9.9\n        imagePullPolicy: IfNotPresent\n        command: [\"inlets-pro\"]\n        args:\n        - \"tcp\"\n        - \"client\"\n        - \"--url=wss://$SERVER_IP:8123\"\n        - \"--upstream=kubernetes.default.svc\"\n        - \"--port=443\"\n        - \"--port=6443\"\n        - \"--token=$TOKEN\"\n        - \"--license=$LICENSE\"\n---\nEOF\n</code></pre> <p>You'll see the tunnel client up and running and ready to receive requests:</p> <pre><code>kubectl logs deploy/inlets-client\n2022/06/24 09:51:18 Licensed to: Alex &lt;contact@openfaas.com&gt;, expires: 128 day(s)\n2022/06/24 09:51:18 Upstream server: kubernetes.default.svc, for ports: 443, 6443\ntime=\"2022/06/24 09:51:18\" level=info msg=\"Connecting to proxy\" url=\"wss://139.160.201.143:8123/connect\"\ninlets-pro TCP client. Copyright OpenFaaS Ltd 2021\ntime=\"2022/06/24 09:51:18\" level=info msg=\"Connection established\" client_id=5309466072564c1c90ce0a0bcaa22b74\n</code></pre> <p>Check the tunnel server's status to confirm the connection:</p> <pre><code>export TOKEN=\"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\"\n\ninlets-pro status --url \"wss://139.160.201.143:8123\" \\\n  --token \"$TOKEN\"\n\ninlets server status. Version: 0.9.3 - 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514\n\nServer info:\nHostname:       localhost\nProcess uptime: 15 minutes ago\nMode:           tcp\nVersion:        0.9.3 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514\n\nConnected clients:\nClient ID                        Remote Address        Connected  Upstreams\n5309466072564c1c90ce0a0bcaa22b74 192.168.1.101:16368 43 seconds kubernetes.default.svc:443, kubernetes.default.svc:6443\n</code></pre> <p>Finally prove that it's working with the new, public address:</p> <pre><code>$ kubectl cluster-info\nKubernetes control plane is running at https://k3s.example.com:443\nCoreDNS is running at https://k3s.example.com:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://k3s.example.com:443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"tutorial/kubernetes-api-server/#wrapping-up","title":"Wrapping up","text":"<p>In a relatively short period of time, with a custom domain, and a small VM, we set up a tunnel server to route traffic from the public Internet to a K3s server on an internal network.</p> <p>This gives you a similar experience to a managed public cloud Kubernetes engine, but running on your own infrastructure, or perhaps within a restrictive VPC.</p> <p>You may also like:</p> <ul> <li>Learn how to manage apps across multiple Kubernetes clusters by Johan Siebens</li> </ul> <p>If you'd like to talk to us about this tutorial, feel free to reach out for a meeting:</p> <p>Set up a meeting</p>"},{"location":"tutorial/kubernetes-generate-yaml/","title":"Generate YAML for a tunnel client","text":"<p>There are a number of ways to run the inlets-pro client in Kubernetes.</p> <p>One way is to generate YAML for a Kubernetes Deployment, then apply it to your cluster.</p>"},{"location":"tutorial/kubernetes-generate-yaml/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster</li> <li>inlets-pro binary</li> </ul>"},{"location":"tutorial/kubernetes-generate-yaml/#generate-the-yaml-for-a-http-tunnel","title":"Generate the YAML for a HTTP tunnel","text":"<p>If you have already created a HTTPS tunnel server, and want to connect a service from within your cluster to the tunnel server, then you can generate the YAML for a tunnel client.</p> <p>For instance, if you wanted to expose the OpenFaaS gateway accessible at <code>http://gateway.openfaas.svc.cluster.local:8080</code> to the Internet:</p> <pre><code>export URL=wss://43.245.192.10:8123\nexport TOKEN=\"TOKEN\"\n\ninlets-pro http client \\\n    --url $URL \\\n    --generate=k8s_yaml \\\n    --generate-name openfaas-gateway-tunnel \\\n    --upstream http://gateway.openfaas.svc.cluster.local:8080 \\\n    --token $TOKEN &gt; openfaas-gateway-tunnel.yaml\n</code></pre> <p>Three objects will be outputted in the YAML:</p> <ol> <li>A <code>Deployment</code> for the inlets-pro binary</li> <li>A <code>Secret</code> for the license key</li> <li>A <code>Service</code> for the tunnel client</li> </ol> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openfaas-gateway-tunnel.yaml\n</code></pre> <p>Based upon the name passed in <code>--generate-name</code>, the tunnel client will be named <code>openfaas-gateway-tunnel-client</code>, so you can check its logs with:</p> <pre><code>kubectl logs deploy/openfaas-gateway-tunnel-client -f\n</code></pre>"},{"location":"tutorial/kubernetes-generate-yaml/#generate-the-yaml-for-a-tcp-tunnel","title":"Generate the YAML for a TCP tunnel","text":"<p>The same commands will work for a TCP tunnel, i.e. for Ingress Nginx:</p> <pre><code>export URL=wss://43.245.192.10:8123\nexport TOKEN=\"TOKEN\"\n\ninlets-pro tcp client \\\n    --url $URL \\\n    --generate=k8s_yaml \\\n    --generate-name ingress-nginx-tunnel \\\n    --upstream ingress-nginx-controller.ingress-nginx.svc.cluster.local \\\n    --ports 80 \\\n    --ports 443 \\\n    --token $TOKEN &gt; ingress-nginx-tunnel.yaml\n</code></pre> <p>You can then apply the YAML to your cluster.</p>"},{"location":"tutorial/kubernetes-ingress/","title":"Tutorial: Expose a local IngressController with the inlets-operator","text":"<p>In this quick-start we will configure the inlets-operator to use inlets-pro in TCP mode to expose ports 80 and 443 of an Ingress Controller (ingress-nginx) so that it can receive HTTPS certificates via LetsEncrypt and cert-manager.</p> <p>The inlets-operator creates a VM for each tunnel server in the cloud of your choice, then plumbs in an inlets client to connect to it using a Deployment. There is an alternative approach that we also recommend which involves creating the tunnel server with inletsctl, followed by installing the inlets client with Helm: Fixing Ingress for short-lived local Kubernetes clusters.</p> <p>Get your inlets subscription here</p>"},{"location":"tutorial/kubernetes-ingress/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A computer or laptop running MacOS or Linux, or Git Bash or WSL on Windows</li> <li>Docker for Mac / Docker Daemon - installed in the normal way, you probably have this already</li> <li>KinD - the \"darling\" of the Kubernetes community is Kubernetes IN Docker, a small one-shot cluster that can run inside a Docker container</li> <li>arkade - arkade is an app installer that takes a helm chart and bundles it behind a simple CLI</li> </ul>"},{"location":"tutorial/kubernetes-ingress/#install-arkade","title":"Install arkade","text":"<p>You can use arkade or helm to install the various applications we are going to add to the cluster below. arkade provides an apps ecosystem that makes things much quicker.</p> <p>MacOS and Linux users:</p> <pre><code>curl -sSLf https://get.arkade.dev/ | sudo sh\n</code></pre> <p>Windows users should install Git Bash and run the above without <code>sudo</code>.</p>"},{"location":"tutorial/kubernetes-ingress/#create-a-kubernetes-cluster-with-kind","title":"Create a Kubernetes cluster with KinD","text":"<p>We're going to use KinD, which runs inside a container with Docker for Mac or the Docker daemon. MacOS cannot actually run containers or Kubernetes itself, so projects like Docker for Mac create a small Linux VM and hide it away.</p> <p>You can use an alternative to KinD if you have a preferred tool.</p> <p>Get a KinD binary release and <code>kubectl</code> (the Kubernetes CLI):</p> <pre><code>arkade get kind --version v0.9.0\narkade get kubectl --version v1.19.3\n</code></pre> <p>Now create a cluster:</p> <pre><code>$ kind create cluster\n</code></pre> <p>The initial creation could take a few minutes, but subsequent clusters creations are much faster.</p> <pre><code>Creating cluster \"kind\" ...\n \u2713 Ensuring node image (kindest/node:v1.19.0) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6  \n \u2713 Writing configuration \ud83d\udcdc \n \u2713 Starting control-plane \ud83d\udd79\ufe0f \n \u2713 Installing CNI \ud83d\udd0c \n \u2713 Installing StorageClass \ud83d\udcbe \nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nHave a nice day! \ud83d\udc4b\n</code></pre> <p>We can check that our single node is ready now:</p> <pre><code>kubectl get node -o wide\n\nNAME                 STATUS     ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION     CONTAINER-RUNTIME\nkind-control-plane      Ready   master   35s   v1.18.0   172.17.0.2    &lt;none&gt;        Ubuntu 19.10   5.3.0-26-generic   containerd://1.3.2\n</code></pre> <p>The above shows one node Ready, so we are ready to move on.</p>"},{"location":"tutorial/kubernetes-ingress/#install-the-inlets-operator","title":"Install the inlets-operator","text":"<p>Save an access token for your cloud provider as <code>$HOME/access-token</code>, in this example we're using DigitalOcean. Other providers may also need a secret token in addition to the API key.</p> <p>Your inlets license should be already saved at: <code>$HOME/.inlets/LICENSE</code>, if it's not, you can move it there or use the <code>--license-file</code> flag.</p> <pre><code>export ACCESS_TOKEN=$HOME/access-token\n\narkade install inlets-operator \\\n --provider digitalocean \\\n --region lon1 \\\n --token-file $ACCESS_TOKEN \\\n --license-file \"$HOME/.inlets/LICENSE\"\n</code></pre> <p>You can run <code>arkade install inlets-operator --help</code> to see a list of other cloud providers.</p> <ul> <li>Set the <code>--region</code> flag as required, it's best to have low latency between your current location and where the exit-servers will be provisioned.</li> </ul>"},{"location":"tutorial/kubernetes-ingress/#install-nginx-ingress","title":"Install nginx-ingress","text":"<p>This installs nginx-ingress using its Helm chart:</p> <pre><code>arkade install nginx-ingress\n</code></pre>"},{"location":"tutorial/kubernetes-ingress/#install-cert-manager","title":"Install cert-manager","text":"<p>Install cert-manager, which can obtain TLS certificates through NginxIngress.</p> <pre><code>arkade install cert-manager\n</code></pre>"},{"location":"tutorial/kubernetes-ingress/#a-quick-review","title":"A quick review","text":"<p>Here's what we have so far:</p> <ul> <li> <p>nginx-ingress</p> <p>An IngressController, Traefik or Caddy are also valid options. It comes with a Service\u00a0of type LoadBalancer that will get a public address via the tunnel</p> </li> <li> <p>inlets-operator configured to use inlets-pro in TCP mode</p> <p>Provides us with a public VirtualIP for the IngressController service.</p> </li> <li> <p>cert-manager</p> <p>Provides TLS certificates through the HTTP01 or DNS01 challenges from LetsEncrypt</p> </li> </ul>"},{"location":"tutorial/kubernetes-ingress/#deploy-an-application-and-get-a-tls-certificate","title":"Deploy an application and get a TLS certificate","text":"<p>This is the final step that shows everything working end to end.</p> <p>TLS certificates require a domain name and DNS A or CNAME entry, so let's set that up</p> <p>Find the External-IP:</p> <pre><code>kubectl get svc\n</code></pre> <p>Now create a DNS A record in your admin panel, so for example: <code>expressjs.example.com</code>.</p> <p>Now when you install a Kubernetes application with an Ingress definition, NginxIngress and cert-manager will work together to provide a TLS certificate.</p> <p>Create a staging issuer for cert-manager <code>staging-issuer.yaml</code> and make sure you edit the <code>email</code> value.</p> <pre><code>export EMAIL=\"you@example.com\"\n\ncat &gt; issuer-staging.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\n  namespace: default\nspec:\n  acme:\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    email: $EMAIL\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    solvers:\n    - selector: {}\n      http01:\n        ingress:\n          class: nginx\nEOF\n</code></pre> <p>Apply the file with <code>kubectl apply -f staging-issuer.yaml</code></p> <p>While the Let's Encrypt production server has strict limits on the API, the staging server is more forgiving, and should be used while you are testing a deployment.</p> <p>Edit <code>email</code>, then run: <code>kubectl apply -f issuer.yaml</code>.</p> <p>Let's use helm3 to install Alex's example Node.js API available on GitHub</p> <p>Create a <code>custom.yaml</code> file with the following:</p> <pre><code>ingress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/issuer: \"letsencrypt-staging\"\n  hosts:\n    - host: expressjs.inlets.dev\n      paths: [\"/\"]\n  tls:\n   - secretName: expressjs-tls\n     hosts:\n       - expressjs.inlets.dev\n</code></pre> <p>Replace the string <code>expressjs.inlets.dev</code> with your own sub-domain created earlier i.e. <code>expressjs.example.com</code>.</p> <p>You can download around a dozen other CLI tools using arkade including helm. Use arkade to download helm and put it in your <code>PATH</code>:</p> <pre><code>arkade get helm\n\n# Put arkade in your path:\nexport PATH=$PATH:$HOME/.arkade/bin/helm3/\n\n# Or alternatively install to /usr/local/bin\nsudo cp $HOME/.arkade/bin/helm3/helm /usr/local/bin/\n</code></pre> <p>Now install the chart using helm:</p> <pre><code>helm repo add expressjs-k8s https://alexellis.github.io/expressjs-k8s/\n\n# Then they run an update\nhelm repo update\n\n# And finally they install\nhelm upgrade --install express expressjs-k8s/expressjs-k8s \\\n  --values custom.yaml\n</code></pre>"},{"location":"tutorial/kubernetes-ingress/#test-it-out","title":"Test it out","text":"<p>Now check the certificate has been created and visit the webpage in a browser:</p> <pre><code>kubectl get certificate\n\nNAME            READY   SECRET          AGE\nexpressjs-tls   True    expressjs-tls   49s\n</code></pre> <p>Open the webpage i.e. https://api.example.com. Since this is a staging certificate, you will get a warning from your browser. You can accept the certificate in order to test your site.</p>"},{"location":"tutorial/kubernetes-ingress/#getting-a-production-certificate","title":"Getting a Production Certificate","text":"<p>Create a production certificate issuer <code>issuer-prod.yaml</code>, similar to the staging issuer you produced earlier. Be sure to change the email address to your email.</p> <pre><code>export EMAIL=\"you@example.com\"\n\ncat &gt; issuer-prod.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\n  namespace: default\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: $EMAIL\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - selector: {}\n      http01:\n        ingress:\n          class: nginx\nEOF\n</code></pre> <p>Then run <code>kubectl apply -f issuer-prod.yaml</code></p> <p>Now you must update your <code>expressjs</code> deployment to use the new certificate issuer. Create a new helm3 overrides file <code>custom-prod.yaml</code>:</p> <pre><code>cat &gt; custom-prod.yaml &lt;&lt;EOF\ningress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/issuer: \"letsencrypt-prod\"\n  hosts:\n    - host: expressjs.inlets.dev\n      paths: [\"/\"]\n  tls:\n   - secretName: expressjs-tls\n     hosts:\n       - expressjs.inlets.dev\nEOF\n</code></pre> <p>Be sure to change the above domain name to your domain name for the sample server.</p> <p>You can update your deployment using the helm command below:</p> <pre><code>helm upgrade express expressjs-k8s/expressjs-k8s \\\n  --values custom-prod.yaml\n</code></pre> <p>Here's my example on my own domain:</p> <p></p> <p>You can view the certificate the certificate that's being served directly from your local cluster and see that it's valid:</p> <p></p>"},{"location":"tutorial/kubernetes-ingress/#install-a-real-world-application","title":"Install a real-world application","text":"<p>Using arkade you can now install OpenFaaS or a Docker Registry with a couple of commands, and since you have Nginx and cert-manager in place, this will only take a few moments.</p>"},{"location":"tutorial/kubernetes-ingress/#openfaas-with-tls","title":"OpenFaaS with TLS","text":"<p>OpenFaaS is a platform for Kubernetes that provides FaaS functionality and microservices. The motto of the project is Serverless Functions Made Simple and you can deploy it along with TLS in just a couple of commands:</p> <pre><code>export DOMAIN=gateway.example.com\narkade install openfaas\narkade install openfaas-ingress \\\n  --email webmaster@$DOMAIN \\\n  --domain $DOMAIN\n</code></pre> <p>That's it, you'll now be able to access your gateway at https://$DOMAIN/</p> <p>For more, see the OpenFaaS workshop</p>"},{"location":"tutorial/kubernetes-ingress/#docker-registry-with-tls","title":"Docker Registry with TLS","text":"<p>A self-hosted Docker Registry with TLS and private authentication can be hard to set up, but we can now do that with two commands.</p> <pre><code>export DOMAIN=registry.example.com\narkade install docker-registry\narkade install docker-registry-ingress \\\n  --email webmaster@$DOMAIN \\\n  --domain $DOMAIN\n</code></pre> <p>Now try your registry:</p> <pre><code>docker login $DOMAIN\ndocker pull alpine:3.22.1\ndocker tag alpine:3.22.1 $DOMAIN/alpine:3.22.1\n\ndocker push $DOMAIN/alpine:3.22.1\n</code></pre> <p>You can even combine the new private registry with OpenFaaS if you like, checkout the docs for more.</p>"},{"location":"tutorial/kubernetes-ingress/#wrapping-up","title":"Wrapping up","text":"<p>Through the use of inlets-pro we have an encrypted control-plane for the websocket tunnel, and encryption for the traffic going to our Express.js app using a TLS certificate from LetsEncrypt.</p> <p>You can now get a green lock and a valid TLS certificate for your local cluster, which also means that this will work with bare-metal Kubernetes, on-premises and with your Raspberry Pi cluster.</p>"},{"location":"tutorial/kubernetes-tcp-loadbalancer/","title":"Create a public TCP LoadBalancer in Kubernetes","text":"<p>There are a number of ways to run inlets clients from within Kubernetes:</p> <ol> <li>Run the inlets-pro client as a Deployment, after running <code>inlets-pro tcp client --generate=k8s_yaml</code></li> <li>Install the inlets-pro client via Helm</li> <li>Use the inlets-operator to watch for LoadBalancer services and create pairs of tunnel servers and clients</li> </ol> <p>This tutorial shows how to create a public LoadBalancer in Kubernetes using the inlets-operator.</p> <p>The inlets-operator is an open-source Kubernetes operator that watches for LoadBalancer services, then creates the tunnel VM using the same approach as inletsctl. After the VM is booted up and ready for connections, the operator will create a tunnel client, and update the service's public IP on the Kubernetes service.</p>"},{"location":"tutorial/kubernetes-tcp-loadbalancer/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster</li> <li>Helm</li> </ul>"},{"location":"tutorial/kubernetes-tcp-loadbalancer/#install-the-inlets-operator","title":"Install the inlets-operator","text":"<p>There are several cloud providers supported, so use the reference guide to install the chart.</p>"},{"location":"tutorial/kubernetes-tcp-loadbalancer/#create-a-service-and-expose-it-as-a-loadbalancer","title":"Create a service and expose it as a LoadBalancer","text":"<p>Of course, you will already have your own applications that you want to expose. You won't tend to want to expose a HTTP endpoint from a container directly, but through an Ingress Controller or Istio Gateway.</p> <p>As a sample, we can run Nginx as a Pod and then create a Service to expose it as a LoadBalancer only using kubectl.</p> <pre><code># Run Nginx in the background in the default namespace\nkubectl run nginx-1 --image=nginx:latest --restart=Always --port=80 --labels app=nginx\n\n# Expose the Nginx service as a LoadBalancer\nkubectl expose deployment nginx-1 --port=80 --type=LoadBalancer\n</code></pre>"},{"location":"tutorial/kubernetes-tcp-loadbalancer/#find-the-loadbalancer-ip","title":"Find the LoadBalancer IP","text":"<p>There are two ways to find the LoadBalancer IP.</p> <ol> <li>Use the CRD</li> </ol> <pre><code>$ kubectl get tunnels -w\nNAMESPACE   NAME             SERVICE   HOSTSTATUS   HOSTIP        CREATED\ndefault     nginx-1-tunnel   nginx-1   active       46.101.1.67   2m45s\n</code></pre> <ol> <li>Use the LoadBalancer service</li> </ol> <p>$ kubectl get svc -n default NAME         TYPE           CLUSTER-IP    EXTERNAL-IP               PORT(S)        AGE kubernetes   ClusterIP      10.96.0.1                         443/TCP        6m26s nginx-1      LoadBalancer   10.96.94.18   46.101.1.67               80:31194/TCP   4m21s <pre><code>You can then access the Nginx service using the LoadBalancer IP.\n\n```bash\ncurl http://46.101.1.67\n</code></pre>"},{"location":"tutorial/kubernetes-tcp-loadbalancer/#delete-the-tunnel-server","title":"Delete the tunnel server","text":"<p>In order to delete the tunnel server, you need to delete the LoadBalancer service.</p> <pre><code>kubectl delete svc nginx-1\n</code></pre>"},{"location":"tutorial/kubernetes-tcp-loadbalancer/#co-existing-with-other-loadbalancers","title":"Co-existing with other LoadBalancers","text":"<p>If you're running metal-lb or kube-vip to provide local IP addresses for LoadBalancer services, then you can annotate the services you wish to expose to the Internet with <code>operator.inlets.dev/manage=1</code>, then set <code>annotatedOnly: true</code> in the inlets-operator Helm chart.</p> <p>i.e.</p> <pre><code>helm install inlets-operator inlets/inlets-operator --set annotatedOnly=true\n</code></pre> <pre><code>kubectl annotate svc nginx-1 operator.inlets.dev/manage=1\n</code></pre>"},{"location":"tutorial/local-port-forwarding/","title":"Local port forwarding","text":"<p>Local port forwarding is the opposite use-case of exposing a local service to the Public internet.</p> <p>Instead, it tunnels a service from a remote machine to your local machine for access over localhost.</p> <p>For example, you may have a private Prometheus, OpenFaaS, or Grafana dashboard running on a remote server, but you want to access it from your local machine.</p> <p>Local port-forwarding can also be used with a tunnel server deployed in a Kubernetes cluster. In a related blog post, we describe how a user needed to access NATS for debugging purposes, but the NATS service was only accessible from within the cluster: Reliable local port-forwarding from Kubernetes</p>"},{"location":"tutorial/local-port-forwarding/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A remote server running something you wish to access locally</li> <li>A local machine with inlets-pro installed</li> </ul>"},{"location":"tutorial/local-port-forwarding/#enable-local-port-forwarding-on-the-server","title":"Enable local port forwarding on the server","text":"<p>The inlets tcp and http server command disables local port forwarding by default.</p> <p>To enable it, use the <code>--client-forwarding</code> flag.</p> <p>If you're running the binary directly, use:</p> <pre><code>inlets-pro tcp server --client-forwarding\n</code></pre> <p>Otherwise, run <code>systemctl cat inlets-pro</code> to find the service running on your system and add the flag to the <code>ExecStart</code> line.</p> <pre><code>ExecStart=/usr/local/bin/inlets-pro tcp server --client-forwarding\n</code></pre> <p>Whether you're running a HTTP server or TCP server, the flag is the same.</p>"},{"location":"tutorial/local-port-forwarding/#run-the-tunnel-client","title":"Run the tunnel client","text":"<p>In the example where you want to bring Grafana back to your local machine, you can use:</p> <pre><code>inlets-pro [http/tcp] client \\\n    --local 3000:127.0.0.1:3000\n</code></pre> <p>Multiple <code>--local</code> flags can be used to forward multiple ports i.e. for both Grafana and Prometheus:</p> <pre><code>inlets-pro [http/tcp] client \\\n    --local 3000:127.0.0.1:3000 \\\n    --local 9090:127.0.0.1:9090\n</code></pre> <p>If the remote server is running on another machine, but is accessible from the tunnel server's network, you can use the remote server's IP address instead of <code>127.0.0.1</code>.</p> <pre><code>inlets-pro [http/tcp] client \\\n    --local 3000:10.0.0.2:3000 \\\n</code></pre> <p>You can then access any of the tunnelled services over localhost.</p> <pre><code>curl http://localhost:3000\ncurl http://localhost:9090\n</code></pre> <p>TCP services can also be tunneled, and remapped to a different port on your local machine.</p> <p>Here's an example for if SSH is not publicly accessible, but you want to access it over localhost.</p> <pre><code>inlets-pro tcp client \\\n    --local 2222:127.0.0.1:22\n</code></pre> <p>You can then access the SSH server over localhost on port 2222.</p> <pre><code>ssh -p 2222 localhost\n</code></pre>"},{"location":"tutorial/manual-http-server/","title":"Create a tunnel server manually","text":""},{"location":"tutorial/manual-http-server/#setting-up-a-http-tunnel-server-manually","title":"Setting up a HTTP tunnel server manually","text":"<p>In this tutorial we will set up an inlets HTTP tunnel server to serve a local website over HTTPS using Let's Encrypt. The steps will be manual, but usually, we would use a provisioning tool like inletsctl to automate everything for us.</p> <p>This may be useful for understanding how the server binary works, and how to use it on existing servers that you may have. Or perhaps you want to run inlets across an internal or private network.</p>"},{"location":"tutorial/manual-http-server/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A Linux server, Windows and MacOS are also supported</li> <li>The inlets-pro binary at /usr/local/bin/</li> <li>Access to a DNS control plane for a domain you control</li> </ul>"},{"location":"tutorial/manual-http-server/#run-the-server","title":"Run the server","text":"<p>For this example, your tunnel server should be accessible from the Internet. The tunnel client will connect to it and then expose one or more local websites so that you can access them remotely.</p> <p>Create a DNS A record for the subdomain or subdomains you want to use, and have each of them point to the public IP address of the server you have provisioned. These short have a short TTL such as 60s to avoid waiting too long for DNS to propagate throughout the Internet. You can increase this value to a higher number later.</p> <p>First generate an authentication token that the client will use to log in:</p> <pre><code>TOKEN=\"$(head -c 32 /dev/urandom | base64 | cut -d \"-\" -f1)\"\n</code></pre> <p>We'll use the built-in support for Let's Encrypt to get a valid HTTPS certificate for any services you wish to expose via your tunnel server. It is also possible to turn off Let's Encrypt support and use your own reverse proxy such as Caddy or Nginx.</p> <pre><code>export DOMAIN=\"example.com\"\n\n  inlets-pro http server \\\n  --auto-tls \\\n  --control-port 8123 \\\n  --auto-tls-san 192.168.0.10 \\\n  --letsencrypt-domain subdomain1.$DOMAIN \\\n  --letsencrypt-domain subdomain2.$DOMAIN \\\n  --letsencrypt-issuer staging \\\n  --token $TOKEN\n</code></pre> <p>Notice that <code>--letsencrypt-domain</code> can be provided more than one, for each of your subdomains.</p> <p>We are also defaulting to the \"staging\" provider for TLS certificates which allows us to obtain a large number of certificates for experimentation purposes only. The default value, if this field is left off is <code>prod</code> as you will see by running <code>inlets-pro http server --help</code>.</p> <p>Now the following will happen:</p> <ul> <li>The tunnel server will start up and listen to TCP traffic on port 80 and 443.</li> <li>The server will try to resolve each of your domains passed via <code>--letsencrypt-domain</code>.</li> <li>Then once each resolves, Let's Encrypt will be contacted for a HTTP01 ACME challenge.</li> <li>Once the certificates are obtained, the server will start serving the HTTPS traffic.</li> </ul> <p>Now you can connect your client running on another machine.</p> <p>Of course you can tunnel whatever HTTP service you like, if you already have one.</p> <p>Inlets has a built-in HTTP server that we can run on our local / private machine to share files with others. Let's use that as our example:</p> <pre><code>mkdir -p /tmp/share\n\necho \"Welcome to my filesharing service.\" &gt; /tmp/share/welcome.txt\n\ninlets-pro fileserver \\\n --allow-browsing \\\n --webroot /tmp/share/\n --port 8080\n</code></pre> <p>Next let's expose that local service running on localhost:8080 via the tunnel server:</p> <pre><code>export TOKEN=\"\" # Obtain this from your server\nexport SERVER_IP=\"\" # Your server's IP\nexport DOMAIN=\"example.com\"\n\ninlets-pro http client \\\n  --url wss://$SERVER_IP:8123 \\\n  --token $TOKEN \\\n  --upstream http://localhost:8080/\n</code></pre> <p>If you set up your server for more than one sub-domain then you can specify a domain for each local service such as:</p> <pre><code>  --upstream subdomain1.$DOMAIN=http://localhost:8080/,subdomain2.$DOMAIN=http://localhost:3000/\n</code></pre> <p>Now that your client is connected, you can access the HTTP fileserver we set up earlier via the public DNS name:</p> <pre><code>curl -k -v https://subdomain1.$DOMAIN/welcome.txt\n</code></pre> <p>Now that you can see everything working, with a staging certificate, you can run the server command again and switch out the <code>--letsencrypt-issuer staging</code> flag for <code>--letsencrypt-issuer prod</code>.</p>"},{"location":"tutorial/manual-http-server/#wrapping-up","title":"Wrapping up","text":"<p>You have now installed an inlets HTTP tunnel server to a machine by hand. The same can be achieved by running the inletsctl tool, which does all of this automatically on a number of cloud providers.</p> <ul> <li> <p>Can I connect more than one client to the same server?     Yes, and each can connect difference services. So client 1 exposes subdomain1.DOMAIN and client 2 exposes subdomain2.DOMAIN. Alternatively, you can have multiple clients exposing the same domain, for high availability.</p> </li> <li> <p>How do I keep the inlets server process running?     You can run it in the background, by using a systemd unit file. You can generate these via the <code>inlets-pro http server --generate=systemd</code> command.</p> </li> <li> <p>How do I keep the inlets client process running?     Do the same as for a server, but use the <code>inlets-pro http client --generate=systemd</code> command.</p> </li> <li> <p>What else can I do with my server?     Browse the available options for the tunnel servers with the <code>inlets-pro http server --help</code> command.</p> </li> </ul>"},{"location":"tutorial/manual-tcp-server/","title":"Create a tunnel server manually","text":""},{"location":"tutorial/manual-tcp-server/#setting-up-a-tcp-server-manually","title":"Setting up a TCP server manually","text":"<p>In this tutorial we will set up a TCP tunnel server manually.</p>"},{"location":"tutorial/manual-tcp-server/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A Linux server, Windows and MacOS are also supported</li> <li>The inlets-pro binary at /usr/local/bin/</li> </ul>"},{"location":"tutorial/manual-tcp-server/#log-into-your-existing-vm","title":"Log into your existing VM","text":"<p>Generate an authentication token for the tunnel:</p> <pre><code>TOKEN=\"$(openssl rand -base64 32)\" &gt; token.txt\n\n# Find the instance's public IPv4 address:\nPUBLIC_IP=\"$(curl -s https://checkip.amazonaws.com)\"\n</code></pre> <p>Let's imagine the public IP resolved to <code>46.101.128.5</code> which is part of the DigitalOcean range.</p> <pre><code>inlets-pro tcp server \\\n --token \"$TOKEN\" \\\n --auto-tls-san $PUBLIC_IP \\\n --generate=systemd &gt; inlets-pro.service\n</code></pre> <p>Example:</p> <pre><code>[Unit]\nDescription=inlets Pro TCP Server\nAfter=network.target\n\n[Service]\nType=simple\nRestart=always\nRestartSec=5\nStartLimitInterval=0\nExecStart=/usr/local/bin/inlets-pro tcp server --auto-tls --auto-tls-san=46.101.128.5 --control-addr=0.0.0.0 --token=\"ISgW7E2TQk+ZmbJldN9ophfE96B93eZKk8L1+gBysg4=\" --control-port=8124 --auto-tls-path=/tmp/inlets-pro\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Next install the unit file with:</p> <pre><code>sudo cp inlets-pro.service /etc/systemd/system/\nsudo systemctl daemon-reload\nsudo systemctl enable inlets-pro.service\n\nsudo systemctl restart inlets-pro.service\n</code></pre> <p>You'll now be able to check the logs for the server:</p> <pre><code>sudo journalctl -u inlets-pro\n</code></pre> <p>Finally you can connect your TCP client from a remote network. In this case, port 5900 is being exposed for VNC, along with port 2222 for SSH. Port 2222 is an extra port added to the <code>/etc/ssh/sshd_config</code> file on the Linux machine to avoid conflicting with SSH on the tunnel server itself.</p> <pre><code>inlets-pro tcp client \\\n  --token \"ISgW7E2TQk+ZmbJldN9ophfE96B93eZKk8L1+gBysg4=\" \\\n  --upstream 192.168.0.15 \\\n  --port 2222 \\\n  --port 5900 \\\n  --url wss://46.101.128.5:8124\n</code></pre> <p>You can now connect to the public IP of your server via SSH and VNC:</p> <p>For example:</p> <pre><code>ssh -p 2222 pi@46.101.128.5\n</code></pre>"},{"location":"tutorial/manual-tcp-server/#wrapping-up","title":"Wrapping up","text":"<p>You now have a TCP tunnel server that you can connect as and when you like.</p> <ul> <li>You can change the ports of the connected client</li> <li>You can change the upstream</li> <li>You can run multiple <code>inlets-pro tcp client</code> commands to load-balance traffic</li> </ul> <p>But bear in mind that you cannot have two clients exposing different ports at the same time unless you're an inlets uplink user.</p> <p>We would recommend creating TCP tunnel servers via inletsctl which automates all of the above in a few seconds.</p>"},{"location":"tutorial/monitoring-and-metrics/","title":"Monitoring and metrics","text":"<p>Learn how you can monitor your tunnel servers using the <code>status</code> command and Prometheus metrics.</p> <p>This can help you understand how tunnels are being used and answer questions like:</p> <ul> <li>What are the Rate, Error, Duration (RED) metrics for any HTTP APIs or websites that are being hosted?</li> <li>How many connections are open at this point in time, and on which ports?</li> <li>Have any clients attempted to connect which failed authentication?</li> </ul>"},{"location":"tutorial/monitoring-and-metrics/#introduction","title":"Introduction","text":"<p>All the information for monitoring tunnels is exposed via the inlets control-plane. It provides a connection endpoint for clients, a status endpoint and a monitoring endpoint.</p> <p>Checkout the FAQ to learn about the difference between the data-plane and control-plane</p> <p>Inlets provides two distinct ways to monitor tunnels. You can use the <code>status</code> command that is part of the CLI or collect Prometheus metrics for background monitoring and alerting. We will explore both methods.</p>"},{"location":"tutorial/monitoring-and-metrics/#the-status-command","title":"The status command","text":"<p>With the <code>inlets-pro status</code> command you can find out some basic tunnel statistics without logging in with a console SSH session. It shows you a list of the connected clients along with the version and uptime information of the server and can be used with both HTTP and TCP tunnels.</p> <p>Here\u2019s an example of a TCP tunnel server:</p> <pre><code>$ inlets-pro status \\\n  --url wss://178.62.70.130:8123 \\\n  --token \"$TOKEN\" \\\n  --auto-tls\n\nQuerying server status. Version DEV - unknown\nHostname: unruffled-banzai4\nStarted: 49 minutes\nMode: tcp\nVersion:        0.8.9-rc1\n\nClient ID                        Remote Address     Connected Upstreams\n730aa1bb96474cbc9f7e76c135e81da8 81.99.136.188:58102 15 minutes localhost:8001, localhost:8000, localhost:2222\n22fbfe123c884e8284ee0da3680c1311 81.99.136.188:64018 6 minutes  localhost:8001, localhost:8000, localhost:2222\n</code></pre> <p>We can see the clients that are connected and the ports they make available on the server. In this case there are two clients. All traffic to the data plane for ports 8001, 8000 and 2222 will be load-balanced between the two clients for HA.</p> <p>The response from a HTTP tunnel:</p> <pre><code>$ inlets-pro status \\\n  --url wss://147.62.70.101:8123 \\\n  --token \"$TOKEN\"  \\\n  --auto-tls\n\nServer info:\nHostname: creative-pine6\nStarted: 1 day\nMode:           http\nVersion:        0.8.9-rc1\nConnected clients:\nClient ID                        Remote Address     Connected Upstreams\n4e35edf5c6a646b79cc580984eac4ea9 192.168.0.19:34988 5 minutes example.com=http://localhost:8000, prometheus.example.com=http://localhost:9090\n</code></pre> <p>In this example we can see that there is only one client connected to the server at the moment. This client provides two separate domains.</p> <p>The command uses the status endpoint that is exposed on the control-plane. It is possible to invoke the HTTP endpoint yourself. The token that is set up for the server has to be set in the Authorization header.</p> <pre><code>$ curl -ksLS https://127.0.0.1:8123/status \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre> <p>Example response from a HTTP tunnel:</p> <pre><code>{\n  \"info\": {\n    \"version\": \"0.8.9-18-gf4fc15b\",\n    \"sha\": \"f4fc15b9604efd0b0ca3cc604c19c200ae6a1d7b\",\n    \"mode\": \"http\",\n    \"startTime\": \"2021-08-13T12:23:17.321388+01:00\",\n    \"hostname\": \"am1.local\"\n  },\n  \"clients\": [\n    {\n      \"clientID\": \"0c5f2a1ca0174ee3a177c3be7cd6d950\",\n      \"remoteAddr\": \"[::1]:63671\",\n      \"since\": \"2021-08-13T12:23:19.72286+01:00\",\n      \"upstreams\": [\n        \"*=http://127.0.0.1:8080\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorial/monitoring-and-metrics/#monitor-inlets-with-prometheus","title":"Monitor inlets with Prometheus","text":"<p>The server collects metrics for both the data-plane and the control-plane. These metrics are exposed through the monitoring endpoint on the control-plane. Prometheus can be set up for metrics collection and alerting.</p> <p>The name of the metrics and the kind of metrics that are exported will depend on the mode that the server is running in. For TCP tunnels the metric name starts with <code>tcp_</code> for HTTP tunnels this will be <code>http_</code>.</p> <p>You don\u2019t need to be a Kubernetes user to take advantage of Prometheus. You can run it locally on your machine by downloading the binary here.</p> <p>As an alternative, Grafana Cloud can give you a complete monitoring stack for your tunnels without having to worry about finding somewhere to run and maintain Prometheus and Grafana. We have a write up on our blog that shows you how to set this up: Monitor inlets tunnels with Grafana Cloud.</p> <p>Create a <code>prometheus.yaml</code> file to configure Prometheus. Replace TOKEN with the token from your server.</p> <pre><code># my global config\nglobal:\n  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n  # scrape_timeout is set to the global default (10s).\n\n# Alertmanager configuration\nalerting:\n  alertmanagers:\n  - static_configs:\n    - targets:\n      # - alertmanager:9093\n\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\nrule_files:\n  # - \"first_rules.yml\"\n  # - \"second_rules.yml\"\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.\n  - job_name: 'prometheus'\n\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n    static_configs:\n    - targets: ['localhost:9090']\n  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.\n  - job_name: 'http-tunnel'\n\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n    static_configs:\n    - targets: ['localhost:8123']\n    scheme: https\n\n    authorization:\n      type: Bearer\n      credentials: TOKEN\n    tls_config:\n      insecure_skip_verify: true\n</code></pre> <p>Start Prometheus with this command. It will listen on port 9090.</p> <pre><code>$ prometheus --config.file=./prometheus.yaml\n\nlevel=info ts=2021-08-13T11:25:31.791Z caller=main.go:428 msg=\"Starting Prometheus\" version=\"(version=2.29.1, branch=HEAD, revision=dcb07e8eac34b5ea37cd229545000b857f1c1637)\"\nlevel=info ts=2021-08-13T11:25:31.931Z caller=main.go:784 msg=\"Server is ready to receive web requests.\"\n</code></pre>"},{"location":"tutorial/monitoring-and-metrics/#metrics-for-the-control-plane","title":"Metrics for the control-plane","text":"<p>The control-plane metrics can give you insights into the number of clients that are connected and the number of http requests made to the different control-plane endpoints.</p> <p>HTTP tunnels</p> Metric Type Description Labels http_controlplane_connected_gauge gauge gauge of inlets clients connected to the control plane http_controlplane_requests_total counter total HTTP requests processed by connecting clients on the control plane <code>code</code>, <code>path</code> <p>TCP tunnels</p> Metric Type Description Labels tcp_controlplane_connected_gauge gauge gauge of inlets clients connected to the control plane tcp_controlplane_requests_total counter total HTTP requests processed by connecting clients on the control plane <code>code</code>, <code>path</code> <p>These metrics can for instance be used to tell you whether there are a lot of clients that attempted to connect but failed authentication.</p> <p>If running on Kubernetes, the connected gauge could be used to scale tunnels down to zero replicas, and back up again in a similar way to OpenFaaS. This could be important for very large-scale installations of devices or tenants that have partial connectivity.</p>"},{"location":"tutorial/monitoring-and-metrics/#metrics-for-the-data-plane","title":"Metrics for the data-plane","text":"<p>The data-plane metrics can give you insights in the services that are exposed through your tunnel.</p> <p>HTTP tunnels</p> Metric Type Description Labels http_dataplane_requests_total counter total HTTP requests processed <code>code</code>, <code>host</code>, <code>method</code> http_dataplane_request_duration_seconds histogram Seconds spent serving HTTP requests. <code>code</code>, <code>host</code>, <code>method</code> <p>TCP tunnels</p> Metric Type Description Labels tcp_dataplane_connections_gauge gauge gauge of TCP connections established over data plane <code>port</code> tcp_dataplane_connections_total counter total count of TCP connections established over data plane <code>port</code> <p>For HTTP tunnels these metrics can be used to get Rate, Error, Duration (RED) information for any API or website that is connected through the tunnel. This essentially allows you to collect basic metrics for your services even if they do not export any metrics themselves.</p> <p>For TCP tunnels these metrics can help answer questions like:</p> <ul> <li>How many connections are open at this point in time, and on which ports? i.e. if exposing SSH on port 2222, how many connections are open?</li> </ul>"},{"location":"tutorial/monitoring-and-metrics/#wrapping-up","title":"Wrapping up","text":"<p>We showed two different options that can be used to monitor your inlets tunnels.</p> <p>The CLI provides a quick and easy way to get some status information for a tunnel. The endpoint that exposes this information can also be invoked directly using HTTP.</p> <p>Prometheus metrics can be collected from the monitoring endpoint. These metrics are useful for background monitoring and alerting. They can provide you with Rate, Error, Duration (RED) metrics for HTTP services that are exposed through Inlets.</p>"},{"location":"tutorial/monitoring-and-metrics/#you-may-also-like","title":"You may also like","text":"<ul> <li>Blog post: Measure and monitor your inlets tunnels</li> </ul>"},{"location":"tutorial/postgresql-tcp-tunnel/","title":"Tutorial: Tunnel a private Postgresql database","text":"<p>In this tutorial we will tunnel Postgresql over inlets Pro to a remote machine. From there you can expose it to the Internet, or bind it to the local network for private VPN-like access.</p> <p>Get your inlets subscription here</p>"},{"location":"tutorial/postgresql-tcp-tunnel/#setup-your-exit-node","title":"Setup your exit node","text":"<p>Provision a cloud VM on DigitalOcean or another IaaS provider using inletsctl:</p> <pre><code>inletsctl create \\\n --provider digitalocean \\\n --region lon1 \\\n --pro\n</code></pre> <p>Note the <code>--url</code> and <code>TOKEN</code> given to you in this step.</p>"},{"location":"tutorial/postgresql-tcp-tunnel/#run-postgresql-on-your-private-server","title":"Run Postgresql on your private server","text":"<p>We can run a Postgresql instance using Docker:</p> <pre><code>head -c 16 /dev/urandom |shasum \n8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\n\nexport PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\n\ndocker run --rm --name postgres -p 5432:5432 -e POSTGRES_PASSWORD=8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 -ti postgres:latest\n</code></pre>"},{"location":"tutorial/postgresql-tcp-tunnel/#connect-the-inlets-pro-client","title":"Connect the inlets Pro client","text":"<p>Fill in the below with the outputs you received from <code>inletsctl create</code>.</p> <p>Note that <code>UPSTREAM=\"localhost\"</code> can be changed to point at a host or IP address accessible from your client. The choice of <code>localhost</code> is suitable when you are running Postgresql in Docker on the same computer as the inlets Pro client.</p> <p>The client will look for your license in <code>$HOME/.inlets/LICENSE</code>, but you can also use the <code>--license/--license-file</code> flag if you wish.</p> <pre><code>export EXIT_IP=\"134.209.21.155\"\nexport TCP_PORTS=\"5432\"\nexport LICENSE_FILE=\"$HOME/LICENSE.txt\"\nexport TOKEN=\"KXJ5Iq1Z5Cc8GjFXdXJrqNhUzoScXnZXOSRKeh8x3f6tdGq1ijdENWQ2IfzdCg4U\"\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --connect \"wss://$EXIT_IP:8123/connect\" \\\n  --token \"$TOKEN\" \\\n  --upstream $UPSTREAM \\\n  --ports $TCP_PORTS\n</code></pre>"},{"location":"tutorial/postgresql-tcp-tunnel/#connect-to-your-private-postgresql-server-from-the-internet","title":"Connect to your private Postgresql server from the Internet","text":"<p>You can run this command from anywhere, since your exit-server has a public IP:</p> <pre><code>export PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\nexport EXIT_IP=\"209.97.141.140\"\n\ndocker run -it -e PGPORT=5432 -e PGPASSWORD=$PASSWORD --rm postgres:latest psql -U postgres -h $EXIT_IP\n</code></pre> <p>Try a command such as <code>CREATE database</code> or <code>\\dt</code>.</p>"},{"location":"tutorial/postgresql-tcp-tunnel/#treat-the-database-as-private-like-a-vpn","title":"Treat the database as private - like a VPN","text":"<p>A split data and control-plane mean that tunnels do not need to be exposed on the Internet and can replace a VPN or a bespoke solution with SSH tunnels</p> <p>A split data and control-plane mean that tunnels do not need to be exposed on the Internet and can replace a VPN or a bespoke solution with SSH tunnels</p> <p>If you would like to keep the database service and port private, you can run the exit-server as a Pod in a Kubernetes cluster, or add an iptables rule to block access from external IPs.</p> <p>Log into your exit-server and update <code>/etc/systemd/system/inlets-pro.service</code></p> <p>To listen on loopback, add: <code>--listen-data=127.0.0.1:</code> To listen on a private adapter such as <code>10.1.0.10</code>, add: <code>--listen-data=10.1.0.10:</code></p> <p>Restart the service, and you'll now find that the database port <code>5432</code> can only be accessed from within the network you specified in <code>--listen-data</code></p> <p>Other databases such as Cassandra, MongoDB and Mysql/MariaDB also work exactly the same. Just change the port from <code>5432</code> to the port of your database.</p>"},{"location":"tutorial/ssh-tcp-tunnel/","title":"Tutorial: Expose a private SSH server over a TCP tunnel","text":"<p>In this tutorial we will use inlets-pro to access your computer behind NAT or a firewall. We'll do this by tunnelling SSH over inlets-pro, and clients will connect to your exit-server.</p> <p>Scenario: You want to allow SSH access to a computer that doesn't have a public IP, is inside a private network or behind a firewall. A common scenario is connecting to a Raspberry Pi on a home network or a home-lab.</p> <p>Get your inlets subscription here</p>"},{"location":"tutorial/ssh-tcp-tunnel/#setup-your-tunnel-server-with-inletsctl","title":"Setup your tunnel server with <code>inletsctl</code>","text":"<p>For this tutorial you will need to have an account and API key with one of the supported providers, or you can create an exit-server manually and install inlets Pro there yourself.</p> <p>For this tutorial, the DigitalOcean provider will be used. You can get free credits on DigitalOcean with this link.</p> <p>Create an API key in the DigitalOcean dashboard with Read and Write permissions, and download it to a file called <code>do-access-token</code> in your home directory.</p> <p>You need to know the IP of the machine you to connect to on your local network, for instance <code>192.168.0.35</code> or <code>127.0.0.1</code> if you are running inlets Pro on the same host as SSH.</p> <p>You can use the <code>inletsctl</code> utility to provision exit-servers with inlets Pro preinstalled, it can also download the <code>inlets-pro</code> CLI.</p> <pre><code>curl -sLSf https://inletsctl.inlets.dev | sh\nsudo mv inletsctl /usr/local/bin/\nsudo inletsctl download\n</code></pre> <p>If you already have <code>inletsctl</code> installed, then make sure you update it with <code>inletsctl update</code>.</p>"},{"location":"tutorial/ssh-tcp-tunnel/#create-an-tunnel-server","title":"Create an tunnel server","text":""},{"location":"tutorial/ssh-tcp-tunnel/#a-automate-your-tunnel-server","title":"A) Automate your tunnel server","text":"<p>The inletsctl tool can create a tunnel server for you in the region and cloud of your choice.</p> <pre><code>inletsctl create \\\n  --provider digitalocean \\\n  --access-token-file ~/do-access-token \\\n  --region lon1\n</code></pre> <p>Run <code>inletsctl create --help</code> to see all the options.</p> <p>After the machine has been created, <code>inletsctl</code> will output a sample command for the <code>inlets-pro client</code> command:</p> <pre><code>inlets-pro tcp client --url \"wss://206.189.114.179:8123/connect\" \\\n    --token \"4NXIRZeqsiYdbZPuFeVYLLlYTpzY7ilqSdqhA0HjDld1QjG8wgfKk04JwX4i6c6F\"\n</code></pre> <p>Don't run this command, but note down the <code>--url</code> and <code>--token</code> parameters for later</p>"},{"location":"tutorial/ssh-tcp-tunnel/#b-manual-setup-of-your-tunnel-server","title":"B) Manual setup of your tunnel server","text":"<p>Use B) if you want to provision your virtual machine manually, or if you already have a host from another provider.</p> <p>Log in to your remote tunnel server with <code>ssh</code> and obtain the binary using <code>inletsctl</code>:</p> <pre><code>curl -sLSf https://inletsctl.inlets.dev | sh\nsudo mv inletsctl /usr/local/bin/\nsudo inletsctl download\n</code></pre> <p>Find your public IP:</p> <pre><code>export IP=$(curl -s ifconfig.co)\n</code></pre> <p>Confirm the IP with <code>echo $IP</code> and save it, you need it for the client</p> <p>Get an auth token and save it for later to use with the client</p> <pre><code>export TOKEN=\"$(head -c 16 /dev/urandom |shasum|cut -d'-' -f1)\"\n\necho $TOKEN\n</code></pre> <p>Start the server:</p> <pre><code>inlets-pro \\\n  tcp \\\n  server \\\n  --auto-tls \\\n  --auto-tls-san $IP \\\n  --token $TOKEN\n</code></pre> <p>If running the inlets client on the same host as SSH, you can simply set <code>PROXY_TO_HERE</code> to <code>localhost</code>. Or if you are running SSH on a different computer to the inlets client, then you can specify a DNS entry or an IP address like <code>192.168.0.15</code>.</p> <p>If using this manual approach to install inlets Pro, you should create a systemd unit file.</p> <p>The easiest option is to run the server with the <code>--generate=systemd</code> flag, which will generate a systemd unit file to stdout. You can then copy the output to <code>/etc/systemd/system/inlets-pro.service</code> and enable it with <code>systemctl enable inlets-pro</code>.</p>"},{"location":"tutorial/ssh-tcp-tunnel/#configure-the-private-ssh-servers-listening-port","title":"Configure the private SSH server's listening port","text":"<p>It's very likely (almost certain) that your exit server will already be listening for traffic on the standard ssh port <code>22</code>. Therefore you will need to configure your internal server to use an additional TCP port such as <code>2222</code>.</p> <p>Once configured, you'll still be able to connect to the internal server on port 22, but to connect via the tunnel, you'll use port <code>2222</code></p> <p>Add the following to  <code>/etc/ssh/sshd_config</code>:</p> <pre><code>Port 22\nPort 2222\n</code></pre> <p>For (optional) additional security, you could also disable password authentication, but make sure that you have inserted your SSH key to the internal server with <code>ssh-copy-id user@ip</code> before reloading the SSH service.</p> <pre><code>PasswordAuthentication no\n</code></pre> <p>Now need to reload the service so these changes take effect</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart sshd\n</code></pre> <p>Check that you can still connect on the internal IP on port 22, and the new port 2222.</p> <p>Use the <code>-p</code> flag to specify the SSH port:</p> <pre><code>export IP=\"192.168.0.35\"\n\nssh -p 22 $IP \"uptime\"\nssh -p 2222 $IP \"uptime\"\n</code></pre>"},{"location":"tutorial/ssh-tcp-tunnel/#start-the-inlets-pro-client","title":"Start the inlets Pro client","text":"<p>First download the inlets-pro client onto the private SSH server:</p> <pre><code>sudo inletsctl download\n</code></pre> <p>Use the command from earlier to start the client on the server:</p> <pre><code>export IP=\"206.189.114.179\"\nexport TCP_PORTS=\"2222\"\nexport LICENSE_FILE=\"$HOME/LICENSE.txt\"\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://$IP:8123/connect\" \\\n  --token \"4NXIRZeqsiYdbZPuFeVYLLlYTpzY7ilqSdqhA0HjDld1QjG8wgfKk04JwX4i6c6F\" \\\n  --license-file \"$LICENSE_FILE\" \\\n  --upstream \"$UPSTREAM\" \\\n  --ports $TCP_PORTS\n</code></pre> <p>The <code>localhost</code> value will be used for <code>--upstream</code> because the tunnel client is running on the same machine as the SSH service. However, you could run the client on another machine within the network, and then change the flag to point to the private SSH server's IP.</p>"},{"location":"tutorial/ssh-tcp-tunnel/#try-it-out","title":"Try it out","text":"<p>Verify the installation by trying to SSH to the public IP, using port <code>2222</code>.</p> <pre><code>ssh -p 2222 user@206.189.114.179\n</code></pre> <p>You should now have access to your server via SSH over the internet with the IP of the exit server.</p> <p>You can also use other compatible tools like <code>sftp</code>, <code>scp</code> and <code>rsync</code>, just make sure that you set the appropriate port flag. The port flag for sftp is <code>-P</code> rather than <code>-p</code>.</p>"},{"location":"tutorial/ssh-tcp-tunnel/#wrapping-up","title":"Wrapping up","text":"<p>The principles in this tutorial can be adapted for other protocols that run over TCP such as MongoDB or PostgreSQL, just adapt the port number as required.</p> <ul> <li>Quick-start: Tunnel a private database over inlets</li> </ul>"},{"location":"uplink/","title":"Inlets Uplink","text":"<p>Inlets Uplink is a complete management solution for tunnels for SaaS companies, infrastructure teams, and service providers. It's designed for scale, multi-tenancy and automation.</p> <p></p> <p>Conceptual architecture: Inlets Uplink is deployed to one or more central Kubernetes cluster(s).</p> <p>Inlets Uplink answers two primary questions:</p> <ol> <li>\"How do we access private/on-premises customer services from within our own product or APIs?\"</li> <li>\"How can we provide tunnels to employees or customers as a managed service?\"</li> </ol> <p>In the first case, of accessing private customer services, you have have already considered building your own agent, using a queue, or a VPN.</p> <p>The first two options involve considerable work both up front and in the long run. VPNs require firewall changes, specific network conditions, additional paperwork, and can have prohibitive costs associated with them.</p> <p>For the second use-case, managed options can be convenient, however they can be expensive as they scale up, may present a security risk, and can't be customised to your specific needs.</p> <p>What's the difference between Inlets Pro and Inlets Uplink?</p> <p>Inlets aka \"Inlets Pro\" is a stand-alone binary that can be use to expose local HTTPs and TCP services on a remote machine or network. This is the easiest option for individuals and small teams.</p> <p>Uplink is an enterprise tunnel solution for Kubernetes.</p>"},{"location":"uplink/#inlets-works-from-behind-restrictive-networks","title":"Inlets works from behind restrictive networks","text":"<p>The Inlets Uplink runs inside a private network to connect to a public server endpoint using an outbound HTTP connection. Once established, the TLS-encrypted connection is upgraded to a websocket for bi-directional communication. This means it works over NAT, firewalls, and HTTP proxies.</p>"},{"location":"uplink/#private-or-public-tunnels","title":"Private or Public tunnels","text":"<p>Depending on your use-case, you can keep the data-plane private or expose it to the Internet on a per-tunnel basis.</p> <p>By default, the data-plane for each inlets-uplink tunnel is kept private and can only be accessed from within the Kubernetes cluster where inlets-uplink is installed.</p> <p>You can then expose the data-plane for any tunnel to the Public Internet if required.</p>"},{"location":"uplink/#features-benefits","title":"Features &amp; benefits","text":"<ul> <li>The management solution is built-in, self-hosted and runs on your own Kubernetes cluster</li> <li>You can create a tunnel almost instantly via CLI, REST API or the \"Tunnel\" Custom Resource</li> <li>The license is installed on the server, instead of being attached to the client, to make it easier to give out a connection command to customers and team members</li> <li>When exposing TCP ports, they can be remapped to avoid conflicts or needing privileged ports</li> <li>A single tunnel can expose HTTP and TCP at the same time</li> <li>All tunnels can be monitored centrally for reliability and usage</li> <li>By default all tunnels are private and only available for access by your own applications</li> <li>Tunnels can also be managed through Helm, ArgoCD or Flux for a GitOps workflow</li> </ul> <p>Support via email is included for the installation and operation of your inlets-uplink installation.</p> <p>You can read more about why we created inlets uplink in the original product announcement.</p>"},{"location":"uplink/#installation","title":"Installation","text":"<p>Learn more about how to install Inlets Uplink.</p> <p>Reach out to us if you have questions or would like to see a demo: Contact the inlets team</p>"},{"location":"uplink/connect-tunnel-client/","title":"Connect the tunnel client","text":"<p>The tunnel plugin for the inlets-pro CLI can be used to get connection instructions for a tunnel.</p> <p>Whether the client needs to be deployed as a systemd service on the customers server or as a Kubernetes service, with the CLI it is easy to generate connection instructions for these different formats by setting the <code>--format</code> flag.</p> <p>Supported formats:</p> <ul> <li>CLI command</li> <li>Systemd </li> <li>Kubernetes YAML Deployment</li> </ul> <p>Make sure you have the latest version of the tunnel command available:</p> <pre><code>inlets-pro plugin get tunnel\n</code></pre>"},{"location":"uplink/connect-tunnel-client/#get-connection-instructions","title":"Get connection instructions","text":"<p>Generate the client command for the selected tunnel:</p> <pre><code>$ inlets-pro tunnel connect openfaas \\\n    --domain uplink.example.com \\\n    --upstream http://127.0.0.1:8080\n\n# Access your HTTP tunnel via: http://openfaas.tunnels:8000\n\n# Access your TCP tunnel via ClusterIP: \n#  openfaas.tunnels:5432\n\ninlets-pro uplink client \\\n  --url=wss://uplink.example.com/tunnels/openfaas \\\n  --token=tbAd4HooCKLRicfcaB5tZvG3Qj36pjFSL3Qob6b9DBlgtslmildACjWZUD \\\n  --upstream=http://127.0.0.1:8080\n</code></pre> <p>Optionally the <code>--quiet</code> flag can be set to print the CLI command without the additional info.</p>"},{"location":"uplink/connect-tunnel-client/#deploy-the-client-as-a-systemd-service","title":"Deploy the client as a systemd service","text":"<p>To generate a systemd service file for the tunnel client command set the <code>--format</code> flag to <code>systemd</code>. </p> <pre><code>$ inlets-pro tunnel connect openfaas \\\n    --domain uplink.example.com \\ \n    --upstream http://127.0.0.1:8080 \\\n    --format systemd\n\n[Unit]\nDescription=openfaas inlets client\nAfter=network.target\n\n[Service]\nType=simple\nRestart=always\nRestartSec=5\nStartLimitInterval=0\nExecStart=/usr/local/bin/inlets-pro uplink client --url=wss://uplink.example.com/tunnels/openfaas --token=tbAd4HooCKLRicfcaB5tZvG3Qj36pjFSL3Qob6b9DBlgtslmildACjWZUD --upstream=http://127.0.0.1:8080\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Copy the service file over to the customer's host. Save the unit file as: <code>/etc/systemd/system/openfaas-tunnel.service</code>.</p> <p>Once the file is in place start the service for the first time:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now openfaas-tunnel\n</code></pre> <p>Verify the tunnel client is running:</p> <pre><code>systemctl status openfaas-tunnel\n</code></pre> <p>You can also check the logs to see if the client connected successfully:</p> <pre><code>journalctl -u openfaas-tunnel\n</code></pre>"},{"location":"uplink/connect-tunnel-client/#deploy-the-client-in-a-kubernetes-cluster","title":"Deploy the client in a Kubernetes cluster","text":"<p>To generate a YAML deployment for a selected tunnel, set the <code>--format</code> flag to <code>k8s_yaml</code>. The generated resource can be deployed in the customers cluster.</p> <pre><code>inlets-pro tunnel connect openfaas \\\n    --domain uplink.example.com \\\n    --upstream http://gateway.openfaas:8080 \\\n    --format k8s_yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openfaas-inlets-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openfaas-inlets-client\n  template:\n    metadata:\n      labels:\n        app: openfaas-inlets-client\n    spec:\n      containers:\n      - name: openfaas-inlets-client\n        image: ghcr.io/inlets/inlets-pro:0.9.14\n        imagePullPolicy: IfNotPresent\n        command: [\"inlets-pro\"]\n        args:\n        - \"uplink\"\n        - \"client\"\n        - \"--url=wss://uplink.example.com/tunnels/openfaas\"\n        - \"--token=tbAd4HooCKLRicfcaB5tZvG3Qj36pjFSL3Qob6b9DBlgtslmildACjWZUD\"\n        - \"--upstream=http://gateway.openfaas:8080\"\n</code></pre> <p>In this example we create a tunnel to uplink an OpenFaaS deployment.</p> <p>Get the logs for the client and check it connected successfully:</p> <pre><code>kubectl logs deploy/openfaas-inlets-client\n</code></pre>"},{"location":"uplink/create-tunnels/","title":"Create a tunnel","text":""},{"location":"uplink/create-tunnels/#create-a-namespace-for-the-tunnel","title":"Create a namespace for the tunnel","text":"<p>The <code>inlets</code> namespace contains the control plane for inlets uplink and should not be used to host tunnels.</p> <p>So you'll need to create at least one additional namespace for your tunnels.</p> One namespace per tenantOne namespace for all tunnels <p>This approach avoids conflicts on names, and gives better isolation between tenants.</p> <p>After creating the tunnel, you'll also need to label it <code>inlets.dev/uplink=1</code></p> <pre><code>export NS=\"tenant1\"\nkubectl create namespace $NS\nkubectl label --overwrite namespace $NS \"inlets.dev/uplink\"=1\n</code></pre> <p>Then, create a copy of the license secret in the new namespace:</p> <pre><code>export NS=\"tenant1\"\nexport LICENSE=$(kubectl get secret -n inlets inlets-uplink-license -o jsonpath='{.data.license}' | base64 -d)\n\nkubectl create secret generic \\\n  -n $NS \\\n  inlets-uplink-license \\\n  --from-literal license=$LICENSE\n</code></pre> <p>For development purposes, you could create a single namespace for all your tenants.</p> <p>After creating the tunnel, you'll also need to label it <code>inlets.dev/uplink=1</code>\\</p> <pre><code>export NS=\"tunnels\"\nkubectl create namespace $NS\nkubectl label --overwrite namespace $NS \"inlets.dev/uplink\"=1\n</code></pre> <p>Then, create a copy of the license secret in the new namespace:</p> <pre><code>export NS=\"tunnels\"\nexport LICENSE=$(kubectl get secret -n inlets inlets-uplink-license -o jsonpath='{.data.license}' | base64 -d)\n\nkubectl create secret generic \\\n  -n $NS \\\n  inlets-uplink-license \\\n  --from-literal license=$LICENSE\n</code></pre> <p>If you're using Istio, then you need to label each additional namespace to enable sidecar injection:</p> <pre><code>export NS=\"tunnels\"\n\nkubectl label namespace $NS \\\n  istio-injection=enabled --overwrite\n</code></pre>"},{"location":"uplink/create-tunnels/#create-a-tunnel-with-an-auto-generated-token","title":"Create a Tunnel with an auto-generated token","text":"<p>The <code>Tunnel</code> Custom Resource describes an inlets-uplink tunnel server. You can specify a reference to a pre-existing Kubernetes secret for the token, or have one generated for you.</p> <p>The below configures a tunnel for a customer called <code>acmeco</code> in the <code>tunnels</code> namespace. Port 8080 is exposed as a TCP tunnel, and the <code>licenceRef</code> needs to reference a secret containing an inlets-uplink license.</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: acmeco\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\n  tcpPorts:\n  - 8080 \n</code></pre> <p>Alternatively the CLI can be used to create a tunnel:</p> <pre><code>inlets-pro tunnel create acmeco \\\n  -n tunnels \\\n  --port 8080\n</code></pre> <p>Once created, you can check the status of the tunnel, and you will see a secret was generated for the token.</p> <pre><code>kubectl get -n tunnels tunnel/acmeco\n</code></pre>"},{"location":"uplink/create-tunnels/#create-a-tunnel-with-a-pre-defined-token","title":"Create a Tunnel with a pre-defined token","text":"<p>If you delete a Tunnel with an auto-generated token, and re-create it later, the token will change. So we recommend that you pre-define your tokens. This style works well for GitOps and automated deployments with Helm.</p> <p>Make sure the secret is in the same namespace as the Tunnel Custom Resource.</p> <p>You can use <code>openssl</code> to generate a strong token:</p> <pre><code>openssl rand -base64 32 |tr -d '\\n' &gt; token.txt\n</code></pre> <p>Note that the <code>tr</code> command is used to remove the newline character from the output, so that there is no new-line within the token.</p> <p>Create a Kubernetes secret for the token named <code>custom-token</code>:</p> <pre><code>kubectl create secret generic \\\n  -n tunnels acmeco-token \\\n  --from-file token=./token.txt\n</code></pre> <p>Reference the token when creating a tunnel, to expose ports 8080 over TCP.</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: acmeco\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\n+  tokenRef:\n+    name: acmeco-token\n+    namespace: tunnels\n  tcpPorts:\n  - 8080\n</code></pre> <p>The <code>tokenRef</code> section is used to reference the secret containing the token.</p> <p>Clients can now connect to the tunnel using the pre-defined token.</p>"},{"location":"uplink/create-tunnels/#node-selection-and-annotations-for-tunnels","title":"Node selection and annotations for tunnels","text":"<p>The tunnel spec has a <code>nodeSelector</code> field that can be used to assign tunnel pods to Nodes. See Assign Pods to Nodes from the kubernetes docs for more information.</p> <p>It is also possible to set additional annotations on the tunnel pod using the <code>podAnnotations</code> field in the tunnel spec.</p> <p>The following example adds an annotation with the customer name to the tunnel pod and uses the node selector to specify a target node with a specific region label.</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: acmeco\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\n  tcpPorts:\n  - 8080\n  podAnnotations:\n    customer: acmeco\n  nodeSelector:\n    region: east\n</code></pre>"},{"location":"uplink/create-tunnels/#connect-to-tunnels","title":"Connect to tunnels","text":"<p>The <code>uplink client</code> command is part of the inlets-pro binary. It is used to connect to tunnels and expose services over the tunnel.</p> <p>There are several ways to get the binary:</p> <ul> <li>Download it from the GitHub releases</li> <li>Get it with arkade: <code>arkade get inlets-pro</code></li> <li>Use the inlets-pro docker image</li> </ul>"},{"location":"uplink/create-tunnels/#example-tunnel-a-customer-http-service","title":"Example: Tunnel a customer HTTP service","text":"<p>We'll use inlets-pro's built in file server as an example of how to tunnel a HTTP service.</p> <p>Run this command on a private network or on your workstation:</p> <pre><code>mkdir -p /tmp/share\ncd /tmp/share\necho \"Hello World\" &gt; README.md\n\ninlets-pro fileserver -w /tmp/share -a\n\nStarting inlets Pro fileserver. Version: 0.9.10-rc1-1-g7bc49ae - 7bc49ae494bd9ec789fc5e9eaf500f2b1fe60786\nServing files from: /tmp/share\nListening on: 127.0.0.1:8080, allow browsing: true, auth: false\n</code></pre> <p>Once the server is running connect to your tunnel using the inlets-uplink client. We will connect to the tunnel called <code>acmeco</code>.</p> <p>Retrieve the token for the tunnel:</p> kubectlcli <pre><code>kubectl get -n tunnels \\\n  secret/acmeco -o jsonpath=\"{.data.token}\" | base64 --decode &gt; token.txt \n</code></pre> <pre><code>inlets-pro tunnel token acmeco \\\n  -n tunnels &gt; token.txt\n</code></pre> <p>The contents will be saved in token.txt.</p> <p>Start the tunnel client:</p> <pre><code>inlets-pro uplink client \\\n  --url wss://uplink.example.com/tunnels/acmeco \\\n  --upstream http://127.0.0.1:8080 \\\n  --token-file ./token.txt\n</code></pre> <p>Tip: get connection instructions</p> <p>The tunnel plugin for the inlets-pro CLI can be used to get connection instructions for a tunnel.</p> <pre><code>inlets-pro tunnel connect acmeco \\\n  --domain uplink.example.com \\\n  --upstream http://127.0.0.1:8080\n</code></pre> <p>Running the command above will print out the instructions to connect to the tunnel:</p> <pre><code># Access your tunnel via ClusterIP: acmeco.tunnels\ninlets-pro uplink client \\\n  --url=wss://uplink.example.com/tunnels/acmeco \\\n  --upstream=http://127.0.0.1:8080 \\\n  --token=z4oubxcamiv89V0dy8ytmjUEPwAmY0yFyQ6uaBmXsIQHKtAzlT3PcGZRgK\n</code></pre> <p>Run a container in the cluster to check the file server is accessible through the http tunnel using curl: <code>curl -i acmeco.tunnels:8000</code></p> <pre><code>$ kubectl run -t -i curl --rm \\\n  --image ghcr.io/openfaas/curl:latest /bin/sh   \n\n$ curl -i acmeco.tunnels:8000\nHTTP/1.1 200 OK\nContent-Type: text/html; charset=utf-8\nDate: Thu, 17 Nov 2022 08:39:48 GMT\nLast-Modified: Mon, 14 Nov 2022 20:52:53 GMT\nContent-Length: 973\n\n&lt;pre&gt;\n&lt;a href=\"README.md\"&gt;README.md&lt;/a&gt;\n&lt;/pre&gt;\n</code></pre>"},{"location":"uplink/create-tunnels/#how-to-tunnel-multiple-http-services-from-a-customer","title":"How to tunnel multiple HTTP services from a customer","text":"<p>The following example shows how to access more than one HTTP service over the same tunnel. It is possible to expose multiple upstream services over a single tunnel.</p> <p>You can add upstreamDomains to the Tunnel resource. Uplink wil create additional Services for each domain so the HTTP data plane is available on different domains.  <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: acmeco\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\n  tcpPorts:\n  - 8080\n+  upstreamDomains:\n+  - gateway\n+  - prometheus\n</code></pre></p> <p>Upstreams can also be added while creating a tunnel with with cli:</p> <pre><code>inlets-pro tunnel create acmeco \\\n  --namespace tunnels \\\n  --upstream gateway \\\n  --upstream prometheus\n</code></pre> <p>Start a tunnel client and add multiple upstreams:</p> <pre><code>inlets-pro uplink client \\\n  --url wss://uplink.example.com/tunnels/acmeco \\\n  --upstream prometheus.tunnels=http://127.0.0.1:9090 \\\n  --upstream gateway.tunnels=http://127.0.0.1:8080 \\\n  --token-file ./token.txt\n</code></pre> <p>Access both services using <code>curl</code>:</p> <pre><code>$ kubectl run -t -i curl --rm \\\n  --image ghcr.io/openfaas/curl:latest /bin/sh   \n\n$ curl -i gateway.tunnels:8000\nHTTP/1.1 302 Found\nContent-Length: 29\nContent-Type: text/html; charset=utf-8\nDate: Thu, 16 Feb 2023 16:29:09 GMT\nLocation: /graph\n\n&lt;a href=\"/graph\"&gt;Found&lt;/a&gt;.\n\n\n$ curl -i -H prometheus.tunnels:8000\nHTTP/1.1 301 Moved Permanently\nContent-Length: 39\nContent-Type: text/html; charset=utf-8\nDate: Thu, 16 Feb 2023 16:29:11 GMT\nLocation: /ui/\n\n&lt;a href=\"/ui/\"&gt;Moved Permanently&lt;/a&gt;.\n</code></pre> <p>Note that the <code>Host</code> header has to be set in the request so the tunnel knows which upstream to send the request to.</p>"},{"location":"uplink/create-tunnels/#tunnel-a-customers-tcp-service","title":"Tunnel a customer's TCP service","text":"<p>Perhaps you need to access a customer's Postgres database from their private network?</p>"},{"location":"uplink/create-tunnels/#create-a-tcp-tunnel-using-a-custom-resource","title":"Create a TCP tunnel using a Custom Resource","text":"<p>Example Custom Resource to deploy a tunnel for acmeco\u2019s production Postgres database:</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: prod-database\n  namespace: acmeco\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: acmeco\n  tcpPorts:\n  - 5432\n</code></pre> <p>Alternatively the cli can be used to create a new tunnel:</p> <pre><code>inlets-pro tunnel create prod-database \\\n  -n acmeco\n  --port 5432\n</code></pre>"},{"location":"uplink/create-tunnels/#run-postgresql-on-your-private-server","title":"Run postgresql on your private server","text":"<p>The quickest way to spin up a Postgres instance on your own machine would be to use Docker:</p> <pre><code>head -c 16 /dev/urandom |shasum \n8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\n\nexport PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\n\ndocker run --rm --name postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 \\\n  -ti postgres:latest\n</code></pre>"},{"location":"uplink/create-tunnels/#connect-with-an-inlets-uplink-client","title":"Connect with an inlets uplink client","text":"<pre><code>export UPLINK_DOMAIN=\"uplink.example.com\"\n\ninlets-pro uplink client \\\n  --url wss://${UPLINK_DOMAIN}/acmeco/prod-database \\\n  --upstream 127.0.0.1:5432 \\\n  --token-file ./token.txt\n</code></pre>"},{"location":"uplink/create-tunnels/#access-the-customer-database-from-within-kubernetes","title":"Access the customer database from within Kubernetes","text":"<p>Now that the tunnel is established, you can connect to the customer's Postgres database from within Kubernetes using its ClusterIP <code>prod-database.acmeco.svc.cluster.local</code>:</p> <p>Try it out:</p> <pre><code>export PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\n\nkubectl run -i -t psql \\\n  --env PGPORT=5432 \\\n  --env PGPASSWORD=$PASSWORD --rm \\\n  --image postgres:latest -- psql -U postgres -h prod-database.acmeco\n</code></pre> <p>Try a command such as <code>CREATE database websites (url TEXT)</code>, <code>\\dt</code> or <code>\\l</code>.</p>"},{"location":"uplink/create-tunnels/#getting-help","title":"Getting help","text":"<p>Feel free to reach out to our team via email for technical support.</p>"},{"location":"uplink/expose-tunnels/","title":"How to Expose Tunnels on the Internet","text":"<p>Info</p> <p>Inlets Uplink is designed to connect customer services to a remote Kubernetes cluster for command and control as part of a SaaS product.</p> <p>Any tunnelled service can be accessed directly from within the cluster using a ClusterIP Service and does not need to be exposed to the public Internet in order to be used by a SaaS product.</p> <p>Each inlets uplink tunnel is provisioned with a ClusterIP service that you can access internally within the cluster. The same service can be used to expose the tunnel to the public Internet using an Ingress resource. This approach is recommended for new users for dozens of tunnels.</p> <p></p> <p>Each tunnel's data-plane is exposed via a separate Ingress and Certificate</p> <p>Alternatively, the data-router component can be used along with a wild-card DNS record and TLS certificate to expose many tunnels with a single Ingress record or Istio Gateway. This approach requires additional setup because the DNS01 challenge requires a special cert-manager Issuer with a secret for the DNS provider's API. It is recommended for users with many tunnels, but is more complex to setup.</p> <p></p> <p>A single certificate and Ingress record can be used for multiple tunnels</p>"},{"location":"uplink/expose-tunnels/#quick-start","title":"Quick start","text":"<p>The instructions assume that you want to expose two HTTP tunnels. We will configure ingress for the first tunnel, called <code>grafana</code>, on the domain <code>grafana.example.com</code>. The second tunnel, called <code>openfaas</code>, will use the domain <code>openfaas.example.com</code>.</p> <p>Both tunnels can be created with <code>kubectl</code> using the Custom Resource Definition, the <code>inlets-pro</code> CLI, or the REST API. See create tunnels for more info:</p> kubectlcli <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f - \napiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: grafana\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\n---\napiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: openfaas\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\nEOF\n</code></pre> <pre><code>$ inlets-pro tunnel create grafana\nCreated tunnel openfaas. OK.\n\n$ inlets-pro tunnel create openfaas\nCreated tunnel openfaas. OK.\n</code></pre> <p>Follow the instruction for Kubernetes Ingress or Istio depending on how you deployed inlets uplink.</p>"},{"location":"uplink/expose-tunnels/#expose-the-tunnel-with-ingress","title":"Expose the Tunnel with Ingress","text":"<ol> <li> <p>Create a new certificate Issuer for tunnels:</p> <pre><code>cat &gt; tunnel-issuer-prod.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: tunnels-letsencrypt-prod\n  namespace: inlets\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n    name: tunnels-letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: \"traefik\"\nEOF\n</code></pre> </li> <li> <p>Create an ingress resource for the tunnel:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-tunnel-ingress\n  namespace: tunnels\n  annotations:\n    cert-manager.io/issuer: tunnels-letsencrypt-prod\nspec:\n  ingressClassName: traefik\n  rules:\n  - host: grafana.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: grafana\n            port:\n              number: 8000\n  tls:\n  - hosts:\n    - grafana.example.com\n    secretName: grafana-cert\n</code></pre> <p>Note that the annotation <code>cert-manager.io/issuer</code> is used to reference the certificate issuer created in the first step.</p> </li> </ol> <p>To setup ingress for multiple tunnels simply define multiple ingress resources. For example, you could create a second ingress resource for the openfaas tunnel:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: openfaas-tunnel-ingress\n  namespace: tunnels\n  annotations:\n    cert-manager.io/issuer: tunnels-letsencrypt-prod\nspec:\n  ingressClassName: traefik\n  rules:\n  - host: openfaas.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: openfaas\n            port:\n              number: 8000\n  tls:\n  - hosts:\n    - openfaas.example.com\n    secretName: openfaas-cert\n</code></pre>"},{"location":"uplink/expose-tunnels/#expose-the-tunnel-with-an-istio-ingress-gateway","title":"Expose the Tunnel with an Istio Ingress Gateway","text":"<ol> <li> <p>Create a new certificate Issuer for tunnels:</p> <pre><code>cat &gt; tunnel-issuer-prod.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: tunnels-letsencrypt-prod\n  namespace: istio-system\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: tunnels-letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: \"istio\"\nEOF\n</code></pre> <p>We are using the Let's Encrypt production server which has strict limits on the API. A staging server is also available at https://acme-staging-v02.api.letsencrypt.org/directory. If you are creating a lot of certificates while testing it would be better to use the staging server.</p> </li> <li> <p>Create a new certificate resource. In this case we want to expose two tunnels on their own domain, <code>grafana.example.com</code> and <code>openfaas.example.com</code>. This will require two certificates, one for each domain:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: grafana-cert\n  namespace: istio-system\nspec:\n  secretName: grafana-cert\n  commonName: grafana.example.com\n  dnsNames:\n  - grafana.example.com\n  issuerRef:\n    name: tunnels-letsencrypt-prod\n    kind: Issuer\n\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: openfaas-cert\n  namespace: istio-system\nspec:\n  secretName: openfaas-cert\n  commonName: openfaas.example.com\n  dnsNames:\n  - openfaas.example.com\n  issuerRef:\n    name: tunnels-letsencrypt-prod\n    kind: Issuer\n</code></pre> <p>Note that both the certificates and issuer are created in the <code>istio-system</code> namespace.</p> </li> <li> <p>Configure the Ingress Gateway for both tunnels. In this case we create a single resource for both hosts but you could also split the configuration into multiple Gateway resources.</p> <pre><code>apiVersion: networking.istio.io/v1\nkind: Gateway\nmetadata:\n  name: tunnel-gateway\n  namespace: inlets\nspec:\n  selector:\n      istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS  \n    tls:\n      mode: SIMPLE\n      credentialName: grafana-cert\n    hosts:\n    - grafana.example.com\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: openfaas-cert\n    hosts:\n    - openfaas.example.com\n</code></pre> <p>Note that the <code>credentialsName</code> references the secrets for the certificates created in the previous step.</p> </li> <li> <p>Configure the gateway's traffic routes by defining corresponding virtual services:</p> <pre><code>apiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: grafana\n  namespace: inlets\nspec:\n  hosts:\n  - grafana.example.com\n  gateways:\n  - tunnel-gateway\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: grafana.tunnels.svc.cluster.local\n        port:\n          number: 8000\n---\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: openfaas\n  namespace: inlets\nspec:\n  hosts:\n  - openfaas.example.com\n  gateways:\n  - tunnel-gateway\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: openfaas.tunnels.svc.cluster.local\n        port:\n          number: 8000\n</code></pre> </li> </ol> <p>After applying these resources you should be able to access the data plane for both tunnels on their custom domain.</p>"},{"location":"uplink/expose-tunnels/#wildcard-ingress-with-the-data-router","title":"Wildcard Ingress with the data-router","text":"<p>As an alternative to creating individual sets of Ingress records, DNS A/CNAME entries and TLS certificates for each tunnel, you can use the <code>data-router</code> to route traffic to the correct tunnel based on the hostname. This approach uses a wildcard DNS entry and a single TLS certificate for all tunnels.</p> <p>To enable the data-router, you will need to modify the values.yaml file you created during the initial installation of the Inlets Uplink Helm chart.</p> <p>The following example is adapted from the cert-manager documentation to use DigitalOcean's DNS servers, however you can find instructions for issuers such as AWS Route53, Cloudflare, Google Cloud DNS, and AzureDNS being listed.</p> <p>DNS01 challenges require a secret to be created containing the credentials for the DNS provider. The secret is referenced by the issuer resource.</p> <pre><code>kubectl create secret generic \\\n  -n inlets digitalocean-dns \\\n  --from-file access-token=$HOME/do-access-token\n</code></pre> <p>Create a separate <code>Issuer</code>, assuming a domain of <code>uplink.example.com</code>, where each tunnel would be i.e. <code>prometheus.uplink.example.com</code> or <code>api.uplink.example.com</code>:</p> <pre><code>export NS=\"inlets\"\nexport ISSUER_NAME=\"inlets-wildcard\"\nexport DOMAIN=\"uplink.example.com\"\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: $ISSUER_NAME\n  namespace: $NS\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: $ISSUER_NAME\n    solvers:\n    - dns01:\n        digitalocean:\n            tokenSecretRef:\n              name: digitalocean-dns\n              key: access-token\nEOF\n</code></pre> <p>Update values.yaml to enable the dataRouter and to specify the wildcard domain:</p> <pre><code>## The dataRouter is an option component to enable easy Ingress to connected tunnels.\n## Learn more under \"Ingress for Tunnels\" in the docs: https://docs.inlets.dev/\ndataRouter:\n  enabled: true\n\n  # Leave out the asterix i.e. *.uplink.example.com would be: uplink.example.com\n  wildcardDomain: \"uplink.example.com\"\n\n  tls:\n    issuerName: \"inlets-wildcard\"\n\n    ingress:\n      enabled: true\n</code></pre> <p>Optional: rate limiting for the data-router</p> <p>Traefik Middleware can be used to apply rate limiting on the data-router Ingress. See Traefik rate limiting for details on creating Middleware resources.</p> <p>To reference the middleware, add the annotation to your <code>values.yaml</code>:</p> <pre><code>dataRouter:\n  tls:\n    ingress:\n      annotations:\n        traefik.ingress.kubernetes.io/router.middlewares: inlets-data-router-ratelimit@kubernetescrd,inlets-data-router-inflight-limit@kubernetescrd\n</code></pre> <p>Apply the updated values:</p> <pre><code>helm upgrade --install inlets-uplink \\\n  oci://ghcr.io/openfaasltd/inlets-uplink-provider \\\n  --namespace inlets \\\n  --values ./values.yaml\n</code></pre> <p>You can now create a new tunnel or modify an existing one and specify the hostname you want to use under the <code>ingressDomains</code> field. This field is an array and can take more than one hostname, depending on how many services you want to expose via the tunnel.</p> <pre><code>export TUNNEL_NS=\"tunnels\"\nexport DOMAIN=\"uplink.example.com\"\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: fileshare\n  namespace: $TUNNEL_NS\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: $TUNNEL_NS\n  ingressDomains:\n    - fileshare.$DOMAIN\nEOF\n</code></pre> <p>On a private computer, create a new directory, a file to serve and then run the built-in HTTP server:</p> <pre><code>cd /tmp\nmkdir -p ./share\ncd ./share\necho \"Hello from inlets\" &gt; index.html\n\ninlets-pro fileserver --port 8080 --allow-browsing --webroot ./\n</code></pre> <p>Get the instructions to connect to the tunnel.</p> <p>The <code>--domain</code> flag here is for your uplink control-plane, where tunnels connect, not the data-plane where ingress is served. This is usually i.e. <code>uplink.example.com</code>.</p> <pre><code>export TUNNEL_NS=\"tunnels\"\nexport UPLINK_DOMAIN=\"uplink.example.com\"\n\ninlets-pro tunnel connect fileshare \\\n  --namespace $TUNNEL_NS \\\n  --domain $UPLINK_DOMAIN\n</code></pre> <p>Add the <code>--upstream fileshare.uplink.example.com=fileshare</code> flag to the command you were given, then run it.</p> <p>The command below is sample output, do not copy it directly.</p> <pre><code>inlets-pro uplink client \\\n  --url=wss://uplink.example.com/tunnels/fileshare \\\n  --token=REDACTED \\\n  --upstream fileshare.uplink.example.com=http://127.0.0.1:8080\n</code></pre> <p>Now, access the tunneled service via the wildcard domain i.e. <code>https://fileshare.uplink.example.com</code>.</p> <p>You should see: \"Hello from inlets\" printed in your browser.</p> <p>Finally, you can view the logs of the data-router, to see it resolving internal tunnel service names for various hostnames:</p> <pre><code>kubectl logs -n inlets deploy/data-router\n\n2024-01-24T11:29:16.965Z        info    data-router/main.go:51  Inlets (tm) Uplink - data-router: \n\n2024-01-24T11:29:16.970Z        info    data-router/main.go:90  Listening on: 8080      Tunnel namespace: (all) Kubernetes version: v1.27.4+k3s1\n\nI0124 11:29:58.858772       1 main.go:151] Host: fileshares.uplink.example.com    Path: /\nI0124 11:29:58.858877       1 roundtripper.go:48] \"No ingress found\" hostname=\"fileshares.uplink.example.com\" path=\"/\"\n\nI0124 11:30:03.588993       1 main.go:151] Host: fileshare.uplink.example.com     Path: /\nI0124 11:30:03.589051       1 roundtripper.go:56] \"Resolved\" hostname=\"fileshare.uplink.example.com\" path=\"/\" tunnel=\"fileshare.tunnels:8000\"\n</code></pre>"},{"location":"uplink/installation/","title":"Install Inlets Uplink","text":"<p>Inlets Uplink requires a Kubernetes cluster, and an inlets uplink subscription.</p> <p>The installation is performed through a Helm chart (inlets-uplink-provider)) which is published as an OCI artifact in a container registry.</p> <p>The default installation keeps tunneled services private, with only the control-plane exposed to the public Internet. To expose the data-plane for one or more tunnels, after you've completed the installation, see the page Expose tunnels.</p>"},{"location":"uplink/installation/#before-you-start","title":"Before you start","text":"<p>Before you start, you'll need the following:</p> <ul> <li>A Kubernetes cluster where you can create a LoadBalancer i.e. a managed Kubernetes service like AWS EKS, Azure AKS, Google GKE, etc.</li> <li>A domain name clients can use to connect to the tunnel control plane.</li> <li>An inlets uplink license (an inlets-pro license cannot be used)</li> <li> <p>Optional: arkade - a tool for installing popular Kubernetes tools</p> <p>To install arkade run:</p> <pre><code>curl -sSLf https://get.arkade.dev/ | sudo sh\n</code></pre> </li> </ul> <p>You can obtain a subscription for inlets uplink here: inlets uplink plans.</p>"},{"location":"uplink/installation/#create-a-kubernetes-cluster","title":"Create a Kubernetes cluster","text":"<p>We recommend creating a Kubernetes cluster with a minimum of three nodes. Each node should have a minimum of 2GB of RAM and 2 CPU cores.</p>"},{"location":"uplink/installation/#install-cert-manager","title":"Install cert-manager","text":"<p>Install cert-manager, which is used to manage TLS certificates for inlets-uplink for the control-plane and the REST API.</p> <p>You can use Helm, or arkade:</p> <pre><code>helm install \\\n  cert-manager oci://quay.io/jetstack/charts/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --set crds.enabled=true\n</code></pre>"},{"location":"uplink/installation/#create-a-namespace-for-the-chart-and-add-the-license-secret","title":"Create a namespace for the chart and add the license secret","text":"<p>Make sure to create the target namespace for you installation first.</p> <pre><code>kubectl create namespace inlets\n</code></pre> <p>Create the required secret with your inlets-uplink license.</p> <p>Check that your license key is in lower-case</p> <p>There is a known issue with LemonSqueezy where the UI will copy the license key in lower-case, it needs to be converted to upper-case before being used with Inlets Uplink.</p> <p>Convert the license to upper-case, if it's in lower-case:</p> <pre><code>(\n  mv $HOME/.inlets/LICENSE_UPLINK{,.lower}\n\n  cat $HOME/.inlets/LICENSE_UPLINK.lower | tr '[:lower:]' '[:upper:]' &gt; $HOME/.inlets/LICENSE_UPLINK\n  rm $HOME/.inlets/LICENSE_UPLINK.lower\n)\n</code></pre> <p>Create the secret for the license:</p> <pre><code>kubectl create secret generic \\\n  -n inlets inlets-uplink-license \\\n  --from-file license=$HOME/.inlets/LICENSE_UPLINK\n</code></pre>"},{"location":"uplink/installation/#setup-up-ingress-for-the-control-plane","title":"Setup up Ingress for the control-plane","text":"<p>Tunnel clients will connect to the client-router component which needs to be exposed via Ingress.</p> <p>You can use Kubernetes Ingress or Istio. We recommend using Ingress (Option A), unless your team or organisation is already using Istio (Option B).</p>"},{"location":"uplink/installation/#a-install-with-kubernetes-ingress","title":"A) Install with Kubernetes Ingress","text":"<p>We recommend Traefik for Ingress, and have finely tuned the configuration to work well for the underlying websocket for inlets. If your organisation uses a different Ingress Controller, you can alter the <code>class</code> fields in the chart.</p> <p>NGINX Ingress Controller Retirement</p> <p>The Kubernetes NGINX Ingress Controller project has announced its retirement in March 2026 and will no longer receive updates or security patches.</p> <p>The uplink chart version 0.5.0 changes the default ingress class from Nginx to Traefik. To upgrade to the latest uplink while keeping NGINX ingress see the Ingress NGINX section for legacy configuration options.</p> <p>Install traefik with Helm:</p> <pre><code>helm repo add traefik https://traefik.github.io/charts\nhelm repo update\n\nhelm install traefik traefik/traefik \\\n  --namespace=traefik \\\n  --create-namespace\n</code></pre> <p>See also: Traefik installation</p> <p>Create a <code>values.yaml</code> file for the inlets-uplink-provider chart:</p> <pre><code>ingress:\n  class: \"traefik\"\n  issuer:\n    # When set, a production issuer will be generated for you\n    # to use a pre-existing issuer, set issuer.enabled=false\n    enabled: true\n\nclientRouter:\n  # Customer tunnels will connect with a URI of:\n  # wss://uplink.example.com/namespace/tunnel\n  domain: uplink.example.com\n\n  tls:\n    issuerName: letsencrypt-prod\n    ingress:\n      enabled: true\n</code></pre> <p>Make sure to replace the domain with your actual domain name.</p> <p>Optionally, you can add rate limiting to the client-router Ingress using Traefik Middleware. See Traefik rate limiting for details.</p> <p>Want to use the staging issuer for testing?</p> <p>To use the Let's Encrypt staging issuer, pre-create your own issuer, update <code>clientRouter.tls.issuerName</code> with the name you have chosen, and then update <code>clientRouter.tls.issuer.enabled</code> and set it to false.</p>"},{"location":"uplink/installation/#b-install-with-istio","title":"B) Install with Istio","text":"<p>We have added support in the inlets-uplink chart for Istio to make it as simple as possible to configure with a HTTP01 challenge.</p> <p>If you don't have Istio setup already you can deploy it with arkade.</p> <pre><code>arkade install istio\n</code></pre> <p>Label the <code>inlets</code> namespace so that Istio can inject its sidecars:</p> <pre><code>kubectl label namespace inlets \\\n  istio-injection=enabled --overwrite\n</code></pre> <p>Create a <code>values.yaml</code> file for the inlets-uplink chart:</p> <pre><code>ingress:\n  issuer:\n    # When set, a production issuer will be generated for you\n    # to use a pre-existing issuer, set issuer.enabled=false\n    enabled: true\n    class: \"istio\"\n\nclientRouter:\n  # Customer tunnels will connect with a URI of:\n  # wss://uplink.example.com/namespace/tunnel\n  domain: uplink.example.com\n\n  tls:\n    issuerName: letsencrypt-prod\n    istio:\n      enabled: true\n</code></pre> <p>Make sure to replace the domain with your actual domain name.</p>"},{"location":"uplink/installation/#deploy-with-helm","title":"Deploy with Helm","text":"<p>The chart is served through a container registry (OCI), not GitHub pages</p> <p>Many Helm charts are served over GitHub pages, from a public repository, making it easy to browse and read the source code. We are using an OCI artifact in a container registry, which makes for a more modern alternative. If you want to browse the source, you can simply run <code>helm template</code> instead of <code>helm upgrade</code>.</p> <p>Unauthorized?</p> <p>The chart artifacts are public and do not require authentication, however if you run into an \"Access denied\" or authorization error when interacting with <code>ghcr.io</code>, try running <code>helm registry login ghcr.io</code> to refresh your credentials, or <code>docker logout ghcr.io</code>.</p> <p>The Helm chart is called inlets-uplink-provider, you can deploy it using the custom values.yaml file created above:</p> <pre><code>helm upgrade --install inlets-uplink \\\n  oci://ghcr.io/openfaasltd/inlets-uplink-provider \\\n  --namespace inlets \\\n  --values ./values.yaml\n</code></pre> <p>If you want to pin the version of the Helm chart, you can do so with the <code>--version</code> flag.</p> <p>Where can I see the various options for values.yaml?</p> <p>All of the various options for the Helm chart are documented in the configuration reference.</p> <p>How can I view the source code?</p> <p>See the note on <code>helm template</code> under the configuration reference.</p> <p>How can I find the latest version of the chart?</p> <p>If you omit a version, Helm will use the latest published OCI artifact, however if you do want to pin it, you can browse all versions of the Helm chart on GitHub</p> <p>As an alternative to using ghcr.io's UI, you can get the list of tags, including the latest tag via the crane CLI:</p> <pre><code>arkade get crane\n\n# List versions\ncrane ls ghcr.io/openfaasltd/inlets-uplink-provider\n\n# Get the latest version\nLATEST=$(crane ls ghcr.io/openfaasltd/inlets-uplink-provider |tail -n 1)\necho $LATEST\n</code></pre>"},{"location":"uplink/installation/#verify-the-installation","title":"Verify the installation","text":"<p>Once you've installed inlets-uplink, you can verify it is deployed correctly by checking the <code>inlets</code> namespace for running pods:</p> <pre><code>$ kubectl get pods --namespace inlets\n\nNAME                               READY   STATUS    RESTARTS   AGE\nclient-router-b5857cf6f-7vrdh      1/1     Running   0          92s\nprometheus-74d8d7db9b-2hptm        1/1     Running   0          16s\nuplink-operator-7fccc9bdbc-twd2q   1/1     Running   0          92s\n</code></pre> <p>You should see the <code>client-router</code> and <code>cloud-operator</code> in a <code>Running</code> state.</p> <p>If you installed inlets-uplink with Kubernetes ingress, you can verify that ingress for the client-router is setup and that a TLS certificate is issued for your domain using these two commands:</p> <pre><code>$ kubectl get -n inlets ingress/client-router\n\nNAME            CLASS    HOSTS                ADDRESS           PORTS     AGE\nclient-router   traefik  uplink.example.com   188.166.194.102   80, 443   31m\n</code></pre> <pre><code>$ kubectl get -n inlets cert/client-router-cert\n\nNAME                 READY   SECRET               AGE\nclient-router-cert   True    client-router-cert   30m\n</code></pre>"},{"location":"uplink/installation/#setup-the-rest-api","title":"Setup the REST API","text":"<p>The REST API for Uplink is enabled by default and accessible on the same domain as the client-router under the <code>/v1</code> path prefix. For example, if your client-router domain is <code>uplink.example.com</code>, the API will be available at <code>https://uplink.example.com/v1</code>.</p> <p>See the REST API reference to learn how to invoke the API and for a full list of endpoints.</p> <p>Optionally, the client-api can be exposed on a separate domain.</p>"},{"location":"uplink/installation/#access-token","title":"Access token","text":"<p>An access token is generated by Helm and stored as a Kubernetes secret during installation. This token can be used to authenticate with the API.</p> <p>If you need to create the token manually, for example to use a specific value, you can do so before installing the chart:</p> <pre><code># Generate a new access token\nexport token=$(openssl rand -base64 32|tr -d '\\n')\necho -n $token &gt; $HOME/.inlets/client-api\n\n# Store the access token in a secret in the inlets namespace.\nkubectl create secret generic \\\n  client-api-token \\\n  -n inlets \\\n  --from-file client-api-token=$HOME/.inlets/client-api\n</code></pre> <p>If the secret already exists, Helm will use the existing token instead of generating a new one.</p> <p>See the OAuth configuration section for instructions on how to enable OAuth.</p>"},{"location":"uplink/installation/#configure-oauth","title":"Configure OAuth","text":"<p>You can configure any OpenID Connect (OIDC) compatible identity provider for use with Inlets Uplink.</p> <ol> <li>Register a new client (application) for Inlets Uplink with your identity provider.</li> <li>Enable the required authentication flows.   The Client Credentials flow is ideal for serve-to-server interactions where there is no direct user involvement. This is the flow we recommend and use in our examples any other authentication flow can be picked depending on your use case.</li> <li> <p>Configure Client API</p> <p>Update your <code>values.yaml</code> file and add the following parameters to the <code>clientApi</code> section:</p> <pre><code>clientApi:\n  # OIDC provider url.\n  issuerURL: \"https://myprovider.example.com\"\n\n  # The audience is generally the same as the value of the domain field, however\n  # some issuers like keycloak make the audience the client_id of the application/client.\n  audience: \"uplink.example.com\"\n</code></pre> </li> </ol> <p>The <code>issuerURL</code> needs to be set to the url of your provider, eg. <code>https://accounts.google.com</code> for google or <code>https://example.eu.auth0.com/</code> for Auth0.</p> <p>The <code>audience</code> is usually the client apis public URL although for some providers it can also be the client id.</p>"},{"location":"uplink/installation/#configure-a-separate-api-domain","title":"Configure a separate API domain","text":"<p>By default, the client-api is exposed on the same domain as the client-router under the <code>/v1</code> path prefix. If you prefer to use a separate domain, set the <code>clientApi.domain</code> field in your <code>values.yaml</code> file and enable the dedicated ingress:</p> <pre><code>clientApi:\n  # Use a dedicated domain for the client API\n  domain: clientapi.example.com\n\n  tls:\n    ingress:\n      enabled: true\n</code></pre> <p>When a dedicated domain is set and <code>clientApi.tls.ingress.enabled</code> is <code>true</code>, a separate Ingress resource is created for the client-api.</p>"},{"location":"uplink/installation/#download-the-tunnel-cli","title":"Download the tunnel CLI","text":"<p>We provide a CLI to help you create and manage tunnels. It is available as a plugin for the inlets-pro CLI.</p> <p>Download the <code>inlets-pro</code> binary:</p> <ul> <li>Download it from the GitHub releases</li> <li>Get it with arkade: <code>arkade get inlets-pro</code></li> </ul> <p>Get the tunnel plugin:</p> <pre><code>inlets-pro plugin get tunnel\n</code></pre> <p>Run <code>inlets-pro tunnel --help</code> to see all available commands.</p>"},{"location":"uplink/installation/#setup-the-first-customer-tunnel","title":"Setup the first customer tunnel","text":"<p>Continue the setup here: Create a customer tunnel</p>"},{"location":"uplink/installation/#upgrading-the-chart-and-components","title":"Upgrading the chart and components","text":"<p>If you have a copy of values.yaml with pinned image versions, you should update these manually.</p> <p>Next, run the Helm chart installation command again, and remember to use the sames values.yaml file that you used to install the software originally.</p> <p>Over time, you may find using a tool like FluxCD or ArgoCD to manage the installation and updates makes more sense than running Helm commands manually.</p> <p>Ingress class change in chart version 0.5.0</p> <p>The default ingress class changed from Nginx to Traefik in chart version 0.5.0. If you are still using NGINX ingress, make sure your values.yaml includes the required configuration from the Ingress NGINX section before upgrading.</p> <p>If the Custom Resource Definition (CRD) has changed, you can extract it from the Chart repo and install it before or after upgrading. As a rule, Helm won't install or upgrade CRDs a second time if there's already an existing version:</p> <pre><code>helm template oci://ghcr.io/openfaasltd/inlets-uplink-provider \\\n  --include-crds=true \\\n  --output-dir=/tmp\n\nkubectl apply -f \\\n  /tmp/inlets-uplink-provider/crds/uplink.inlets.dev_tunnels.yaml\n</code></pre>"},{"location":"uplink/installation/#upgrading-existing-customer-tunnels","title":"Upgrading existing customer tunnels","text":"<p>The operator will upgrade the <code>image:</code> version of all deployed inlets uplink tunnels automatically based upon the tag set in values.yaml.</p> <p>If no value is set in your overridden values.yaml file, then whatever the default is in the chart will be used.</p> <pre><code>inletsVersion: 0.9.23\n</code></pre> <p>When a tunnel is upgraded, you'll see a log line like this:</p> <pre><code>2024-01-11T12:25:15.442Z        info    operator/controller.go:860      Upgrading version       {\"tunnel\": \"ce.inlets\", \"from\": \"0.9.21\", \"to\": \"0.9.23\"}\n</code></pre>"},{"location":"uplink/installation/#configuration-reference","title":"Configuration reference","text":"<p>Looking for the source for the Helm chart? The source is published directly to a container registry as an OCI bundle. View the source with: <code>helm template oci://ghcr.io/openfaasltd/inlets-uplink-provider</code></p> <p>If you need a configuration option outside of what's already available, feel free to raise an issue on the inlets-pro repository.</p> <p>Overview of inlets-uplink parameters in <code>values.yaml</code>.</p> Parameter Description Default <code>pullPolicy</code> The a imagePullPolicy applied to inlets-uplink components. <code>Always</code> <code>operator.image</code> Container image used for the uplink operator. <code>ghcr.io/openfaasltd/uplink-operator:0.1.5</code> <code>ingress.issuer.name</code> Name of cert-manager Issuer. <code>letsencrypt-prod</code> <code>ingress.issuer.enabled</code> Create a cert-manager Issuer. Set to false if you wish to specify your own pre-existing object for each component. <code>true</code> <code>ingress.issuer.email</code> Let's Encrypt email. Only used for certificate renewing notifications. <code>\"\"</code> <code>ingress.class</code> Ingress class for client router ingress. <code>traefik</code> <code>clientRouter.image</code> Container image used for the client router. <code>ghcr.io/openfaasltd/uplink-client-router:0.1.5</code> <code>clientRouter.domain</code> Domain name for inlets uplink. Customer tunnels will connect with a URI of: wss://uplink.example.com/namespace/tunnel. <code>\"\"</code> <code>clientRouter.tls.ingress.enabled</code> Enable ingress for the client router. <code>enabled</code> <code>clientRouter.tls.ingress.annotations</code> Annotations to be added to the client router ingress resource. <code>{}</code> <code>clientRouter.tls.istio.enabled</code> Use an Istio Gateway for incoming traffic to the client router. <code>false</code> <code>clientRouter.service.type</code> Client router service type <code>ClusterIP</code> <code>clientRouter.service.nodePort</code> Client router service port for NodePort service type, assigned automatically when left empty. (only if clientRouter.service.type is set to \"NodePort\") <code>nil</code> <code>tunnelsNamespace</code> Deployments, Services and Secrets will be created in this namespace. Leave blank for a cluster-wide scope, with tunnels in multiple namespaces. <code>\"\"</code> <code>inletsVersion</code> Inlets Pro release version for tunnel server Pods. <code>0.9.12</code> <code>clientApi.enabled</code> Enable tunnel management REST API. <code>true</code> <code>clientApi.domain</code> Domain for a dedicated client API ingress. By default the API is exposed on the client-router's domain under the <code>/v1</code> path prefix. <code>\"\"</code> <code>clientApi.tls.ingress.enabled</code> Enable a dedicated ingress for the client API. Requires <code>clientApi.domain</code> to be set. <code>false</code> <code>clientApi.tls.ingress.annotations</code> Annotations to be added to the client API ingress resource. <code>{}</code> <code>clientApi.image</code> Container image used for the client API. <code>ghcr.io/openfaasltd/uplink-api:0.1.5</code> <code>prometheus.create</code> Create the Prometheus monitoring component. <code>true</code> <code>prometheus.resources</code> Resource limits and requests for prometheus containers. <code>{}</code> <code>prometheus.image</code> Container image used for prometheus. <code>prom/prometheus:v2.40.1</code> <code>prometheus.service.type</code> Prometheus service type <code>ClusterIP</code> <code>prometheus.service.nodePort</code> Prometheus service port for NodePort service type, assigned automatically when left empty. (only if prometheus.service.type is set to \"NodePort\") <code>nil</code> <code>nodeSelector</code> Node labels for pod assignment. <code>{}</code> <code>affinity</code> Node affinity for pod assignments. <code>{}</code> <code>tolerations</code> Node tolerations for pod assignment. <code>[]</code> <p>Specify each parameter using the <code>--set key=value[,key=value]</code> argument to <code>helm install</code></p>"},{"location":"uplink/installation/#telemetry-and-usage-data","title":"Telemetry and usage data","text":"<p>The inlets-uplink Kubernetes operator will send telemetry data to OpenFaaS Ltd on a periodic basis. This information is used for calculating accurate usage metrics for billing purposes. This data is sent over HTTPS, does not contain any personal information, and is not shared with any third parties.</p> <p>This data includes the following:</p> <ul> <li>Number of tunnels deployed</li> <li>Number of namespaces with at least one tunnel contained</li> <li>Kubernetes version</li> <li>Inlets Uplink version</li> <li>Number of installations of Inlets Uplink</li> </ul>"},{"location":"uplink/installation/#traefik-rate-limiting","title":"Traefik rate limiting","text":"<p>With Traefik, rate limiting is configured using Middleware custom resources. You can use the RateLimit middleware to limit requests per second and the InFlightReq middleware to limit simultaneous connections.</p> <p>Create a <code>Middleware</code> resource for rate limiting in the <code>inlets</code> namespace:</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: uplink-rate-limit\n  namespace: inlets\nspec:\n  rateLimit:\n    average: 17\n    period: 1s\n    burst: 50\n</code></pre> <p>Create a <code>Middleware</code> resource for limiting simultaneous connections:</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: uplink-inflight-limit\n  namespace: inlets\nspec:\n  inFlightReq:\n    amount: 300\n</code></pre> <p>To apply the middleware to the client-router Ingress, add the <code>traefik.ingress.kubernetes.io/router.middlewares</code> annotation in your <code>values.yaml</code>:</p> <pre><code>clientRouter:\n  tls:\n    ingress:\n      annotations:\n        traefik.ingress.kubernetes.io/router.middlewares: inlets-uplink-rate-limit@kubernetescrd,inlets-uplink-inflight-limit@kubernetescrd\n</code></pre> <p>The annotation value follows the format <code>&lt;namespace&gt;-&lt;middleware-name&gt;@kubernetescrd</code>. Multiple middleware can be chained with commas.</p>"},{"location":"uplink/installation/#ingress-nginx","title":"Ingress NGINX","text":"<p>The Kubernetes NGINX Ingress Controller project has announced its retirement in March 2026 and will no longer receive updates or security patches. The uplink chart version 0.5.0 changes the default ingress class from Nginx to Traefik. If you want to update to the latest uplink version but have not migrated your ingress controller yet, you need to add the following additional parameters in the values.yaml configuration for the uplink Helm chart.</p> <pre><code>ingress:\n  class: \"nginx\"\n\nclientRouter:\n  tls:\n    ingress:\n      annotations:\n        nginx.ingress.kubernetes.io/limit-connections: \"300\"\n        nginx.ingress.kubernetes.io/limit-rpm: \"1000\"\n        # 10 minutes for the websocket\n        nginx.ingress.kubernetes.io/proxy-read-timeout: \"3600\"\n        nginx.ingress.kubernetes.io/proxy-send-timeout: \"3600\"\n        # Up the keepalive timeout to max\n        nginx.ingress.kubernetes.io/keepalive-timeout: \"350\"\n        nginx.ingress.kubernetes.io/proxy-buffer-size: 128k\n</code></pre> <p>When you have the data-router deployed you can add these additional rate-limiting annotations as well. They used to be set as defaults by the chart.</p> <pre><code>dataRouter:\n  tls:\n    ingress:\n      annotations:\n        nginx.ingress.kubernetes.io/limit-connections: \"300\"\n        nginx.ingress.kubernetes.io/limit-rpm: \"1000\"\n</code></pre>"},{"location":"uplink/manage-tunnels/","title":"Manage tunnels","text":"<p>You can use <code>kubectl</code> or the tunnel plugin for the <code>inlets-pro</code> CLI to manage tunnels.</p>"},{"location":"uplink/manage-tunnels/#list-tunnels","title":"List tunnels","text":"<p>List tunnels across all namespaces:</p> kubectlcli <pre><code>$ kubectl get tunnels -A\n\nNAMESPACE     NAME         AUTHTOKENNAME   DEPLOYMENTNAME   TCP PORTS   DOMAINS\ntunnels       acmeco       acmeco          acmeco           [8080]      \ncustomer1     ssh          ssh             ssh              [50035]\ncustomer1     prometheus   prometheus      prometheus       []         [prometheus.customer1.example.com]\n</code></pre> <pre><code>$ inlets-pro tunnel list -A\n\nTUNNEL     DOMAINS                              PORTS   CREATED\nacmeco     []                                   [8080]  2022-11-22 11:51:35 +0100 CET\nssh        []                                   [50035] 2022-11-24 18:19:01 +0100 CET\nprometheus [prometheus.customer1.example.com]   []      2022-11-24 11:43:23 +0100 CET\n</code></pre> <p>To list the tunnels within a namespace:</p> kubectlcli <pre><code>$ kubectl get tunnels -n customer1\n\nNAME         AUTHTOKENNAME   DEPLOYMENTNAME   TCP PORTS   DOMAINS\nssh          ssh             ssh              [50035]\n</code></pre> <pre><code>$ inlets-pro tunnel list -n customer1\n\nTUNNEL     DOMAINS   PORTS   CREATED\nssh        []        [50035] 2022-11-22 11:51:35 +0100 CET\n</code></pre>"},{"location":"uplink/manage-tunnels/#delete-a-tunnel","title":"Delete a tunnel","text":"<p>Deleting a tunnel will remove all resources for the tunnel.</p> <p>To remove a tunnel run:</p> kubectlcli <pre><code>kubectl delete -n tunnels \\\n  tunnel/acmeco \n</code></pre> <pre><code>inlets-pro tunnel remove acmeco \\\n  -n tunnels\n</code></pre> <p>Do also remember to stop the customer's inlets uplink client.</p>"},{"location":"uplink/manage-tunnels/#update-the-ports-or-domains-for-a-tunnel","title":"Update the ports or domains for a tunnel","text":"<p>You can update a tunnel and configure its TCP ports or domain names by editing the Tunnel Custom Resource:</p> <pre><code>kubectl edit -n tunnels \\\n  tunnel/acmeco  \n</code></pre> <p>Imagine you wanted to add port 8081, when you already had port 8080 exposed:</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: acmeco\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\n  tcpPorts:\n  - 8080\n+ - 8081\n</code></pre> <p>Alternatively, if you have the tunnel saved as a YAML file, you can edit it and apply it again with <code>kubectl apply</code>.</p>"},{"location":"uplink/manage-tunnels/#check-the-logs-of-a-tunnel","title":"Check the logs of a tunnel","text":"<p>The logs for tunnels can be useful for troubleshooting or to see if clients are connecting successfully.</p> <p>Get the logs for a tunnel deployment: </p> <pre><code>$ kubectl logs -n tunnels deploy/acmeco -f\n\n2022/11/22 12:07:38 Inlets Uplink For SaaS &amp; Service Providers (Inlets Uplink for 5x Customers)\n2022/11/22 12:07:38 Licensed to: user@example.com\ninlets (tm) uplink server\nAll rights reserved OpenFaaS Ltd (2022)\n\nMetrics on: 0.0.0.0:8001\nControl-plane on: 0.0.0.0:8123\nHTTP data-plane on: 0.0.0.0:8000\ntime=\"2022/11/22 12:33:34\" level=info msg=\"Added upstream: * =&gt; http://127.0.0.1:9090 (9355de15c687471da9766cbe51423e54)\"\ntime=\"2022/11/22 12:33:34\" level=info msg=\"Handling backend connection request [9355de15c687471da9766cbe51423e54]\"\n</code></pre>"},{"location":"uplink/manage-tunnels/#rotate-the-secret-for-a-tunnel","title":"Rotate the secret for a tunnel","text":"<p>You may want to rotate a secret for a customer if you think the secret has been leaked. The token can be rotated manually using <code>kubectl</code> or with a single command using the <code>tunnel</code> CLI plugin.</p> kubectlcli <p>Delete the token secret. The default secret has the same name as the tunnel. The inlets uplink controller will automatically create a new secret.</p> <pre><code>kubectl delete -n tunnels \\\n  secret/acmeco \n</code></pre> <p>The tunnel has to be restarted to use the new token. </p> <pre><code>kubectl rollout restart -n tunnels \\\n  deploy/acmeco\n</code></pre> <p>Rotate the tunnel token:</p> <pre><code>inlets-pro tunnel rotate acmeco \\\n  -n tunnels\n</code></pre> <p>Any connected tunnels will disconnect at this point, and won\u2019t be able to reconnect until you configure them with the updated token.</p> <p>Retrieve the new token for the tunnel and save it to a file:</p> kubectlcli <pre><code>kubectl get -n tunnels secret/acmeco \\\n  -o jsonpath=\"{.data.token}\" | base64 --decode &gt; token.txt \n</code></pre> <pre><code>inlets-pro tunnel token acmeco \\\n  -n tunnels &gt; token.txt\n</code></pre> <p>The contents will be saved in <code>token.txt</code></p>"},{"location":"uplink/monitoring-tunnels/","title":"Monitoring inlets uplink","text":"<p>Inlets Uplink comes with an integrated Prometheus deployment that automatically collects metrics for each tunnel.</p> <p>Note</p> <p>Prometheus is deployed with Inlets Uplink by default. If you don't need monitoring you can disable it in the <code>values.yaml</code> of the Inlets Uplink Helm chart:</p> <pre><code>prometheus:\n  create: false\n</code></pre> <p>You can explore the inlets data using Prometheus's built-in expression browser. To access it, port forward the prometheus service and than navigate to http://localhost:9090/graph</p> <pre><code>kubectl port-forward \\\n  -n inlets \\\n  svc/prometheus 9090:9090\n</code></pre>"},{"location":"uplink/monitoring-tunnels/#metrics-for-the-control-plane","title":"Metrics for the control-plane","text":"<p>The control-plane metrics can give you insights into the number of clients that are connected and the number of http requests made to the control-plane endpoint for each tunnel.</p> Metric Type Description Labels controlplane_connected_gauge gauge gauge of inlets clients connected to the control plane <code>tunnel_name</code> controlplane_requests_total counter total HTTP requests processed by connecting clients on the control plane <code>code</code>, <code>tunnel_name</code>"},{"location":"uplink/monitoring-tunnels/#metrics-for-the-data-plane","title":"Metrics for the data-plane","text":"<p>The data-plane metrics can give you insights in the services that are exposed through your tunnel.</p> Metric Type Description Labels dataplane_connections_gauge gauge gauge of connections established over data plane <code>port</code>, <code>type</code>, <code>tunnel_name</code> dataplane_connections_total counter total count of connections established over data plane <code>port</code>, <code>type</code>, <code>tunnel_name</code> dataplane_requests_total counter total HTTP requests processed <code>code</code>, <code>host</code>, <code>method</code>, <code>tunnel_name</code> dataplane_request_duration_seconds histogram seconds spent serving HTTP requests <code>code</code>, <code>host</code>, <code>method</code>, <code>tunnel_name</code>, <p>The connections metrics show the number of connections that are open at this point in time, and on which ports. The <code>type</code> label indicates whether the connection is for a <code>http</code> or <code>tcp</code> upstream.</p> <p>The request metrics only include HTTP upstreams. These metrics can be used to get Rate, Error, Duration (RED) information for any API or website that is connected through the tunnel.</p>"},{"location":"uplink/monitoring-tunnels/#setup-grafana-for-monitoring","title":"Setup Grafana for monitoring","text":"<p>Grafana can be used to visualize the data collected by the inlets uplink Prometheus instance. We provide a sample dashboard that you can use as a starting point.</p> <p> </p> <p>Inlets uplink Grafana dashboard</p> <p>The dashboard can help you get insights in:</p> <ul> <li>The number of client connected to each tunnel.</li> <li>Invocation to the control plane for each tunnel. This can help with detecting misbehaving clients.</li> <li>Rate, Error, Duration (RED) information for HTTP tunnels.</li> <li>The number of connections TCP connections opened for each tunnel.</li> </ul>"},{"location":"uplink/monitoring-tunnels/#install-grafana","title":"Install Grafana","text":"<p>There are three options we recommend for getting access to Grafana.</p> <ul> <li>Grafana installed with its Helm chart</li> <li>Grafana Cloud</li> <li>AWS managed Grafana</li> </ul> <p>You can install Grafana in one line with arkade:</p> <p><pre><code>arkade install grafana\n</code></pre> Grafana can also be installed with Helm. See: Grafana Helm Chart</p> <p>Port forward grafana and retrieve the admin password to login:</p> <pre><code># Expose the service via port-forward:\nkubectl --namespace grafana port-forward service/grafana 3000:80\n\n# Get the admin password:\nkubectl get secret --namespace grafana grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre> <p>Access Grafana on http://127.0.0.1:3000 and login as admin.</p>"},{"location":"uplink/monitoring-tunnels/#add-a-data-source","title":"Add a data source","text":"<p>Before you import the dashboard, you need to add the inlets-uplink prometheus instance as a data source:</p> <ol> <li>Select the cog icon on the side menu to show the configuration options.</li> <li> <p>Select Data sources.</p> <p>This opens the data sources page, which displays a list of previously configured data sources for the Grafana instance.</p> </li> <li> <p>Select Add data source and pick Prometheus from the list of supported data sources.</p> </li> <li> <p>Configure the inlets Prometheus instance as a data source:</p> <p></p> <ul> <li>In the name field set: <code>inlets-prometheus</code></li> <li>For the URL use: <code>http://prometheus.inlets:9090</code> <p>if you installed inlets uplink in a different namespace this url should be <code>http://prometheus.&lt;namespace&gt;:9090</code></p> </li> <li>Set the scrape interval field to <code>30s</code></li> </ul> </li> </ol>"},{"location":"uplink/monitoring-tunnels/#import-the-dashboard","title":"Import the dashboard","text":"<p>Import the inlets uplink dashboard in Grafana:</p> <ol> <li>Click Dashboards &gt; Import in the side menu.</li> <li>Copy the dashboard JSON text</li> <li> <p>Paste the dashboard JSON into the text area.</p> <p></p> </li> </ol>"},{"location":"uplink/private-tunnels/","title":"Access tunnels privately","text":""},{"location":"uplink/private-tunnels/#access-tunnels-privately","title":"Access tunnels privately","text":"<p>By default, only the control-plane is made public, and the data-plane is kept private.</p> <p>This suits two use-cases:</p> <ul> <li>A SaaS or Service Provider who needs to give customers a way to tunnel a private endpoint or service to their control plane</li> <li>An infrastructure team managing their own applications or services across multiple environments</li> </ul> <p>Once the tunnel client is connected, the data-plane of the tunnel can be access from within the Inlets Uplink control-plane cluster.</p> <p>You can deploy your applications to the management Kubernetes Cluster, then access them via the tunnel's ClusterIP just like they were running directly within your cluster.</p> <p>The traffic will go to the ClusterIP, which points at the tunnel server's data-plane ports, which then causes a connection to be dialed back to the client.</p> <p></p> <p>Uplink with no external ingress or exposed tunnels</p> <p>If you need external ingress for one or more tunnels, then you should see the expose tunnels page.</p>"},{"location":"uplink/private-tunnels/#access-a-tcp-service-via-clusterip","title":"Access a TCP service via ClusterIP","text":"<p>If you have forwarded a TCP service, then you can access it via the ClusterIP of the service.</p> <p>Let's say that you port-forwarded SSH and remapped port 22 to 2222 to avoid needing to use any additional privileged for the tunnel server Pod:</p> <pre><code>kubectl run -t -i --rm tcp-access --image=ubuntu --restart=Never -- sh\n</code></pre> <p>Then from within the pod, you can access the service via the ClusterIP:</p> <pre><code># apt update\n# apt install -qy openssh-client\n\n# ssh -p 2222 user@tunnel1.customer1\n</code></pre> <p>The address is made up of the tunnel name plus the namespace you used for the customer.</p>"},{"location":"uplink/private-tunnels/#access-a-http-service-via-clusterip","title":"Access a HTTP service via ClusterIP","text":"<p>If you have forwarded an HTTP service, then you can access it via the ClusterIP of the service, using port 8000.</p> <p>If you have forwarded multiple HTTP endpoints, use the HTTP \"Host\" header to access one or the other.</p> <pre><code>kubectl run -t -i --rm http-access --image=ubuntu --restart=Never -- sh\n</code></pre> <p>Then from within the pod, you can access the service via the ClusterIP:</p> <pre><code># apt update\n# apt install -qy curl\n\n# curl -i http://tunnel1.customer1:8000\n</code></pre> <p>To access a specific domain, use the HTTP \"Host\" header:</p> <pre><code># apt update\n# apt install -qy curl\n\n# curl -H \"Host:  www.example.com\" -i http://tunnel1.customer1:8000\n</code></pre>"},{"location":"uplink/rest-api/","title":"Inlets Uplink REST API","text":"<p>Inlets uplink tunnels and namespaces can be managed through a REST API.</p> <p>For setup instructions, including how to configure API authentication and enable the API ingress, see Setup the REST API.</p>"},{"location":"uplink/rest-api/#authentication","title":"Authentication","text":"<p>The Inlets Uplink client API supports authentication through a static API token or using OAuth.</p>"},{"location":"uplink/rest-api/#static-api-token","title":"Static API token","text":"<p>The authentication token can be retrieved from the cluster at any time by an administrator.</p> <pre><code>export TOKEN=$(kubectl get secret -n inlets client-api-token \\\n  -o jsonpath=\"{.data.client-api-token}\" \\\n  | base64 --decode)\n</code></pre> <p>Use the token as bearer token in the <code>Authorization</code> header when making requests to the API.</p>"},{"location":"uplink/rest-api/#oauth","title":"OAuth","text":"<p>If you have OAuth enabled you can obtain a token from your provider that can be used to invoke the Uplink Client API. See Configure OAuth for setup instructions.</p> <p>The example uses the client credentials grant. Replace the token url, client id and client secret with the values obtained from your identity provider.</p> <pre><code>export IDP_TOKEN_URL=\"https://myprovider.example.com/token\"\nexport CLIENT_ID=\"inlets-uplink\"\nexport CLIENT_SECRET=\"$(cat ./client-secret.txt)\"\n\ncurl -S -L -X POST \"${IDP_TOKEN_URL}\" \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --data-urlencode \"client_id=${CLIENT_ID}\" \\\n  --data-urlencode \"client_secret=${CLIENT_SECRET}\" \\\n  --data-urlencode 'scope=openid' \\\n  --data-urlencode 'grant_type=client_credentials'\n</code></pre> <p>Use the token as bearer token in the <code>Authorization</code> header when making requests to the API.</p> <pre><code>export CLIENT_API=\"https://uplink.example.com\"\nexport NAME=\"acmeco\"\nexport NAMESPACE=\"acmeco\"\n\ncurl -i \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/tunnels/$NAME?namespace=$NAMESPACE\"\n</code></pre>"},{"location":"uplink/rest-api/#tunnel-management","title":"Tunnel management","text":"<p>We will be create an tunnel named <code>acmeco</code> in the <code>acmeco</code> namespace in the API examples. </p>"},{"location":"uplink/rest-api/#get-a-tunnel","title":"Get a tunnel","text":"<pre><code>export CLIENT_API=\"https://uplink.example.com\"\nexport NAME=\"acmeco\"\nexport NAMESPACE=\"acmeco\"\n\ncurl -i \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/tunnels/$NAME?namespace=$NAMESPACE\"\n</code></pre> <p>Adding the query parameter <code>metrics=1</code> includes additional tunnel metrics in the response like RX and TX and TCP connection rate.</p> <p>Path parameters:</p> <ul> <li><code>name</code> - Name of the tunnel.</li> </ul> <p>Query parameters:</p> <ul> <li><code>namespace</code> - Namespace where the tunnel should be looked up.</li> <li><code>metrics</code> - Include tunnel metrics in the response.</li> </ul> <p>Example response with metrics:</p> <pre><code>{\n  \"name\": \"acmeco\",\n  \"namespace\": \"acmeco\",\n  \"tcpPorts\": [80, 443],\n  \"authToken\": \"TAjFZExVq6qUfnqojwR2HOej347fRXqV3vLexlyoP6GcRZ2SjIUALY8Jdx8\",\n  \"connectedClients\": 1,\n  \"created\": \"2024-09-10T14:48:21Z\",\n  \"metrics\": {\n    \"rx\": 195482,\n    \"tx\": 32348,\n    \"tcpConnectionRate\": 62.99\n  }\n}\n</code></pre> <p>The metrics section includes rx/tx bytes per second and tcp connection rate over the last 5 minutes.</p>"},{"location":"uplink/rest-api/#list-tunnels","title":"List tunnels","text":"<pre><code>export CLIENT_API=\"https://uplink.example.com\"\nexport NAMESPACE=\"acmeco\"\n\ncurl -i \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/tunnels?namespace=$NAMESPACE\"\n</code></pre> <p>Query parameters:</p> <ul> <li><code>namespace</code> - Namespace where the tunnel should be looked up.</li> </ul>"},{"location":"uplink/rest-api/#create-a-tunnel","title":"Create a tunnel","text":"<pre><code>export CLIENT_API=\"https://uplink.example.com\"\n\ncurl -i \\\n  -X POST \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/tunnels\"\n  -d '{ \"name\": \"acmeco\", \"namespace\": \"acmeco\", \"tcpPorts\": [ 80, 443 ]  }'\n</code></pre>"},{"location":"uplink/rest-api/#update-a-tunnel","title":"Update a tunnel","text":"<pre><code>export CLIENT_API=\"https://uplink.example.com\"\n\ncurl -i \\\n  -X PUT \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/tunnels\"\n  -d '{ \"name\": \"acmeco\", \"namespace\": \"acmeco\", \"tcpPorts\": [ 80, 443, 4222 ] }'\n</code></pre>"},{"location":"uplink/rest-api/#delete-a-tunnel","title":"Delete a tunnel","text":"<pre><code>export CLIENT_API=\"https://uplink.example.com\"\nexport NAME=\"acmeco\"\nexport NAMESPACE=\"acmeco\"\n\ncurl -i \\\n  -X DELETE \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/tunnels/$NAME?namespace=$NAMESPACE\"\n</code></pre> <p>Path parameters:</p> <ul> <li><code>name</code> - Name of the tunnel.</li> </ul> <p>Query parameters:</p> <ul> <li><code>namespace</code> - Namespace where the tunnel should be looked up.</li> </ul>"},{"location":"uplink/rest-api/#namespace-management","title":"Namespace management","text":"<p>The inlets uplink client API includes REST endpoints for listing, creating and deleting namespaces. Namespaces created through the API are automatically labeled for use with inlets uplink. The <code>kube-system</code> and <code>inlets</code> namespace can not be used as tunnel namespaces.</p>"},{"location":"uplink/rest-api/#list-uplink-namespace","title":"List uplink namespace","text":"<p>List all inlets uplink namespaces. This endpoint will list all namespaces with a label <code>inlets.dev/uplink=1</code>.</p> <pre><code>export CLIENT_API=\"https://uplink.example.com\"\n\ncurl -i \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/namespace\n</code></pre>"},{"location":"uplink/rest-api/#create-a-namespace","title":"Create a namespace","text":"<pre><code>export CLIENT_API=\"https://uplink.example.com\"\n\ncurl -i \\\n  -X POST \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/namespace\n  -d '{ \"name\": \"acmeco\" }'\n</code></pre> <p>Every namespace created through the API will have the <code>inlets.dev/uplink=1</code> label set.</p> <p>The API supports adding additional namespace labels and annotations:</p> <pre><code>{\n  \"name\": \"acmeco\",\n  \"annotations\": {\n    \"customer\": \"acmeco\"\n  },\n  \"labels\": {\n    \"customer\": \"acmeco\"\n  }\n}\n</code></pre>"},{"location":"uplink/rest-api/#delete-a-namespace","title":"Delete a namespace","text":"<pre><code>export CLIENT_API=\"https://uplink.example.com\"\nexport NAME=\"acmeco\"\n\ncurl -i \\\n  -X DELETE \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  \"$CLIENT_API/v1/namespace/$NAME\"\n</code></pre>"},{"location":"uplink/troubleshooting/","title":"Troubleshooting","text":""},{"location":"uplink/troubleshooting/#client-connections","title":"Client connections","text":"<p>Depending on how you are running the client, with Kubernetes or as a systemd service, use the following commands to check the logs.</p> <ul> <li> <p>kubectl logs command (if deployed in Kubernetes):     <pre><code>kubectl logs deploy/&lt;tunnel-name&gt;-inlets-client -f\n</code></pre></p> </li> <li> <p>systemd logs (if deployed as service):     <pre><code>journalctl -u &lt;tunnel-name&gt;-tunnel -f\n</code></pre></p> </li> </ul> <p>Logs for the client will show you the connection status, reconnect attempts and errors and warnings that occurred during the connection.</p> <ul> <li>Verify the client is using a valid auth token for the tunnel.</li> <li>Verify the client is using the same inlets version as the tunnel server</li> <li>Verify the tunnel exist and is running, see: troubleshoot tunnel servers</li> <li>Verify the tunnels is reachable through the client-router. See: troubleshoot the client router</li> </ul>"},{"location":"uplink/troubleshooting/#tunnel-servers","title":"Tunnel servers","text":"<p>Each tunnel gets its own tunnel server deployment that handles the actual data forwarding.</p> <p>Check the tunnel exists:</p> <pre><code>kubectl get tunnels.uplink.inlets.dev -A\n</code></pre> <p>Check the tunnel deployment is running and healthy</p> <pre><code>kubectl get deploy -n &lt;tunnel-namespace&gt;\n</code></pre> <p>Check the service for the tunnel got created and has the correct ports:</p> <pre><code>kubectl get svc -n &lt;tunnel-namespace&gt;\n</code></pre> <p>A tunnels service should have 8123/TCP,8000/TCP,8001/TCP + any additional TCP ports from the tunnel spec.</p> <p>Check the logs for the tunnel deployment:</p> <pre><code>kubectl logs -n &lt;tunnel-namespace&gt; deploy/&lt;tunnel-name&gt;\n</code></pre> <p>Check event in the tunnel namespace:</p> <pre><code>kubectl get events -n &lt;tunnel-namespace&gt; \\\n  --sort-by=.metadata.creationTimestamp\n</code></pre> <p>Common issues:</p> <ul> <li>The Uplink license has not been copied to the tunnel namespace.</li> <li>The tunnel secret referenced by <code>tokenRef</code> in the tunnel spec does not exist.</li> <li>You have reached the maximum number of tunnels allowed by your license.</li> </ul> <p>See troubleshoot the uplink operator if the deployment or service for the tunnel does not exist.</p>"},{"location":"uplink/troubleshooting/#uplink-operator","title":"Uplink operator","text":"<p>The uplink operator manages the lifecycle of tunnels, creating deployments, services, and secrets for each tunnel.</p> <p>Check the uplink operator is running:</p> <pre><code>kubectl get deploy/uplink-operator -n inlets\n</code></pre> <p>Issues creating the tunnel resources will be shown in the logs.</p> <pre><code>kubectl logs -n inlets deploy/uplink-operator -f\n</code></pre> <p>The logs will show you if a tunnel did not get reconciled because you reached the maximum number of tunnels allowed by your license.</p>"},{"location":"uplink/troubleshooting/#client-router","title":"Client router","text":"<p>The client router handles incoming WebSocket connections from tunnel clients and routes them to the appropriate tunnel server.</p> <p>Check the client router is running:</p> <pre><code>kubectl get deploy/client-router -n inlets\n</code></pre> <p>All client connection attempts are logged by the router. If there are any issues reaching the upstream tunnel server will be shown in the logs.</p> <pre><code>kubectl logs -n inlets deploy/client-router -f\n</code></pre> <p>If client connections are not showing up in the client-router logs make sure to check the client router is reachable. Check your ingress configuration and the logs of your ingress server.</p>"},{"location":"uplink/troubleshooting/#network-reliability-issues","title":"Network reliability issues","text":"<p>Some protocols may experience network reliability issues under the standard inlets uplink mode due to Head-of-Line (HOL) blocking or slow reader/writer problems that can affect connection stability.</p>"},{"location":"uplink/troubleshooting/#demux-mode","title":"Demux Mode","text":"<p>The demux (demultiplexed) mode addresses these issues by opening a separate WebSocket connection for each remote connection to a forwarded port. This prevents slow or problematic connections from affecting other connections on the same tunnel.</p> <p>Enable demux mode by setting the <code>uplink_demux</code> environment variable in the Tunnel spec:</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: kubernetes-api\n  namespace: tenant1\nspec:\n+ env:\n+   uplink_demux: \"1\"\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tenant1\n  tcpPorts:\n    - 6443\n</code></pre>"},{"location":"uplink/troubleshooting/#proxy-protocol-issues","title":"Proxy Protocol Issues","text":"<p>When using TCP tunnels, Proxy Protocol can be a common source of connectivity issues. Proxy Protocol is used to preserve the original client IP address when traffic passes through multiple proxy layers.</p>"},{"location":"uplink/troubleshooting/#common-proxy-protocol-problems","title":"Common Proxy Protocol Problems","text":"<ol> <li> <p>Misconfigured Proxy Protocol at any hop: If Proxy Protocol is enabled at one point in the chain but not properly handled at every subsequent hop, connections will fail.</p> </li> <li> <p>Version mismatch: Ensure all components use the same Proxy Protocol version (v1 or v2).</p> </li> <li> <p>Service configuration: The tunneled service must be configured to expect and parse Proxy Protocol headers when enabled.</p> </li> </ol>"},{"location":"uplink/troubleshooting/#troubleshooting-proxy-protocol","title":"Troubleshooting Proxy Protocol","text":"<p>Check if your tunneled service supports Proxy Protocol and verify the configuration at each network hop:</p> <ul> <li>Load balancers (AWS ALB, Traefik, HAProxy)</li> <li>Ingress controllers</li> <li>Service meshes (Istio, Linkerd)</li> <li>The tunneled application itself</li> </ul> <p>Disable Proxy Protocol temporarily to isolate whether it's the source of connection issues.</p>"},{"location":"uplink/troubleshooting/#best-practices","title":"Best Practices","text":""},{"location":"uplink/troubleshooting/#minimize-proxy-hops","title":"Minimize Proxy Hops","text":"<p>To ensure optimal tunnel performance and reliability, minimize the number of hops through proxies for any tunnels. Each additional proxy hop can introduce latency, potential points of failure, and complicate troubleshooting. Direct connections or configurations with fewer intermediary proxies will generally provide better performance and stability.</p>"},{"location":"uplink/tutorials/argo-cd/","title":"Manage apps on remote Kubernetes clusters with ArgoCD","text":"<p>This tutorial shows how to use ArgoCD with Inlets Uplink to manage applications across multiple remote Kubernetes clusters from a centralized GitOps control plane.</p> <p>With Inlets Uplink, each remote cluster connects back to the control plane over a secure, TLS-encrypted tunnel. This allows Argo CD to manage remote clusters without exposing their APIs to the public internet.</p> <p></p> <p>In this tutorial, we will:</p> <ul> <li>Create Uplink tunnels for one or more remote Kubernetes clusters.</li> <li>Register those clusters with Argo CD.</li> <li>Show how an applicatioin can be deployed to all registerd cluster.</li> </ul> <p>Why use this approach?</p> <ul> <li>Security - Cluster APIs stay private, with no public exposure. Inlets Uplink creates a secure, encrypted tunnel for Argo CD to connect.</li> <li>Centralized management - Operate and deploy to all clusters from a single Argo CD control plane.</li> <li>Scalability - Add new clusters easily by creating a tunnel and registering it in Argo CD.</li> </ul> <p>This tutorial has been tested with K3s clusters but works with any Kubernetes distribution.</p>"},{"location":"uplink/tutorials/argo-cd/#prerequisites","title":"Prerequisites","text":"<ul> <li>A central cluster with Inlets Uplink installed</li> <li>One or more remote K3s clusters to manage</li> <li>arkade installed</li> <li>Basic knowledge of Kubernetes and GitOps</li> </ul>"},{"location":"uplink/tutorials/argo-cd/#step-1-create-uplink-tunnels-for-remote-clusters","title":"Step 1: Create Uplink tunnels for remote clusters","text":"<p>For each remote cluster, create a dedicated tunnel. These could be created in separate namespaces for better isolation when e.g. adding clusters for multiple different tenants.</p> <p>Create a namespace for the first remote cluster:</p> <pre><code>export CLUSTER_NAME=\"production-east\"\nexport NS=\"tenant1\"\n\nkubectl create namespace $NS\nkubectl label namespace $NS \"inlets.dev/uplink\"=1\n</code></pre> <p>Copy your Uplink license to the new namespace:</p> <pre><code>export LICENSE=$(kubectl get secret -n inlets inlets-uplink-license -o jsonpath='{.data.license}' | base64 -d)\n\nkubectl create secret generic \\\n  -n $NS \\\n  inlets-uplink-license \\\n  --from-literal license=$LICENSE\n</code></pre> <p>Create the tunnel resource for the Kubernetes API:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: kubernetes-$CLUSTER_NAME\n  namespace: $NS\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: $NS\n  tcpPorts:\n    - 6443\nEOF\n</code></pre> <p>See create tunnel for more details on managing tunnels.</p> <p>Get the inlets uplink management CLI, and use it to generate a Kubernetes Deployment definition for the tunnel client.</p> <pre><code>inlets-pro plugin get tunnel\n\ninlets-pro tunnel connect kubernetes-$CLUSTER_NAME \\\n  --namespace $NS \\\n  --domain uplink.example.com \\\n  --upstream 6443=kubernetes.default.svc:443 \\\n  --format k8s_yaml &gt; $CLUSTER_NAME-tunnel-client.yaml\n</code></pre> <p>Replace <code>uplink.example.com</code> with your actual Uplink domain.</p> <p>The client Deployment YAML will have to be applied to the remote cluster.</p> <p>Switch over to the customer\u2019s Kubernetes cluster and apply the YAML for the tunnel client:</p> <pre><code># Switch to remote cluster context\nkubectl config use-context $CLUSTER_NAME\n\n# Deploy the tunnel client\nkubectl apply -f $CLUSTER_NAME-tunnel-client.yaml\n\n# Verify connection\nkubectl logs deploy/kubernetes-$CLUSTER_NAME-inlets-client\n</code></pre> <p>See connect the tunnel client for more details on connecting the client.</p> <p>Repeat these steps for each remote cluster you want to manage.</p>"},{"location":"uplink/tutorials/argo-cd/#step-2-install-argocd-on-the-central-cluster","title":"Step 2: Install ArgoCD on the central cluster","text":"<p>Switch back to your central cluster and install ArgoCD:</p> <pre><code>arkade install argocd\n</code></pre> <p>Get the ArgoCD CLI:</p> <pre><code>arkade get argocd\n</code></pre> <p>Port-forward the ArgoCD server:</p> <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443 &amp;\n</code></pre> <p>Login to ArgoCD:</p> <pre><code>PASS=$(kubectl get secret argocd-initial-admin-secret \\\n  -n argocd \\\n  -o jsonpath=\"{.data.password}\" | base64 -d)\n\nargocd login 127.0.0.1:8080 --insecure \\\n  --username admin \\\n  --password $PASS\n</code></pre>"},{"location":"uplink/tutorials/argo-cd/#step-3-register-remote-clusters-with-argocd","title":"Step 3: Register remote clusters with ArgoCD","text":"<p>Clusters can be registered declaratively with Argo CD by adding a sectet containing the cluster credentials. Each secret must have label <code>argocd.argoproj.io/secret-type: cluster</code>. Checkout the Argo CD documentation for more info.</p> <p>We provide a small utility to generate these secrets from a kubeconfig file.</p> <p>Generate and apply the cluster secret for a remote cluster:</p> <pre><code># Ensure the tunnel plugin is installed\ninlets-pro plugin get tunnel\n\ninlets-rpo tunnel argo generate $CLUSTER_NAME \\\n  --kubeconfig ~/.kube/$CLUSTER_NAME.yaml \\\n  --upstream https://kubernetes-$CLUSTER_NAME.$NS:6443 | \\\n  kubectl apply -f -\n</code></pre> <p>Repeat this for each remoter cluster you want to add.</p> <p>Verify the cluster is registered:</p> <pre><code>argocd cluster list\n</code></pre> <p>You should see your remote cluster listed with the tunneled URL.</p>"},{"location":"uplink/tutorials/argo-cd/#deploy-applications-to-all-clusters","title":"Deploy applications to all clusters","text":"<p>Now that the cluster are reachable and registered with ArgoCD we can start deploying applications to them. From here on we are just going to apply standard ArgoCD practices and patterns.</p> <p>Checkout the Argo CD documentation for more info on a declarative application setup</p> <p>As an example we are going to install the OpenFaaS ArgoCD App. This is a sample application that uses the app of apps pattern to deploy two applications. The first application is named <code>openfaas-core</code> and deploys OpenFaaS. The second application is named <code>openfaas-functions</code> and deploys a set of OpenFaaS functions.</p> <p>For a detailed description of the application you can read our blog post: How to update your OpenFaaS functions automatically with the Argo CD Image Updater</p> <p>We can deploy the application to a single cluster as described in the original article but ideally we want to deploy it to multiple remote clusters. To achieve this we can use an ApplicationSet to automatically generate multiple applications.</p> <p>An ApplicationSet uses generators to create Applications dynamically. The cluster generator discovers all registered clusters and creates an Application for each one. This eliminates the need to manually create separate Application manifests for every cluster.</p> <p>Example ApplicationSet for deploying to multiple clusters:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: openfaas-multi-cluster\n  namespace: argocd\nspec:\n  generators:\n    # Cluster Generator: Creates parameters for every registered cluster\n    - clusters:\n        # Use a label selector to filter for cluster Secrets.\n        # The 'cluster' label is automatically added to Secrets\n        # created for *remote* clusters, but not for the local 'in-cluster' setup.\n        selector:\n          matchLabels:\n            argocd.argoproj.io/secret-type: cluster\n  template:\n    metadata:\n      # Template creates unique app names using cluster name: openfaas-production-east\n      name: openfaas-{{name}}\n      namespace: argocd\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/welteki/openfaas-argocd-example.git\n        path: chart/openfaas\n        helm:\n          parameters:\n            # Override the destination server URL and appNameSuffix with the values\n            # provided by the generator\n            - name: spec.destination.server\n              value: \"{{server}}\"\n            - name: appNameSuffix\n              value: \"{{name}}\"\n      destination:\n        # Deploy to ArgoCD namespace on central cluster (manages remote deployments)\n        server: \"https://kubernetes.default.svc\"\n        namespace: argocd\n      syncPolicy:\n        automated:\n          selfHeal: true\n</code></pre> <p>Apply the ApplicationSet:</p> <pre><code>kubectl apply -f openfaas-applicationset.yaml\n</code></pre> <p>The ApplicationSet will automatically create individual Applications for each registered cluster. You can monitor the deployment progress in the ArgoCD UI or via CLI:</p> <pre><code>argocd app list\n</code></pre> <p></p> <p>Argo CD UI showing the openfaas application deployed to 3 different remore clusters.</p>"},{"location":"uplink/tutorials/argo-cd/#conclusion","title":"Conclusion","text":"<p>You now have a reference architecture for centralized GitOps management of multiple remote Kubernetes clusters using ArgoCD and Inlets Uplink. This approach provides:</p> <ul> <li>Security: No public exposure of cluster APIs</li> <li>Scalability: Easy addition of new clusters</li> <li>Centralization: Single point of control for all deployments</li> <li>Reliability: Secure, encrypted tunnel connections</li> </ul> <p>The combination of Inlets Uplink tunnels and ArgoCD provides a robust foundation for multi-cluster GitOps at scale while maintaining strong security boundaries.</p> <p>If you run into network reliability issues consider running the Uplink tunnels in demux mode to improve reliability.</p>"}]}